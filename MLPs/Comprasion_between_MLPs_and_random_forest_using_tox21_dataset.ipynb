{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Comprasion_between_MLPs_and_random_forest_using_tox21_dataset.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MassimilianoBiancucci/Tensorflow-exercises/blob/master/MLPs/Comprasion_between_MLPs_and_random_forest_using_tox21_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "407WW85KfoG4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Training an MLP network to recognize if a molecule is toxic for humans\n"
      ]
    },
    {
      "metadata": {
        "id": "M1_z56TAzYXO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We starting to setting up our VM to get all packets needed for run our code."
      ]
    },
    {
      "metadata": {
        "id": "I3n04hHnrKqe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install deepchem\n",
        "!pip install simdna\n",
        "!pip install nosetests\n",
        "!wget -c https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
        "!time bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
        "!time conda install -q -y -c conda-forge rdkit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l2ZBudqz17Kh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this section we download the packets needed for run a tensorboard remotly on colab and get a link at ngrok.com to access to the tensorboard trought the colab's firewall.\n",
        "![alt text](https://gitcdn.xyz/cdn/Tony607/blog_statics/d425c3fe4cf0d92067572e25ae6cc3198d51936b//images/ngrok/ngrok.jpg)"
      ]
    },
    {
      "metadata": {
        "id": "X73uV25F08pr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z5rXtmcd17iv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "here we launch the tensorboard in background and  set the directory for saving session log.\n"
      ]
    },
    {
      "metadata": {
        "id": "iMyAOfmG09cR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "LOG_DIR = './log'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KssEpVBS2GwZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then, we can run ngrok to tunnel TensorBoard port 6006 to the outside world. This command also runs in the background.\n"
      ]
    },
    {
      "metadata": {
        "id": "dzJYm_O12zKu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jGQdt_OC2zvi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we get the public URL where we can access the colab TensorBoard.\n",
        "It's important keep in mind that the training have to start before you can see somthing in. \n"
      ]
    },
    {
      "metadata": {
        "id": "MBPAVeUu2OH1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qM_-1V682PFP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Import libraries:**\n",
        "\n",
        "we start importing **numpy**, a python library to work efficiently with tensors, folowed by **tensorflow** and  **sklearn.metrics**, the libraries that contain the toolkit for work with data, networks and in this case tools for getting the performance of model.\n",
        "**Matplotlib** for display data and in the end **deepchem** that contain the tox21 dataset.\n",
        "\n",
        "**Tox21**, is a unique collaboration between several federal agencies to develop new ways to rapidly test whether substances adversely affect human health. This dataset consists of a set of 10,000 molecules represented vectorially  tested for interaction with the androgen receptor.\n"
      ]
    },
    {
      "metadata": {
        "id": "UyXZ1Je7TL9T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import os\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')\n",
        "import numpy as np\n",
        "np.random.seed(456)\n",
        "import  tensorflow as tf\n",
        "tf.set_random_seed(456)\n",
        "import deepchem.molnet as dc\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MHTx4slHfmtK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we load tox21 dataset and prepare it, removing usefull data."
      ]
    },
    {
      "metadata": {
        "id": "bWFBDvA3mzxl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "_, (train, valid, test), _ = dc.load_tox21()\n",
        "train_X, train_y, train_w = train.X, train.y, train.w\n",
        "valid_X, valid_y, valid_w = valid.X, valid.y, valid.w\n",
        "test_X, test_y, test_w = test.X, test.y, test.w\n",
        "\n",
        "\n",
        "# Remove extra tasks\n",
        "train_y = train_y[:, 0]\n",
        "valid_y = valid_y[:, 0]\n",
        "test_y = test_y[:, 0]\n",
        "train_w = train_w[:, 0]\n",
        "valid_w = valid_w[:, 0]\n",
        "test_w = test_w[:, 0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V4bke0dV2EiV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this section we declare the structure of the tensor graph, which represents the model."
      ]
    },
    {
      "metadata": {
        "id": "Wl1zIsI2zBVq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Generate tensorflow graph\n",
        "#general parameters\n",
        "d = 1024\n",
        "n_hidden = 50\n",
        "n_hidden2 = 30\n",
        "n_hidden3 = 10\n",
        "learning_rate = .01\n",
        "n_epochs = 20\n",
        "batch_size = 100\n",
        "\n",
        "#dataset\n",
        "with tf.name_scope(\"dataset\"):\n",
        "  x = tf.placeholder(tf.float32, (None, d))\n",
        "  y = tf.placeholder(tf.float32, (None,))\n",
        "  \n",
        "  \n",
        "with tf.name_scope(\"hidden-layer1\"):\n",
        "  W = tf.Variable(tf.random_normal((d, n_hidden)))\n",
        "  b = tf.Variable(tf.random_normal((n_hidden,)))\n",
        "  x_hidden = tf.nn.relu(tf.matmul(x, W) + b)\n",
        "  \n",
        "with tf.name_scope(\"hidden-layer2\"):\n",
        "  W = tf.Variable(tf.random_normal((n_hidden, n_hidden2)))\n",
        "  b = tf.Variable(tf.random_normal((n_hidden2,)))\n",
        "  x_hidden2 = tf.nn.relu(tf.matmul(x_hidden, W) + b)\n",
        "  \n",
        "with tf.name_scope(\"hidden-layer3\"):\n",
        "  W = tf.Variable(tf.random_normal((n_hidden2, n_hidden3)))\n",
        "  b = tf.Variable(tf.random_normal((n_hidden3,)))\n",
        "  x_hidden3 = tf.nn.relu(tf.matmul(x_hidden2, W) + b)\n",
        "  \n",
        "with tf.name_scope(\"output\"):\n",
        "  W = tf.Variable(tf.random_normal((n_hidden3, 1)))\n",
        "  b = tf.Variable(tf.random_normal((1,)))\n",
        "  y_logit = tf.matmul(x_hidden3, W) + b\n",
        "  \n",
        "  # the sigmoid gives the class probability of 1\n",
        "  y_one_prob = tf.sigmoid(y_logit)\n",
        "  # Rounding P(y=1) will give the correct prediction.\n",
        "  y_pred = tf.round(y_one_prob)\n",
        "  \n",
        " #setting of loss function\n",
        "with tf.name_scope(\"loss\"):\n",
        "  \n",
        "  # Compute the cross-entropy term for each datapoint\n",
        "  y_expand = tf.expand_dims(y, 1)\n",
        "  entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_logit, labels=y_expand)\n",
        "  \n",
        "  # Sum all contributions\n",
        "  l = tf.reduce_sum(entropy)\n",
        "\n",
        "#setting of optimization algorithm\n",
        "with tf.name_scope(\"optim\"):\n",
        "  train_op = tf.train.AdamOptimizer(learning_rate).minimize(l)\n",
        "\n",
        "#setting variables to show in tensorboard scalar section \n",
        "with tf.name_scope(\"summaries\"):\n",
        "  tf.summary.scalar(\"loss\", l)\n",
        "  merged = tf.summary.merge_all()\n",
        "\n",
        "#configure folder for tensorboard data\n",
        "train_writer = tf.summary.FileWriter(LOG_DIR, tf.get_default_graph())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rHz2XtUPf6ha",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this section the model is trained"
      ]
    },
    {
      "metadata": {
        "id": "uqHbB1vWf7H2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "N = train_X.shape[0]\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  step = 0\n",
        "  for epoch in range(n_epochs):\n",
        "    pos = 0\n",
        "    while pos < N:\n",
        "      batch_X = train_X[pos:pos+batch_size]\n",
        "      batch_y = train_y[pos:pos+batch_size]\n",
        "      feed_dict = {x: batch_X, y: batch_y}\n",
        "      _, summary, loss = sess.run([train_op, merged, l], feed_dict=feed_dict)\n",
        "      print(\"epoch %d, step %d, loss: %f\" % (epoch, step, loss))\n",
        "      train_writer.add_summary(summary, step)\n",
        "    \n",
        "      step += 1\n",
        "      pos += batch_size\n",
        "\n",
        "  # Make Predictions for model evaluetion\n",
        "  # for the training dataset\n",
        "  train_y_pred = sess.run(y_pred, feed_dict={x: train_X})\n",
        "  #for the validation dataset\n",
        "  valid_y_pred = sess.run(y_pred, feed_dict={x: valid_X})\n",
        "  #for the test dataset\n",
        "  test_y_pred = sess.run(y_pred, feed_dict={x: test_X})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P8PsQ11yhuC_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this section we use **train_y_perd**, **valid_y_perd**, **test_y_perd** which is used for the calulation of two score for each through the sklearn function **accuracy_score**, this comand take an optional argument **sample_weight** which is set normaly at 1. The first score use the same weight for all of examples (1), in the second case we set it with the given array form the tox21 dataset.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Set a correct metric**\n",
        "\n",
        "Tox21 pass this array because we want that the model learn to recognize toxic substances, in the dataset this substances are only the 5% of the total, therefore the right way to set a metric is ballance this inequality. To do this we use mentionated array that contain a weight of 19 for toxic subtances and 1 for non-toxic substances. In this way, we have a complessive weight of toxic examples of 50% of total and a weight of 50% of total for a not-toxic examples.\n",
        "Getting a balanced metric for our purposes\n"
      ]
    },
    {
      "metadata": {
        "id": "WLLt1E5PgdsU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#calculation of normal score for the train dataset\n",
        "train_score = accuracy_score(train_y, train_y_pred)\n",
        "print(\"Unweighted Classification Accuracy for the training dataset: %f\" % train_score)\n",
        "#calculation of weighted score for the train dataset\n",
        "weighted_train_score = accuracy_score(train_y, train_y_pred, sample_weight=train_w)\n",
        "print(\"Weighted Classification Accuracy for the training dataset: %f\" % weighted_train_score)\n",
        "\n",
        "#calculation of normal score for the validation dataset\n",
        "valid_score = accuracy_score(valid_y, valid_y_pred)\n",
        "print(\"Unweighted Classification Accuracy for the validation dataset: %f\" % valid_score)\n",
        "#calculation of weighted score for the validation dataset\n",
        "weighted_valid_score = accuracy_score(valid_y, valid_y_pred, sample_weight=valid_w)\n",
        "print(\"Weighted Classification Accuracy for the validation dataset: %f\" % weighted_valid_score)\n",
        "\n",
        "#calculation of normal score for the test dataset\n",
        "test_score = accuracy_score(test_y, test_y_pred)\n",
        "print(\"Unweighted Classification Accuracy for the test dataset: %f\" % test_score)\n",
        "#calculation of weighted score for the test dataset\n",
        "weighted_test_score = accuracy_score(test_y, test_y_pred, sample_weight=test_w)\n",
        "print(\"Weighted Classification Accuracy for the test dataset: %f\" % weighted_test_score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HigzclB_aXTH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Training a Random Forest to recognize if a molecule is toxic for humans"
      ]
    },
    {
      "metadata": {
        "id": "yGefFTs4bFTd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we importing from sklearn library a usefull method for train a random forest classifier "
      ]
    },
    {
      "metadata": {
        "id": "ThVh1pjCaraH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OPBHRPFxbaz7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Whit this sample function is easy create a standard machine learning algorithm and train it with the sample function **fit**, noting that the two arguments of this function is the input data first and the lables, the model do all necessary adjustment."
      ]
    },
    {
      "metadata": {
        "id": "nBr6EDrYbdQJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sklearnModel = RandomForestClassifier(class_weight=\"balanced\", n_estimators=100)\n",
        "sklearnModel.fit(train_X, train_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W7t2WJHBcfdu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After training the model we can get the result from the entire validation and test dataset and store it in same variables, with the sample command **predict**."
      ]
    },
    {
      "metadata": {
        "id": "c4xZ8S08ciLX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_y_pred = sklearnModel.predict(train_X)\n",
        "valid_y_pred = sklearnModel.predict(valid_X)\n",
        "test_y_pred = sklearnModel.predict(test_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1ahAV-lOfIg-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we doing the same thing done in the precedent example for ballance the metric also for the random forset classifier."
      ]
    },
    {
      "metadata": {
        "id": "ovnqaXfSfJEf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#calculation of weighted score for the train dataset\n",
        "weighted_score = accuracy_score(train_y, train_y_pred, sample_weight = train_w)\n",
        "print(\"Weighted train Classification Accurancy: %f\" %weighted_score)\n",
        "#calculation of weighted score for the validation dataset\n",
        "weighted_score =  accuracy_score(valid_y, valid_y_pred, sample_weight = valid_w)\n",
        "print(\"Weighted validation Classification Accurancy: %f\" %weighted_score)\n",
        "#calculation of weighted score for the test dataset\n",
        "weighted_score =  accuracy_score(test_y, test_y_pred, sample_weight = test_w)\n",
        "print(\"Weighted test Classification Accurancy: %f\" %weighted_score)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}