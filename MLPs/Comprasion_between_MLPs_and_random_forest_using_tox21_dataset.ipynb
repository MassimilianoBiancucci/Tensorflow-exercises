{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Comprasion_between_MLPs_and_random_forest_using_tox21_dataset.ipynb","version":"0.3.2","provenance":[{"file_id":"https://github.com/MassimilianoBiancucci/Tensorflow-exercises/blob/master/MLPs/fully_connected_deep_networks_using_tox21_dataset.ipynb","timestamp":1552922683492}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"407WW85KfoG4","colab_type":"text"},"cell_type":"markdown","source":["#Training an MLP network to recognize if a molecule is toxic for humans\n"]},{"metadata":{"id":"M1_z56TAzYXO","colab_type":"text"},"cell_type":"markdown","source":["We starting to setting up our VM to get all packets needed for run our code."]},{"metadata":{"id":"I3n04hHnrKqe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":3763},"outputId":"3256d6e7-b82a-41cc-9abd-5b0768a5e1d0","executionInfo":{"status":"ok","timestamp":1552926073863,"user_tz":-60,"elapsed":154264,"user":{"displayName":"simone viozzi","photoUrl":"https://lh4.googleusercontent.com/-lHfKAhqXixo/AAAAAAAAAAI/AAAAAAAABF8/q48tw94xHaI/s64/photo.jpg","userId":"00173786927897941269"}}},"cell_type":"code","source":["!pip install deepchem\n","!pip install simdna\n","!pip install nosetests\n","!wget -c https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n","!chmod +x Miniconda3-latest-Linux-x86_64.sh\n","!time bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n","!time conda install -q -y -c conda-forge rdkit"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting deepchem\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/01/5eafa6217cb522d24531244501bf42b9e18012581cce3b7e655b97549288/deepchem-2.1.1.dev353-py3-none-any.whl (2.1MB)\n","\r\u001b[K    0% |▏                               | 10kB 15.0MB/s eta 0:00:01\r\u001b[K    0% |▎                               | 20kB 1.8MB/s eta 0:00:02\r\u001b[K    1% |▌                               | 30kB 2.6MB/s eta 0:00:01\r\u001b[K    1% |▋                               | 40kB 1.7MB/s eta 0:00:02\r\u001b[K    2% |▉                               | 51kB 2.1MB/s eta 0:00:01\r\u001b[K    2% |█                               | 61kB 2.5MB/s eta 0:00:01\r\u001b[K    3% |█                               | 71kB 2.9MB/s eta 0:00:01\r\u001b[K    3% |█▎                              | 81kB 3.3MB/s eta 0:00:01\r\u001b[K    4% |█▍                              | 92kB 3.7MB/s eta 0:00:01\r\u001b[K    4% |█▋                              | 102kB 2.8MB/s eta 0:00:01\r\u001b[K    5% |█▊                              | 112kB 2.8MB/s eta 0:00:01\r\u001b[K    5% |██                              | 122kB 4.0MB/s eta 0:00:01\r\u001b[K    6% |██                              | 133kB 4.0MB/s eta 0:00:01\r\u001b[K    6% |██▏                             | 143kB 7.5MB/s eta 0:00:01\r\u001b[K    7% |██▍                             | 153kB 7.5MB/s eta 0:00:01\r\u001b[K    7% |██▌                             | 163kB 7.5MB/s eta 0:00:01\r\u001b[K    8% |██▊                             | 174kB 7.6MB/s eta 0:00:01\r\u001b[K    8% |██▉                             | 184kB 7.6MB/s eta 0:00:01\r\u001b[K    9% |███                             | 194kB 7.6MB/s eta 0:00:01\r\u001b[K    9% |███▏                            | 204kB 8.0MB/s eta 0:00:01\r\u001b[K    10% |███▎                            | 215kB 8.0MB/s eta 0:00:01\r\u001b[K    10% |███▌                            | 225kB 8.0MB/s eta 0:00:01\r\u001b[K    11% |███▋                            | 235kB 8.1MB/s eta 0:00:01\r\u001b[K    11% |███▉                            | 245kB 8.1MB/s eta 0:00:01\r\u001b[K    12% |████                            | 256kB 8.1MB/s eta 0:00:01\r\u001b[K    12% |████                            | 266kB 8.0MB/s eta 0:00:01\r\u001b[K    13% |████▎                           | 276kB 8.0MB/s eta 0:00:01\r\u001b[K    13% |████▍                           | 286kB 8.0MB/s eta 0:00:01\r\u001b[K    14% |████▋                           | 296kB 8.0MB/s eta 0:00:01\r\u001b[K    14% |████▊                           | 307kB 52.4MB/s eta 0:00:01\r\u001b[K    15% |████▉                           | 317kB 54.6MB/s eta 0:00:01\r\u001b[K    15% |█████                           | 327kB 54.3MB/s eta 0:00:01\r\u001b[K    16% |█████▏                          | 337kB 53.8MB/s eta 0:00:01\r\u001b[K    16% |█████▍                          | 348kB 47.4MB/s eta 0:00:01\r\u001b[K    17% |█████▌                          | 358kB 47.7MB/s eta 0:00:01\r\u001b[K    17% |█████▊                          | 368kB 54.9MB/s eta 0:00:01\r\u001b[K    18% |█████▉                          | 378kB 55.3MB/s eta 0:00:01\r\u001b[K    18% |██████                          | 389kB 55.3MB/s eta 0:00:01\r\u001b[K    19% |██████▏                         | 399kB 55.3MB/s eta 0:00:01\r\u001b[K    19% |██████▎                         | 409kB 54.2MB/s eta 0:00:01\r\u001b[K    20% |██████▌                         | 419kB 55.0MB/s eta 0:00:01\r\u001b[K    20% |██████▋                         | 430kB 10.4MB/s eta 0:00:01\r\u001b[K    21% |██████▊                         | 440kB 10.3MB/s eta 0:00:01\r\u001b[K    21% |███████                         | 450kB 10.3MB/s eta 0:00:01\r\u001b[K    22% |███████                         | 460kB 10.2MB/s eta 0:00:01\r\u001b[K    22% |███████▎                        | 471kB 10.2MB/s eta 0:00:01\r\u001b[K    23% |███████▍                        | 481kB 10.2MB/s eta 0:00:01\r\u001b[K    23% |███████▋                        | 491kB 10.2MB/s eta 0:00:01\r\u001b[K    24% |███████▊                        | 501kB 10.2MB/s eta 0:00:01\r\u001b[K    24% |███████▉                        | 512kB 10.0MB/s eta 0:00:01\r\u001b[K    25% |████████                        | 522kB 10.0MB/s eta 0:00:01\r\u001b[K    25% |████████▏                       | 532kB 47.7MB/s eta 0:00:01\r\u001b[K    26% |████████▍                       | 542kB 49.4MB/s eta 0:00:01\r\u001b[K    26% |████████▌                       | 552kB 58.2MB/s eta 0:00:01\r\u001b[K    27% |████████▊                       | 563kB 60.3MB/s eta 0:00:01\r\u001b[K    27% |████████▉                       | 573kB 59.4MB/s eta 0:00:01\r\u001b[K    28% |█████████                       | 583kB 60.3MB/s eta 0:00:01\r\u001b[K    28% |█████████▏                      | 593kB 59.8MB/s eta 0:00:01\r\u001b[K    29% |█████████▎                      | 604kB 59.2MB/s eta 0:00:01\r\u001b[K    29% |█████████▌                      | 614kB 65.4MB/s eta 0:00:01\r\u001b[K    30% |█████████▋                      | 624kB 64.3MB/s eta 0:00:01\r\u001b[K    30% |█████████▊                      | 634kB 63.8MB/s eta 0:00:01\r\u001b[K    31% |██████████                      | 645kB 63.6MB/s eta 0:00:01\r\u001b[K    31% |██████████                      | 655kB 61.8MB/s eta 0:00:01\r\u001b[K    32% |██████████▎                     | 665kB 47.0MB/s eta 0:00:01\r\u001b[K    32% |██████████▍                     | 675kB 47.0MB/s eta 0:00:01\r\u001b[K    32% |██████████▋                     | 686kB 47.4MB/s eta 0:00:01\r\u001b[K    33% |██████████▊                     | 696kB 47.4MB/s eta 0:00:01\r\u001b[K    33% |██████████▉                     | 706kB 47.3MB/s eta 0:00:01\r\u001b[K    34% |███████████                     | 716kB 47.5MB/s eta 0:00:01\r\u001b[K    34% |███████████▏                    | 727kB 47.5MB/s eta 0:00:01\r\u001b[K    35% |███████████▍                    | 737kB 47.3MB/s eta 0:00:01\r\u001b[K    35% |███████████▌                    | 747kB 47.6MB/s eta 0:00:01\r\u001b[K    36% |███████████▋                    | 757kB 47.6MB/s eta 0:00:01\r\u001b[K    36% |███████████▉                    | 768kB 62.8MB/s eta 0:00:01\r\u001b[K    37% |████████████                    | 778kB 62.7MB/s eta 0:00:01\r\u001b[K    37% |████████████▏                   | 788kB 60.7MB/s eta 0:00:01\r\u001b[K    38% |████████████▎                   | 798kB 61.3MB/s eta 0:00:01\r\u001b[K    38% |████████████▌                   | 808kB 61.5MB/s eta 0:00:01\r\u001b[K    39% |████████████▋                   | 819kB 60.6MB/s eta 0:00:01\r\u001b[K    39% |████████████▊                   | 829kB 61.7MB/s eta 0:00:01\r\u001b[K    40% |█████████████                   | 839kB 61.9MB/s eta 0:00:01\r\u001b[K    40% |█████████████                   | 849kB 61.2MB/s eta 0:00:01\r\u001b[K    41% |█████████████▎                  | 860kB 54.5MB/s eta 0:00:01\r\u001b[K    41% |█████████████▍                  | 870kB 53.9MB/s eta 0:00:01\r\u001b[K    42% |█████████████▌                  | 880kB 55.2MB/s eta 0:00:01\r\u001b[K    42% |█████████████▊                  | 890kB 56.6MB/s eta 0:00:01\r\u001b[K    43% |█████████████▉                  | 901kB 18.0MB/s eta 0:00:01\r\u001b[K    43% |██████████████                  | 911kB 17.7MB/s eta 0:00:01\r\u001b[K    44% |██████████████▏                 | 921kB 17.5MB/s eta 0:00:01\r\u001b[K    44% |██████████████▍                 | 931kB 17.4MB/s eta 0:00:01\r\u001b[K    45% |██████████████▌                 | 942kB 17.3MB/s eta 0:00:01\r\u001b[K    45% |██████████████▋                 | 952kB 17.2MB/s eta 0:00:01\r\u001b[K    46% |██████████████▉                 | 962kB 18.0MB/s eta 0:00:01\r\u001b[K    46% |███████████████                 | 972kB 18.1MB/s eta 0:00:01\r\u001b[K    47% |███████████████▏                | 983kB 18.0MB/s eta 0:00:01\r\u001b[K    47% |███████████████▎                | 993kB 17.9MB/s eta 0:00:01\r\u001b[K    48% |███████████████▍                | 1.0MB 56.2MB/s eta 0:00:01\r\u001b[K    48% |███████████████▋                | 1.0MB 56.7MB/s eta 0:00:01\r\u001b[K    49% |███████████████▊                | 1.0MB 58.8MB/s eta 0:00:01\r\u001b[K    49% |████████████████                | 1.0MB 59.6MB/s eta 0:00:01\r\u001b[K    50% |████████████████                | 1.0MB 61.3MB/s eta 0:00:01\r\u001b[K    50% |████████████████▎               | 1.1MB 62.3MB/s eta 0:00:01\r\u001b[K    51% |████████████████▍               | 1.1MB 61.0MB/s eta 0:00:01\r\u001b[K    51% |████████████████▌               | 1.1MB 60.7MB/s eta 0:00:01\r\u001b[K    52% |████████████████▊               | 1.1MB 60.3MB/s eta 0:00:01\r\u001b[K    52% |████████████████▉               | 1.1MB 42.9MB/s eta 0:00:01\r\u001b[K    53% |█████████████████               | 1.1MB 42.3MB/s eta 0:00:01\r\u001b[K    53% |█████████████████▏              | 1.1MB 43.4MB/s eta 0:00:01\r\u001b[K    54% |█████████████████▍              | 1.1MB 44.1MB/s eta 0:00:01\r\u001b[K    54% |█████████████████▌              | 1.1MB 44.2MB/s eta 0:00:01\r\u001b[K    55% |█████████████████▋              | 1.1MB 44.0MB/s eta 0:00:01\r\u001b[K    55% |█████████████████▉              | 1.2MB 44.4MB/s eta 0:00:01\r\u001b[K    56% |██████████████████              | 1.2MB 44.3MB/s eta 0:00:01\r\u001b[K    56% |██████████████████▏             | 1.2MB 44.3MB/s eta 0:00:01\r\u001b[K    57% |██████████████████▎             | 1.2MB 44.2MB/s eta 0:00:01\r\u001b[K    57% |██████████████████▍             | 1.2MB 61.9MB/s eta 0:00:01\r\u001b[K    58% |██████████████████▋             | 1.2MB 63.3MB/s eta 0:00:01\r\u001b[K    58% |██████████████████▊             | 1.2MB 62.8MB/s eta 0:00:01\r\u001b[K    59% |███████████████████             | 1.2MB 61.8MB/s eta 0:00:01\r\u001b[K    59% |███████████████████             | 1.2MB 62.4MB/s eta 0:00:01\r\u001b[K    60% |███████████████████▎            | 1.2MB 62.2MB/s eta 0:00:01\r\u001b[K    60% |███████████████████▍            | 1.3MB 62.1MB/s eta 0:00:01\r\u001b[K    61% |███████████████████▌            | 1.3MB 62.5MB/s eta 0:00:01\r\u001b[K    61% |███████████████████▊            | 1.3MB 61.7MB/s eta 0:00:01\r\u001b[K    62% |███████████████████▉            | 1.3MB 62.5MB/s eta 0:00:01\r\u001b[K    62% |████████████████████            | 1.3MB 63.5MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▏           | 1.3MB 62.1MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▎           | 1.3MB 62.2MB/s eta 0:00:01\r\u001b[K    64% |████████████████████▌           | 1.3MB 62.4MB/s eta 0:00:01\r\u001b[K    64% |████████████████████▋           | 1.3MB 62.0MB/s eta 0:00:01\r\u001b[K    65% |████████████████████▉           | 1.4MB 62.1MB/s eta 0:00:01\r\u001b[K    65% |█████████████████████           | 1.4MB 61.6MB/s eta 0:00:01\r\u001b[K    65% |█████████████████████▏          | 1.4MB 61.8MB/s eta 0:00:01\r\u001b[K    66% |█████████████████████▎          | 1.4MB 61.7MB/s eta 0:00:01\r\u001b[K    66% |█████████████████████▍          | 1.4MB 61.1MB/s eta 0:00:01\r\u001b[K    67% |█████████████████████▋          | 1.4MB 39.0MB/s eta 0:00:01\r\u001b[K    67% |█████████████████████▊          | 1.4MB 39.0MB/s eta 0:00:01\r\u001b[K    68% |██████████████████████          | 1.4MB 39.6MB/s eta 0:00:01\r\u001b[K    68% |██████████████████████          | 1.4MB 39.9MB/s eta 0:00:01\r\u001b[K    69% |██████████████████████▏         | 1.4MB 40.0MB/s eta 0:00:01\r\u001b[K    69% |██████████████████████▍         | 1.5MB 40.0MB/s eta 0:00:01\r\u001b[K    70% |██████████████████████▌         | 1.5MB 40.2MB/s eta 0:00:01\r\u001b[K    70% |██████████████████████▊         | 1.5MB 39.5MB/s eta 0:00:01\r\u001b[K    71% |██████████████████████▉         | 1.5MB 39.5MB/s eta 0:00:01\r\u001b[K    71% |███████████████████████         | 1.5MB 38.9MB/s eta 0:00:01\r\u001b[K    72% |███████████████████████▏        | 1.5MB 60.2MB/s eta 0:00:01\r\u001b[K    72% |███████████████████████▎        | 1.5MB 60.9MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▌        | 1.5MB 59.2MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▋        | 1.5MB 58.7MB/s eta 0:00:01\r\u001b[K    74% |███████████████████████▉        | 1.5MB 58.3MB/s eta 0:00:01\r\u001b[K    74% |████████████████████████        | 1.6MB 53.7MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████▏       | 1.6MB 53.1MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████▎       | 1.6MB 53.1MB/s eta 0:00:01\r\u001b[K    76% |████████████████████████▍       | 1.6MB 41.4MB/s eta 0:00:01\r\u001b[K    76% |████████████████████████▋       | 1.6MB 41.8MB/s eta 0:00:01\r\u001b[K    77% |████████████████████████▊       | 1.6MB 42.1MB/s eta 0:00:01\r\u001b[K    77% |█████████████████████████       | 1.6MB 42.9MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████       | 1.6MB 43.7MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████▏      | 1.6MB 43.7MB/s eta 0:00:01\r\u001b[K    79% |█████████████████████████▍      | 1.6MB 43.9MB/s eta 0:00:01\r\u001b[K    79% |█████████████████████████▌      | 1.7MB 46.9MB/s eta 0:00:01\r\u001b[K    80% |█████████████████████████▊      | 1.7MB 47.7MB/s eta 0:00:01\r\u001b[K    80% |█████████████████████████▉      | 1.7MB 48.5MB/s eta 0:00:01\r\u001b[K    81% |██████████████████████████      | 1.7MB 65.6MB/s eta 0:00:01\r\u001b[K    81% |██████████████████████████▏     | 1.7MB 67.0MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▎     | 1.7MB 66.8MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▌     | 1.7MB 65.2MB/s eta 0:00:01\r\u001b[K    83% |██████████████████████████▋     | 1.7MB 65.1MB/s eta 0:00:01\r\u001b[K    83% |██████████████████████████▉     | 1.7MB 64.8MB/s eta 0:00:01\r\u001b[K    84% |███████████████████████████     | 1.8MB 65.5MB/s eta 0:00:01\r\u001b[K    84% |███████████████████████████     | 1.8MB 65.8MB/s eta 0:00:01\r\u001b[K    85% |███████████████████████████▎    | 1.8MB 65.1MB/s eta 0:00:01\r\u001b[K    85% |███████████████████████████▍    | 1.8MB 65.6MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▋    | 1.8MB 66.1MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▊    | 1.8MB 65.1MB/s eta 0:00:01\r\u001b[K    87% |████████████████████████████    | 1.8MB 65.8MB/s eta 0:00:01\r\u001b[K    87% |████████████████████████████    | 1.8MB 66.4MB/s eta 0:00:01\r\u001b[K    88% |████████████████████████████▏   | 1.8MB 65.8MB/s eta 0:00:01\r\u001b[K    88% |████████████████████████████▍   | 1.8MB 64.8MB/s eta 0:00:01\r\u001b[K    89% |████████████████████████████▌   | 1.9MB 44kB/s eta 0:00:06\r\u001b[K    89% |████████████████████████████▊   | 1.9MB 44kB/s eta 0:00:05\r\u001b[K    90% |████████████████████████████▉   | 1.9MB 44kB/s eta 0:00:05\r\u001b[K    90% |█████████████████████████████   | 1.9MB 44kB/s eta 0:00:05\r\u001b[K    91% |█████████████████████████████▏  | 1.9MB 44kB/s eta 0:00:05\r\u001b[K    91% |█████████████████████████████▎  | 1.9MB 44kB/s eta 0:00:04\r\u001b[K    92% |█████████████████████████████▌  | 1.9MB 44kB/s eta 0:00:04\r\u001b[K    92% |█████████████████████████████▋  | 1.9MB 44kB/s eta 0:00:04\r\u001b[K    93% |█████████████████████████████▉  | 1.9MB 44kB/s eta 0:00:04\r\u001b[K    93% |██████████████████████████████  | 1.9MB 44kB/s eta 0:00:03\r\u001b[K    94% |██████████████████████████████  | 2.0MB 41.1MB/s eta 0:00:01\r\u001b[K    94% |██████████████████████████████▎ | 2.0MB 41.8MB/s eta 0:00:01\r\u001b[K    95% |██████████████████████████████▍ | 2.0MB 41.7MB/s eta 0:00:01\r\u001b[K    95% |██████████████████████████████▋ | 2.0MB 42.4MB/s eta 0:00:01\r\u001b[K    96% |██████████████████████████████▊ | 2.0MB 42.1MB/s eta 0:00:01\r\u001b[K    96% |██████████████████████████████▉ | 2.0MB 42.1MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████ | 2.0MB 29.2MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████▏| 2.0MB 29.8MB/s eta 0:00:01\r\u001b[K    98% |███████████████████████████████▍| 2.0MB 30.5MB/s eta 0:00:01\r\u001b[K    98% |███████████████████████████████▌| 2.0MB 31.0MB/s eta 0:00:01\r\u001b[K    98% |███████████████████████████████▊| 2.1MB 32.2MB/s eta 0:00:01\r\u001b[K    99% |███████████████████████████████▉| 2.1MB 33.1MB/s eta 0:00:01\r\u001b[K    99% |████████████████████████████████| 2.1MB 34.5MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 2.1MB 12.0MB/s \n","\u001b[?25hInstalling collected packages: deepchem\n","Successfully installed deepchem-2.1.1.dev353\n","Collecting simdna\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/9d/1bbc1b684ea5edcfbf4f3d46127bc47d9aa5237408d046607c7e2af11772/simdna-0.4.2.tar.gz (633kB)\n","\u001b[K    100% |████████████████████████████████| 634kB 22.7MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.9 in /usr/local/lib/python3.6/dist-packages (from simdna) (1.14.6)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from simdna) (3.0.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from simdna) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->simdna) (0.10.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->simdna) (2.5.3)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->simdna) (2.3.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->simdna) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->simdna) (1.11.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->simdna) (40.8.0)\n","Building wheels for collected packages: simdna\n","  Building wheel for simdna (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/f8/f1/4a/ae406fdaa872d25ed14ea18b4a72b6ae03d7f5eab56e02976f\n","Successfully built simdna\n","Installing collected packages: simdna\n","Successfully installed simdna-0.4.2\n","Collecting nosetests\n","\u001b[31m  Could not find a version that satisfies the requirement nosetests (from versions: )\u001b[0m\n","\u001b[31mNo matching distribution found for nosetests\u001b[0m\n","--2019-03-18 16:19:10--  https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n","Resolving repo.continuum.io (repo.continuum.io)... 104.18.200.79, 104.18.201.79, 2606:4700::6812:c84f, ...\n","Connecting to repo.continuum.io (repo.continuum.io)|104.18.200.79|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 69826864 (67M) [application/x-sh]\n","Saving to: ‘Miniconda3-latest-Linux-x86_64.sh’\n","\n","Miniconda3-latest-L 100%[===================>]  66.59M   112MB/s    in 0.6s    \n","\n","2019-03-18 16:19:10 (112 MB/s) - ‘Miniconda3-latest-Linux-x86_64.sh’ saved [69826864/69826864]\n","\n","PREFIX=/usr/local\n","reinstalling: python-3.7.1-h0371630_7 ...\n","Python 3.7.1\n","reinstalling: ca-certificates-2018.03.07-0 ...\n","reinstalling: conda-env-2.6.0-1 ...\n","reinstalling: libgcc-ng-8.2.0-hdf63c60_1 ...\n","reinstalling: libstdcxx-ng-8.2.0-hdf63c60_1 ...\n","reinstalling: libffi-3.2.1-hd88cf55_4 ...\n","reinstalling: ncurses-6.1-he6710b0_1 ...\n","reinstalling: openssl-1.1.1a-h7b6447c_0 ...\n","reinstalling: xz-5.2.4-h14c3975_4 ...\n","reinstalling: yaml-0.1.7-had09818_2 ...\n","reinstalling: zlib-1.2.11-h7b6447c_3 ...\n","reinstalling: libedit-3.1.20170329-h6b74fdf_2 ...\n","reinstalling: readline-7.0-h7b6447c_5 ...\n","reinstalling: tk-8.6.8-hbc83047_0 ...\n","reinstalling: sqlite-3.26.0-h7b6447c_0 ...\n","reinstalling: asn1crypto-0.24.0-py37_0 ...\n","reinstalling: certifi-2018.11.29-py37_0 ...\n","reinstalling: chardet-3.0.4-py37_1 ...\n","reinstalling: idna-2.8-py37_0 ...\n","reinstalling: pycosat-0.6.3-py37h14c3975_0 ...\n","reinstalling: pycparser-2.19-py37_0 ...\n","reinstalling: pysocks-1.6.8-py37_0 ...\n","reinstalling: ruamel_yaml-0.15.46-py37h14c3975_0 ...\n","reinstalling: six-1.12.0-py37_0 ...\n","reinstalling: cffi-1.11.5-py37he75722e_1 ...\n","reinstalling: setuptools-40.6.3-py37_0 ...\n","reinstalling: cryptography-2.4.2-py37h1ba5d50_0 ...\n","reinstalling: wheel-0.32.3-py37_0 ...\n","reinstalling: pip-18.1-py37_0 ...\n","reinstalling: pyopenssl-18.0.0-py37_0 ...\n","reinstalling: urllib3-1.24.1-py37_0 ...\n","reinstalling: requests-2.21.0-py37_0 ...\n","reinstalling: conda-4.5.12-py37_0 ...\n","installation finished.\n","WARNING:\n","    You currently have a PYTHONPATH environment variable set. This may cause\n","    unexpected behavior when running the Python interpreter in Miniconda3.\n","    For best results, please verify that your PYTHONPATH only points to\n","    directories of packages that are compatible with the Python interpreter\n","    in Miniconda3: /usr/local\n","\n","real\t0m15.573s\n","user\t0m13.735s\n","sys\t0m3.150s\n","Solving environment: ...working... done\n","\n","## Package Plan ##\n","\n","  environment location: /usr/local\n","\n","  added / updated specs: \n","    - rdkit\n","\n","\n","The following packages will be downloaded:\n","\n","    package                    |            build\n","    ---------------------------|-----------------\n","    freetype-2.10.0            |       he983fc9_0         885 KB  conda-forge\n","    pillow-5.3.0               |py37h00a061d_1000         595 KB  conda-forge\n","    xorg-renderproto-0.11.1    |    h14c3975_1002           8 KB  conda-forge\n","    pixman-0.34.0              |    h14c3975_1003         595 KB  conda-forge\n","    bzip2-1.0.6                |    h14c3975_1002         415 KB  conda-forge\n","    libxcb-1.13                |    h14c3975_1002         396 KB  conda-forge\n","    certifi-2019.3.9           |           py37_0         149 KB  conda-forge\n","    libcblas-3.8.0             |       4_openblas           6 KB  conda-forge\n","    openssl-1.1.1b             |       h14c3975_1         4.0 MB  conda-forge\n","    xorg-libx11-1.6.7          |    h14c3975_1000         940 KB  conda-forge\n","    xorg-libxau-1.0.9          |       h14c3975_0          13 KB  conda-forge\n","    xorg-xproto-7.0.31         |    h14c3975_1007          72 KB  conda-forge\n","    xorg-libxdmcp-1.1.2        |    h14c3975_1007          18 KB  conda-forge\n","    libtiff-4.0.10             |    h648cc4a_1001         592 KB  conda-forge\n","    pycairo-1.18.0             |   py37h2a1e443_0          74 KB\n","    libiconv-1.15              |    h14c3975_1004         2.0 MB  conda-forge\n","    numpy-1.16.2               |   py37h8b7e671_1         4.3 MB  conda-forge\n","    fontconfig-2.13.1          |    he4413a7_1000         327 KB  conda-forge\n","    xorg-libice-1.0.9          |    h516909a_1004          58 KB  conda-forge\n","    xorg-xextproto-7.3.0       |    h14c3975_1002          27 KB  conda-forge\n","    libxml2-2.9.8              |    h143f9aa_1005         2.0 MB  conda-forge\n","    libpng-1.6.36              |    h84994c4_1000         303 KB  conda-forge\n","    icu-58.2                   |    hf484d3e_1000        22.6 MB  conda-forge\n","    ca-certificates-2019.3.9   |       hecc5488_0         146 KB  conda-forge\n","    jpeg-9c                    |    h14c3975_1001         251 KB  conda-forge\n","    xorg-libxext-1.3.3         |    h516909a_1004          47 KB  conda-forge\n","    olefile-0.46               |             py_0          31 KB  conda-forge\n","    libblas-3.8.0              |       4_openblas           6 KB  conda-forge\n","    pcre-8.41                  |    hf484d3e_1003         249 KB  conda-forge\n","    liblapack-3.8.0            |       4_openblas           6 KB  conda-forge\n","    gettext-0.19.8.1           |    h9745a5d_1001         3.7 MB  conda-forge\n","    xorg-kbproto-1.0.7         |    h14c3975_1002          26 KB  conda-forge\n","    pthread-stubs-0.4          |    h14c3975_1001           5 KB  conda-forge\n","    openblas-0.3.5             |    h9ac9557_1001        15.8 MB  conda-forge\n","    xorg-libxrender-0.9.10     |    h516909a_1002          31 KB  conda-forge\n","    pandas-0.24.2              |   py37hf484d3e_0        11.1 MB  conda-forge\n","    libgfortran-ng-7.2.0       |       hdf63c60_3         1.2 MB  conda-forge\n","    pytz-2018.9                |             py_0         229 KB  conda-forge\n","    boost-1.68.0               |py37h8619c78_1001         310 KB  conda-forge\n","    conda-4.6.8                |           py37_0         872 KB  conda-forge\n","    xorg-libsm-1.2.3           |    h4937e3b_1000          25 KB  conda-forge\n","    boost-cpp-1.68.0           |    h11c811c_1000        20.5 MB  conda-forge\n","    cairo-1.16.0               |    ha4e643d_1000         1.5 MB  conda-forge\n","    rdkit-2018.09.2            |   py37h270f4b7_0        20.0 MB  conda-forge\n","    glib-2.58.3                |    hf63aee3_1001         3.3 MB  conda-forge\n","    python-dateutil-2.8.0      |             py_0         219 KB  conda-forge\n","    libuuid-2.32.1             |    h14c3975_1000          26 KB  conda-forge\n","    ------------------------------------------------------------\n","                                           Total:       119.7 MB\n","\n","The following NEW packages will be INSTALLED:\n","\n","    boost:            1.68.0-py37h8619c78_1001 conda-forge\n","    boost-cpp:        1.68.0-h11c811c_1000     conda-forge\n","    bzip2:            1.0.6-h14c3975_1002      conda-forge\n","    cairo:            1.16.0-ha4e643d_1000     conda-forge\n","    fontconfig:       2.13.1-he4413a7_1000     conda-forge\n","    freetype:         2.10.0-he983fc9_0        conda-forge\n","    gettext:          0.19.8.1-h9745a5d_1001   conda-forge\n","    glib:             2.58.3-hf63aee3_1001     conda-forge\n","    icu:              58.2-hf484d3e_1000       conda-forge\n","    jpeg:             9c-h14c3975_1001         conda-forge\n","    libblas:          3.8.0-4_openblas         conda-forge\n","    libcblas:         3.8.0-4_openblas         conda-forge\n","    libgfortran-ng:   7.2.0-hdf63c60_3         conda-forge\n","    libiconv:         1.15-h14c3975_1004       conda-forge\n","    liblapack:        3.8.0-4_openblas         conda-forge\n","    libpng:           1.6.36-h84994c4_1000     conda-forge\n","    libtiff:          4.0.10-h648cc4a_1001     conda-forge\n","    libuuid:          2.32.1-h14c3975_1000     conda-forge\n","    libxcb:           1.13-h14c3975_1002       conda-forge\n","    libxml2:          2.9.8-h143f9aa_1005      conda-forge\n","    numpy:            1.16.2-py37h8b7e671_1    conda-forge\n","    olefile:          0.46-py_0                conda-forge\n","    openblas:         0.3.5-h9ac9557_1001      conda-forge\n","    pandas:           0.24.2-py37hf484d3e_0    conda-forge\n","    pcre:             8.41-hf484d3e_1003       conda-forge\n","    pillow:           5.3.0-py37h00a061d_1000  conda-forge\n","    pixman:           0.34.0-h14c3975_1003     conda-forge\n","    pthread-stubs:    0.4-h14c3975_1001        conda-forge\n","    pycairo:          1.18.0-py37h2a1e443_0               \n","    python-dateutil:  2.8.0-py_0               conda-forge\n","    pytz:             2018.9-py_0              conda-forge\n","    rdkit:            2018.09.2-py37h270f4b7_0 conda-forge\n","    xorg-kbproto:     1.0.7-h14c3975_1002      conda-forge\n","    xorg-libice:      1.0.9-h516909a_1004      conda-forge\n","    xorg-libsm:       1.2.3-h4937e3b_1000      conda-forge\n","    xorg-libx11:      1.6.7-h14c3975_1000      conda-forge\n","    xorg-libxau:      1.0.9-h14c3975_0         conda-forge\n","    xorg-libxdmcp:    1.1.2-h14c3975_1007      conda-forge\n","    xorg-libxext:     1.3.3-h516909a_1004      conda-forge\n","    xorg-libxrender:  0.9.10-h516909a_1002     conda-forge\n","    xorg-renderproto: 0.11.1-h14c3975_1002     conda-forge\n","    xorg-xextproto:   7.3.0-h14c3975_1002      conda-forge\n","    xorg-xproto:      7.0.31-h14c3975_1007     conda-forge\n","\n","The following packages will be UPDATED:\n","\n","    ca-certificates:  2018.03.07-0                         --> 2019.3.9-hecc5488_0 conda-forge\n","    certifi:          2018.11.29-py37_0                    --> 2019.3.9-py37_0     conda-forge\n","    conda:            4.5.12-py37_0                        --> 4.6.8-py37_0        conda-forge\n","    openssl:          1.1.1a-h7b6447c_0                    --> 1.1.1b-h14c3975_1   conda-forge\n","\n","Preparing transaction: ...working... done\n","Verifying transaction: ...working... done\n","Executing transaction: ...working... done\n","\n","real\t1m23.659s\n","user\t1m12.176s\n","sys\t0m6.249s\n"],"name":"stdout"}]},{"metadata":{"id":"l2ZBudqz17Kh","colab_type":"text"},"cell_type":"markdown","source":["In this section we download the packets needed for run a tensorboard remotly on colab and get a link at ngrok.com to access to the tensorboard trought the colab's firewall.\n","![alt text](https://gitcdn.xyz/cdn/Tony607/blog_statics/d425c3fe4cf0d92067572e25ae6cc3198d51936b//images/ngrok/ngrok.jpg)"]},{"metadata":{"id":"X73uV25F08pr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":251},"outputId":"55433708-d78a-4087-96fa-48467773d70f","executionInfo":{"status":"ok","timestamp":1552927252339,"user_tz":-60,"elapsed":12828,"user":{"displayName":"simone viozzi","photoUrl":"https://lh4.googleusercontent.com/-lHfKAhqXixo/AAAAAAAAAAI/AAAAAAAABF8/q48tw94xHaI/s64/photo.jpg","userId":"00173786927897941269"}}},"cell_type":"code","source":["!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","!unzip ngrok-stable-linux-amd64.zip"],"execution_count":14,"outputs":[{"output_type":"stream","text":["--2019-03-18 16:40:42--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","Resolving bin.equinox.io (bin.equinox.io)... 35.173.3.255, 54.173.32.212, 52.202.60.111, ...\n","Connecting to bin.equinox.io (bin.equinox.io)|35.173.3.255|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 14910739 (14M) [application/octet-stream]\n","Saving to: ‘ngrok-stable-linux-amd64.zip’\n","\n","\r          ngrok-sta   0%[                    ]       0  --.-KB/s               \r         ngrok-stab  71%[=============>      ]  10.16M  50.6MB/s               \rngrok-stable-linux- 100%[===================>]  14.22M  52.6MB/s    in 0.3s    \n","\n","2019-03-18 16:40:42 (52.6 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [14910739/14910739]\n","\n","Archive:  ngrok-stable-linux-amd64.zip\n","  inflating: ngrok                   \n"],"name":"stdout"}]},{"metadata":{"id":"z5rXtmcd17iv","colab_type":"text"},"cell_type":"markdown","source":["here we launch the tensorboard in background and  set the directory for saving session log.\n"]},{"metadata":{"id":"iMyAOfmG09cR","colab_type":"code","colab":{}},"cell_type":"code","source":["LOG_DIR = './log'\n","get_ipython().system_raw(\n","    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n","    .format(LOG_DIR)\n",")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KssEpVBS2GwZ","colab_type":"text"},"cell_type":"markdown","source":["Then, we can run ngrok to tunnel TensorBoard port 6006 to the outside world. This command also runs in the background.\n"]},{"metadata":{"id":"dzJYm_O12zKu","colab_type":"code","colab":{}},"cell_type":"code","source":["get_ipython().system_raw('./ngrok http 6006 &')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jGQdt_OC2zvi","colab_type":"text"},"cell_type":"markdown","source":["Now we get the public URL where we can access the colab TensorBoard.\n","It's important keep in mind that the training have to start before you can see somthing in. \n"]},{"metadata":{"id":"MBPAVeUu2OH1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"3bac3e99-d612-4c47-b750-2a539a7f9112","executionInfo":{"status":"ok","timestamp":1552927300886,"user_tz":-60,"elapsed":10475,"user":{"displayName":"simone viozzi","photoUrl":"https://lh4.googleusercontent.com/-lHfKAhqXixo/AAAAAAAAAAI/AAAAAAAABF8/q48tw94xHaI/s64/photo.jpg","userId":"00173786927897941269"}}},"cell_type":"code","source":["! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""],"execution_count":17,"outputs":[{"output_type":"stream","text":["https://912717e7.ngrok.io\n"],"name":"stdout"}]},{"metadata":{"id":"qM_-1V682PFP","colab_type":"text"},"cell_type":"markdown","source":["**Import libraries:**\n","\n","we start importing **numpy**, a python library to work efficiently with tensors, folowed by **tensorflow** and  **sklearn.metrics**, the libraries that contain the toolkit for work with data, networks and in this case tools for getting the performance of model.\n","**Matplotlib** for display data and in the end **deepchem** that contain the tox21 dataset.\n","\n","**Tox21**, is a unique collaboration between several federal agencies to develop new ways to rapidly test whether substances adversely affect human health. This dataset consists of a set of 10,000 molecules represented vectorially  tested for interaction with the androgen receptor.\n"]},{"metadata":{"id":"UyXZ1Je7TL9T","colab_type":"code","colab":{}},"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import sys\n","import os\n","sys.path.append('/usr/local/lib/python3.7/site-packages/')\n","import numpy as np\n","np.random.seed(456)\n","import  tensorflow as tf\n","tf.set_random_seed(456)\n","import deepchem.molnet as dc\n","from sklearn.metrics import accuracy_score"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MHTx4slHfmtK","colab_type":"text"},"cell_type":"markdown","source":["Now we load tox21 dataset and prepare it, removing usefull data."]},{"metadata":{"id":"bWFBDvA3mzxl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":431},"outputId":"af70232b-8b2b-4d69-a780-bf8794fd03eb","executionInfo":{"status":"ok","timestamp":1552926135072,"user_tz":-60,"elapsed":35545,"user":{"displayName":"simone viozzi","photoUrl":"https://lh4.googleusercontent.com/-lHfKAhqXixo/AAAAAAAAAAI/AAAAAAAABF8/q48tw94xHaI/s64/photo.jpg","userId":"00173786927897941269"}}},"cell_type":"code","source":["_, (train, valid, test), _ = dc.load_tox21()\n","train_X, train_y, train_w = train.X, train.y, train.w\n","valid_X, valid_y, valid_w = valid.X, valid.y, valid.w\n","test_X, test_y, test_w = test.X, test.y, test.w\n","\n","\n","# Remove extra tasks\n","train_y = train_y[:, 0]\n","valid_y = valid_y[:, 0]\n","test_y = test_y[:, 0]\n","train_w = train_w[:, 0]\n","valid_w = valid_w[:, 0]\n","test_w = test_w[:, 0]\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Loading raw samples now.\n","shard_size: 8192\n","About to start loading CSV from /tmp/tox21.csv.gz\n","Loading shard 1 of size 8192.\n","Featurizing sample 0\n","Featurizing sample 1000\n","Featurizing sample 2000\n","Featurizing sample 3000\n","Featurizing sample 4000\n","Featurizing sample 5000\n","Featurizing sample 6000\n","Featurizing sample 7000\n","TIMING: featurizing shard 0 took 28.765 s\n","TIMING: dataset construction took 29.264 s\n","Loading dataset from disk.\n","TIMING: dataset construction took 0.448 s\n","Loading dataset from disk.\n","TIMING: dataset construction took 0.408 s\n","Loading dataset from disk.\n","TIMING: dataset construction took 0.211 s\n","Loading dataset from disk.\n","TIMING: dataset construction took 0.209 s\n","Loading dataset from disk.\n"],"name":"stdout"}]},{"metadata":{"id":"V4bke0dV2EiV","colab_type":"text"},"cell_type":"markdown","source":["In this section we declare the structure of the tensor graph, which represents the model."]},{"metadata":{"id":"Wl1zIsI2zBVq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":91},"outputId":"9a967f3f-ba30-4023-9c74-dc5c360c9116","executionInfo":{"status":"ok","timestamp":1552927347075,"user_tz":-60,"elapsed":1482,"user":{"displayName":"simone viozzi","photoUrl":"https://lh4.googleusercontent.com/-lHfKAhqXixo/AAAAAAAAAAI/AAAAAAAABF8/q48tw94xHaI/s64/photo.jpg","userId":"00173786927897941269"}}},"cell_type":"code","source":["# Generate tensorflow graph\n","#general parameters\n","d = 1024\n","n_hidden = 100\n","n_hidden2 = 80\n","n_hidden3 = 30\n","learning_rate = .001\n","n_epochs = 100\n","batch_size = 100\n","\n","#dataset\n","with tf.name_scope(\"dataset\"):\n","  x = tf.placeholder(tf.float32, (None, d))\n","  y = tf.placeholder(tf.float32, (None,))\n","  \n","  \n","with tf.name_scope(\"hidden-layer1\"):\n","  W = tf.Variable(tf.random_normal((d, n_hidden)))\n","  b = tf.Variable(tf.random_normal((n_hidden,)))\n","  x_hidden = tf.nn.relu(tf.matmul(x, W) + b)\n","  \n","with tf.name_scope(\"hidden-layer2\"):\n","  W = tf.Variable(tf.random_normal((n_hidden, n_hidden2)))\n","  b = tf.Variable(tf.random_normal((n_hidden2,)))\n","  x_hidden2 = tf.nn.relu(tf.matmul(x_hidden, W) + b)\n","  \n","with tf.name_scope(\"hidden-layer3\"):\n","  W = tf.Variable(tf.random_normal((n_hidden2, n_hidden3)))\n","  b = tf.Variable(tf.random_normal((n_hidden3,)))\n","  x_hidden3 = tf.nn.relu(tf.matmul(x_hidden2, W) + b)\n","  \n","with tf.name_scope(\"output\"):\n","  W = tf.Variable(tf.random_normal((n_hidden3, 1)))\n","  b = tf.Variable(tf.random_normal((1,)))\n","  y_logit = tf.matmul(x_hidden3, W) + b\n","  \n","  # the sigmoid gives the class probability of 1\n","  y_one_prob = tf.sigmoid(y_logit)\n","  # Rounding P(y=1) will give the correct prediction.\n","  y_pred = tf.round(y_one_prob)\n","  \n"," #setting of loss function\n","with tf.name_scope(\"loss\"):\n","  \n","  # Compute the cross-entropy term for each datapoint\n","  y_expand = tf.expand_dims(y, 1)\n","  entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_logit, labels=y_expand)\n","  \n","  # Sum all contributions\n","  l = tf.reduce_sum(entropy)\n","\n","#setting of optimization algorithm\n","with tf.name_scope(\"optim\"):\n","  train_op = tf.train.AdamOptimizer(learning_rate).minimize(l)\n","\n","#setting variables to show in tensorboard scalar section \n","with tf.name_scope(\"summaries\"):\n","  tf.summary.scalar(\"loss\", l)\n","  merged = tf.summary.merge_all()\n","\n","#configure folder for tensorboard data\n","train_writer = tf.summary.FileWriter(LOG_DIR, tf.get_default_graph())"],"execution_count":18,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n"],"name":"stdout"}]},{"metadata":{"id":"rHz2XtUPf6ha","colab_type":"text"},"cell_type":"markdown","source":["In this section the model is trained"]},{"metadata":{"id":"uqHbB1vWf7H2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":113417},"outputId":"91098abc-46dd-4d47-b434-498885f84e0f","executionInfo":{"status":"ok","timestamp":1552928439789,"user_tz":-60,"elapsed":23130,"user":{"displayName":"simone viozzi","photoUrl":"https://lh4.googleusercontent.com/-lHfKAhqXixo/AAAAAAAAAAI/AAAAAAAABF8/q48tw94xHaI/s64/photo.jpg","userId":"00173786927897941269"}}},"cell_type":"code","source":["N = train_X.shape[0]\n","with tf.Session() as sess:\n","  sess.run(tf.global_variables_initializer())\n","  step = 0\n","  for epoch in range(n_epochs):\n","    pos = 0\n","    while pos < N:\n","      batch_X = train_X[pos:pos+batch_size]\n","      batch_y = train_y[pos:pos+batch_size]\n","      feed_dict = {x: batch_X, y: batch_y}\n","      _, summary, loss = sess.run([train_op, merged, l], feed_dict=feed_dict)\n","      print(\"epoch %d, step %d, loss: %f\" % (epoch, step, loss))\n","      train_writer.add_summary(summary, step)\n","    \n","      step += 1\n","      pos += batch_size\n","\n","  # Make Predictions for model evaluetion\n","  # for the training dataset\n","  train_y_pred = sess.run(y_pred, feed_dict={x: train_X})\n","  #for the validation dataset\n","  valid_y_pred = sess.run(y_pred, feed_dict={x: valid_X})\n","  #for the test dataset\n","  test_y_pred = sess.run(y_pred, feed_dict={x: test_X})"],"execution_count":24,"outputs":[{"output_type":"stream","text":["epoch 0, step 0, loss: 1849.667236\n","epoch 0, step 1, loss: 3371.239502\n","epoch 0, step 2, loss: 3489.422852\n","epoch 0, step 3, loss: 4627.305664\n","epoch 0, step 4, loss: 6364.175781\n","epoch 0, step 5, loss: 7174.217773\n","epoch 0, step 6, loss: 2630.965576\n","epoch 0, step 7, loss: 1379.385986\n","epoch 0, step 8, loss: 2205.701660\n","epoch 0, step 9, loss: 4433.526367\n","epoch 0, step 10, loss: 1709.043945\n","epoch 0, step 11, loss: 3231.979736\n","epoch 0, step 12, loss: 2416.412109\n","epoch 0, step 13, loss: 2871.615723\n","epoch 0, step 14, loss: 6504.375000\n","epoch 0, step 15, loss: 5111.712891\n","epoch 0, step 16, loss: 2684.760742\n","epoch 0, step 17, loss: 3646.722168\n","epoch 0, step 18, loss: 5546.844727\n","epoch 0, step 19, loss: 1263.343750\n","epoch 0, step 20, loss: 5182.317871\n","epoch 0, step 21, loss: 2403.914551\n","epoch 0, step 22, loss: 2458.281494\n","epoch 0, step 23, loss: 1785.650635\n","epoch 0, step 24, loss: 2708.125977\n","epoch 0, step 25, loss: 1730.923828\n","epoch 0, step 26, loss: 2031.601318\n","epoch 0, step 27, loss: 3950.700684\n","epoch 0, step 28, loss: 1080.654175\n","epoch 0, step 29, loss: 246.007126\n","epoch 0, step 30, loss: 4066.307373\n","epoch 0, step 31, loss: 2343.902832\n","epoch 0, step 32, loss: 1734.741455\n","epoch 0, step 33, loss: 1704.741333\n","epoch 0, step 34, loss: 5470.985352\n","epoch 0, step 35, loss: 2597.041992\n","epoch 0, step 36, loss: 309.647552\n","epoch 0, step 37, loss: 978.101135\n","epoch 0, step 38, loss: 2098.653076\n","epoch 0, step 39, loss: 1830.547363\n","epoch 0, step 40, loss: 1681.070190\n","epoch 0, step 41, loss: 3504.286133\n","epoch 0, step 42, loss: 1659.247437\n","epoch 0, step 43, loss: 1215.965088\n","epoch 0, step 44, loss: 1609.436523\n","epoch 0, step 45, loss: 3476.747803\n","epoch 0, step 46, loss: 2855.393555\n","epoch 0, step 47, loss: 2622.120117\n","epoch 0, step 48, loss: 4772.084473\n","epoch 0, step 49, loss: 2089.941650\n","epoch 0, step 50, loss: 2394.133545\n","epoch 0, step 51, loss: 5370.815430\n","epoch 0, step 52, loss: 2553.618896\n","epoch 0, step 53, loss: 1315.159180\n","epoch 0, step 54, loss: 2207.925781\n","epoch 0, step 55, loss: 1323.138428\n","epoch 0, step 56, loss: 2668.949219\n","epoch 0, step 57, loss: 4336.572754\n","epoch 0, step 58, loss: 3757.866699\n","epoch 0, step 59, loss: 2703.616943\n","epoch 0, step 60, loss: 2938.585938\n","epoch 0, step 61, loss: 2340.800781\n","epoch 0, step 62, loss: 2941.995605\n","epoch 1, step 63, loss: 549.974304\n","epoch 1, step 64, loss: 1114.957275\n","epoch 1, step 65, loss: 1552.720215\n","epoch 1, step 66, loss: 3227.753906\n","epoch 1, step 67, loss: 4130.774414\n","epoch 1, step 68, loss: 4400.321777\n","epoch 1, step 69, loss: 1620.356689\n","epoch 1, step 70, loss: 957.186218\n","epoch 1, step 71, loss: 556.000854\n","epoch 1, step 72, loss: 2857.847900\n","epoch 1, step 73, loss: 1416.690552\n","epoch 1, step 74, loss: 1593.614746\n","epoch 1, step 75, loss: 794.174500\n","epoch 1, step 76, loss: 1571.741943\n","epoch 1, step 77, loss: 3814.338379\n","epoch 1, step 78, loss: 2355.938232\n","epoch 1, step 79, loss: 1148.677734\n","epoch 1, step 80, loss: 1892.848389\n","epoch 1, step 81, loss: 3494.419434\n","epoch 1, step 82, loss: 959.302185\n","epoch 1, step 83, loss: 2654.190918\n","epoch 1, step 84, loss: 1682.669312\n","epoch 1, step 85, loss: 1661.151733\n","epoch 1, step 86, loss: 1003.337769\n","epoch 1, step 87, loss: 1191.146851\n","epoch 1, step 88, loss: 1039.077393\n","epoch 1, step 89, loss: 1050.156860\n","epoch 1, step 90, loss: 2685.337646\n","epoch 1, step 91, loss: 247.326340\n","epoch 1, step 92, loss: 93.039505\n","epoch 1, step 93, loss: 2444.236816\n","epoch 1, step 94, loss: 1168.233521\n","epoch 1, step 95, loss: 1323.470459\n","epoch 1, step 96, loss: 1137.817993\n","epoch 1, step 97, loss: 3139.217285\n","epoch 1, step 98, loss: 1974.700317\n","epoch 1, step 99, loss: 116.065811\n","epoch 1, step 100, loss: 452.619110\n","epoch 1, step 101, loss: 901.016113\n","epoch 1, step 102, loss: 385.085938\n","epoch 1, step 103, loss: 1005.969360\n","epoch 1, step 104, loss: 1807.077271\n","epoch 1, step 105, loss: 1049.058350\n","epoch 1, step 106, loss: 530.503296\n","epoch 1, step 107, loss: 1120.211426\n","epoch 1, step 108, loss: 2341.293701\n","epoch 1, step 109, loss: 895.636108\n","epoch 1, step 110, loss: 1573.401367\n","epoch 1, step 111, loss: 2883.616211\n","epoch 1, step 112, loss: 1582.724976\n","epoch 1, step 113, loss: 753.834595\n","epoch 1, step 114, loss: 2810.044922\n","epoch 1, step 115, loss: 1087.554077\n","epoch 1, step 116, loss: 698.790955\n","epoch 1, step 117, loss: 1621.863037\n","epoch 1, step 118, loss: 881.648560\n","epoch 1, step 119, loss: 1728.181152\n","epoch 1, step 120, loss: 3030.694336\n","epoch 1, step 121, loss: 2978.875244\n","epoch 1, step 122, loss: 1883.335327\n","epoch 1, step 123, loss: 2211.019775\n","epoch 1, step 124, loss: 1383.347412\n","epoch 1, step 125, loss: 2184.420898\n","epoch 2, step 126, loss: 314.470490\n","epoch 2, step 127, loss: 521.903198\n","epoch 2, step 128, loss: 962.204041\n","epoch 2, step 129, loss: 2473.092529\n","epoch 2, step 130, loss: 3198.699951\n","epoch 2, step 131, loss: 3209.890869\n","epoch 2, step 132, loss: 1182.086304\n","epoch 2, step 133, loss: 692.877319\n","epoch 2, step 134, loss: 344.550903\n","epoch 2, step 135, loss: 2401.918701\n","epoch 2, step 136, loss: 1096.928711\n","epoch 2, step 137, loss: 752.383667\n","epoch 2, step 138, loss: 259.606323\n","epoch 2, step 139, loss: 909.986694\n","epoch 2, step 140, loss: 2789.856689\n","epoch 2, step 141, loss: 926.634399\n","epoch 2, step 142, loss: 641.400085\n","epoch 2, step 143, loss: 1138.338745\n","epoch 2, step 144, loss: 2563.105225\n","epoch 2, step 145, loss: 756.977905\n","epoch 2, step 146, loss: 1735.682373\n","epoch 2, step 147, loss: 1226.457031\n","epoch 2, step 148, loss: 1224.046265\n","epoch 2, step 149, loss: 689.246948\n","epoch 2, step 150, loss: 876.312683\n","epoch 2, step 151, loss: 694.543945\n","epoch 2, step 152, loss: 786.326111\n","epoch 2, step 153, loss: 1878.412720\n","epoch 2, step 154, loss: 195.837952\n","epoch 2, step 155, loss: 35.483273\n","epoch 2, step 156, loss: 1384.447754\n","epoch 2, step 157, loss: 574.385315\n","epoch 2, step 158, loss: 1137.831055\n","epoch 2, step 159, loss: 822.579956\n","epoch 2, step 160, loss: 1904.047119\n","epoch 2, step 161, loss: 1453.046509\n","epoch 2, step 162, loss: 125.064041\n","epoch 2, step 163, loss: 424.765625\n","epoch 2, step 164, loss: 458.112915\n","epoch 2, step 165, loss: 167.841827\n","epoch 2, step 166, loss: 622.516968\n","epoch 2, step 167, loss: 1099.094238\n","epoch 2, step 168, loss: 827.612549\n","epoch 2, step 169, loss: 290.896271\n","epoch 2, step 170, loss: 871.437378\n","epoch 2, step 171, loss: 1629.317139\n","epoch 2, step 172, loss: 511.310547\n","epoch 2, step 173, loss: 1109.369507\n","epoch 2, step 174, loss: 1915.006836\n","epoch 2, step 175, loss: 1267.844604\n","epoch 2, step 176, loss: 71.108482\n","epoch 2, step 177, loss: 1815.660522\n","epoch 2, step 178, loss: 712.526978\n","epoch 2, step 179, loss: 439.893646\n","epoch 2, step 180, loss: 1461.173828\n","epoch 2, step 181, loss: 736.904114\n","epoch 2, step 182, loss: 1143.880005\n","epoch 2, step 183, loss: 2276.544922\n","epoch 2, step 184, loss: 2326.043945\n","epoch 2, step 185, loss: 1477.558350\n","epoch 2, step 186, loss: 1520.407959\n","epoch 2, step 187, loss: 938.828735\n","epoch 2, step 188, loss: 1704.950439\n","epoch 3, step 189, loss: 247.691284\n","epoch 3, step 190, loss: 317.217224\n","epoch 3, step 191, loss: 643.841492\n","epoch 3, step 192, loss: 1905.937744\n","epoch 3, step 193, loss: 2357.330811\n","epoch 3, step 194, loss: 2503.634277\n","epoch 3, step 195, loss: 795.976196\n","epoch 3, step 196, loss: 495.154572\n","epoch 3, step 197, loss: 173.770905\n","epoch 3, step 198, loss: 2029.188721\n","epoch 3, step 199, loss: 771.911865\n","epoch 3, step 200, loss: 494.293335\n","epoch 3, step 201, loss: 324.505280\n","epoch 3, step 202, loss: 505.630554\n","epoch 3, step 203, loss: 2077.477783\n","epoch 3, step 204, loss: 402.860443\n","epoch 3, step 205, loss: 417.451935\n","epoch 3, step 206, loss: 937.869751\n","epoch 3, step 207, loss: 2009.746826\n","epoch 3, step 208, loss: 567.032593\n","epoch 3, step 209, loss: 1122.795898\n","epoch 3, step 210, loss: 828.796875\n","epoch 3, step 211, loss: 933.090637\n","epoch 3, step 212, loss: 586.203064\n","epoch 3, step 213, loss: 660.849243\n","epoch 3, step 214, loss: 635.298645\n","epoch 3, step 215, loss: 440.398682\n","epoch 3, step 216, loss: 1270.779907\n","epoch 3, step 217, loss: 187.595337\n","epoch 3, step 218, loss: 18.482025\n","epoch 3, step 219, loss: 840.424011\n","epoch 3, step 220, loss: 368.032440\n","epoch 3, step 221, loss: 834.600952\n","epoch 3, step 222, loss: 637.723450\n","epoch 3, step 223, loss: 1171.899658\n","epoch 3, step 224, loss: 1070.761841\n","epoch 3, step 225, loss: 27.346790\n","epoch 3, step 226, loss: 350.421997\n","epoch 3, step 227, loss: 224.639252\n","epoch 3, step 228, loss: 56.035011\n","epoch 3, step 229, loss: 402.832886\n","epoch 3, step 230, loss: 741.962646\n","epoch 3, step 231, loss: 653.528503\n","epoch 3, step 232, loss: 190.013077\n","epoch 3, step 233, loss: 640.166748\n","epoch 3, step 234, loss: 1138.477539\n","epoch 3, step 235, loss: 327.695312\n","epoch 3, step 236, loss: 845.022827\n","epoch 3, step 237, loss: 1313.926514\n","epoch 3, step 238, loss: 1078.423950\n","epoch 3, step 239, loss: 0.000177\n","epoch 3, step 240, loss: 1443.371582\n","epoch 3, step 241, loss: 563.737793\n","epoch 3, step 242, loss: 216.344254\n","epoch 3, step 243, loss: 1114.526978\n","epoch 3, step 244, loss: 624.821167\n","epoch 3, step 245, loss: 682.878967\n","epoch 3, step 246, loss: 1939.867432\n","epoch 3, step 247, loss: 1922.328979\n","epoch 3, step 248, loss: 1153.485962\n","epoch 3, step 249, loss: 1088.975830\n","epoch 3, step 250, loss: 781.175537\n","epoch 3, step 251, loss: 1370.217163\n","epoch 4, step 252, loss: 170.884430\n","epoch 4, step 253, loss: 280.674438\n","epoch 4, step 254, loss: 507.506561\n","epoch 4, step 255, loss: 1454.228149\n","epoch 4, step 256, loss: 1862.948120\n","epoch 4, step 257, loss: 2012.869629\n","epoch 4, step 258, loss: 574.578308\n","epoch 4, step 259, loss: 384.329529\n","epoch 4, step 260, loss: 196.660126\n","epoch 4, step 261, loss: 1771.158203\n","epoch 4, step 262, loss: 649.718994\n","epoch 4, step 263, loss: 220.694946\n","epoch 4, step 264, loss: 202.575882\n","epoch 4, step 265, loss: 289.189209\n","epoch 4, step 266, loss: 1641.049072\n","epoch 4, step 267, loss: 104.863106\n","epoch 4, step 268, loss: 322.463196\n","epoch 4, step 269, loss: 772.674377\n","epoch 4, step 270, loss: 1599.195801\n","epoch 4, step 271, loss: 448.694519\n","epoch 4, step 272, loss: 733.840515\n","epoch 4, step 273, loss: 574.479492\n","epoch 4, step 274, loss: 705.175232\n","epoch 4, step 275, loss: 409.276550\n","epoch 4, step 276, loss: 481.937042\n","epoch 4, step 277, loss: 395.827667\n","epoch 4, step 278, loss: 223.886230\n","epoch 4, step 279, loss: 948.215698\n","epoch 4, step 280, loss: 129.075104\n","epoch 4, step 281, loss: 6.143378\n","epoch 4, step 282, loss: 402.980713\n","epoch 4, step 283, loss: 201.750458\n","epoch 4, step 284, loss: 582.826843\n","epoch 4, step 285, loss: 419.059357\n","epoch 4, step 286, loss: 675.351868\n","epoch 4, step 287, loss: 783.436218\n","epoch 4, step 288, loss: 99.719505\n","epoch 4, step 289, loss: 379.820740\n","epoch 4, step 290, loss: 188.340485\n","epoch 4, step 291, loss: 62.569206\n","epoch 4, step 292, loss: 325.138519\n","epoch 4, step 293, loss: 501.106049\n","epoch 4, step 294, loss: 547.800720\n","epoch 4, step 295, loss: 86.759758\n","epoch 4, step 296, loss: 425.547668\n","epoch 4, step 297, loss: 863.242432\n","epoch 4, step 298, loss: 247.816498\n","epoch 4, step 299, loss: 716.093018\n","epoch 4, step 300, loss: 872.788452\n","epoch 4, step 301, loss: 920.670715\n","epoch 4, step 302, loss: 3.556727\n","epoch 4, step 303, loss: 1152.804932\n","epoch 4, step 304, loss: 420.825806\n","epoch 4, step 305, loss: 148.420074\n","epoch 4, step 306, loss: 789.137695\n","epoch 4, step 307, loss: 526.921021\n","epoch 4, step 308, loss: 516.567932\n","epoch 4, step 309, loss: 1668.681396\n","epoch 4, step 310, loss: 1575.103516\n","epoch 4, step 311, loss: 819.829407\n","epoch 4, step 312, loss: 681.455383\n","epoch 4, step 313, loss: 706.766479\n","epoch 4, step 314, loss: 1059.401611\n","epoch 5, step 315, loss: 154.903870\n","epoch 5, step 316, loss: 269.967865\n","epoch 5, step 317, loss: 354.415222\n","epoch 5, step 318, loss: 1026.519531\n","epoch 5, step 319, loss: 1328.149902\n","epoch 5, step 320, loss: 1579.114746\n","epoch 5, step 321, loss: 396.810242\n","epoch 5, step 322, loss: 300.791840\n","epoch 5, step 323, loss: 238.017990\n","epoch 5, step 324, loss: 1472.775757\n","epoch 5, step 325, loss: 479.898834\n","epoch 5, step 326, loss: 122.067055\n","epoch 5, step 327, loss: 144.563248\n","epoch 5, step 328, loss: 162.606232\n","epoch 5, step 329, loss: 1322.861084\n","epoch 5, step 330, loss: 5.037926\n","epoch 5, step 331, loss: 272.470398\n","epoch 5, step 332, loss: 529.934143\n","epoch 5, step 333, loss: 1292.525146\n","epoch 5, step 334, loss: 326.964661\n","epoch 5, step 335, loss: 465.866730\n","epoch 5, step 336, loss: 415.148193\n","epoch 5, step 337, loss: 488.351440\n","epoch 5, step 338, loss: 316.124268\n","epoch 5, step 339, loss: 397.124146\n","epoch 5, step 340, loss: 351.003448\n","epoch 5, step 341, loss: 220.055801\n","epoch 5, step 342, loss: 716.857056\n","epoch 5, step 343, loss: 100.541580\n","epoch 5, step 344, loss: 8.429673\n","epoch 5, step 345, loss: 185.725037\n","epoch 5, step 346, loss: 92.432465\n","epoch 5, step 347, loss: 408.699585\n","epoch 5, step 348, loss: 276.734406\n","epoch 5, step 349, loss: 396.496765\n","epoch 5, step 350, loss: 562.030945\n","epoch 5, step 351, loss: 53.359734\n","epoch 5, step 352, loss: 282.413635\n","epoch 5, step 353, loss: 99.993561\n","epoch 5, step 354, loss: 14.494417\n","epoch 5, step 355, loss: 257.845703\n","epoch 5, step 356, loss: 454.653656\n","epoch 5, step 357, loss: 463.490967\n","epoch 5, step 358, loss: 44.335194\n","epoch 5, step 359, loss: 289.234558\n","epoch 5, step 360, loss: 618.555298\n","epoch 5, step 361, loss: 198.222107\n","epoch 5, step 362, loss: 552.114136\n","epoch 5, step 363, loss: 671.713013\n","epoch 5, step 364, loss: 774.906982\n","epoch 5, step 365, loss: 0.000000\n","epoch 5, step 366, loss: 923.584839\n","epoch 5, step 367, loss: 309.026062\n","epoch 5, step 368, loss: 111.471207\n","epoch 5, step 369, loss: 580.418457\n","epoch 5, step 370, loss: 431.363342\n","epoch 5, step 371, loss: 372.108643\n","epoch 5, step 372, loss: 1426.357666\n","epoch 5, step 373, loss: 1243.778809\n","epoch 5, step 374, loss: 577.574951\n","epoch 5, step 375, loss: 451.190613\n","epoch 5, step 376, loss: 509.841858\n","epoch 5, step 377, loss: 858.896423\n","epoch 6, step 378, loss: 94.669746\n","epoch 6, step 379, loss: 255.672577\n","epoch 6, step 380, loss: 242.414444\n","epoch 6, step 381, loss: 823.274292\n","epoch 6, step 382, loss: 914.597656\n","epoch 6, step 383, loss: 1146.005859\n","epoch 6, step 384, loss: 185.063309\n","epoch 6, step 385, loss: 253.716202\n","epoch 6, step 386, loss: 160.645538\n","epoch 6, step 387, loss: 1221.441895\n","epoch 6, step 388, loss: 358.971893\n","epoch 6, step 389, loss: 79.034172\n","epoch 6, step 390, loss: 206.830826\n","epoch 6, step 391, loss: 69.155830\n","epoch 6, step 392, loss: 1106.308838\n","epoch 6, step 393, loss: 6.394463\n","epoch 6, step 394, loss: 252.015900\n","epoch 6, step 395, loss: 367.768005\n","epoch 6, step 396, loss: 990.593079\n","epoch 6, step 397, loss: 282.136902\n","epoch 6, step 398, loss: 381.752472\n","epoch 6, step 399, loss: 273.564606\n","epoch 6, step 400, loss: 375.023560\n","epoch 6, step 401, loss: 227.171600\n","epoch 6, step 402, loss: 315.380310\n","epoch 6, step 403, loss: 179.733109\n","epoch 6, step 404, loss: 146.411133\n","epoch 6, step 405, loss: 425.953583\n","epoch 6, step 406, loss: 61.955101\n","epoch 6, step 407, loss: 0.000008\n","epoch 6, step 408, loss: 81.159401\n","epoch 6, step 409, loss: 0.000175\n","epoch 6, step 410, loss: 175.138992\n","epoch 6, step 411, loss: 222.677170\n","epoch 6, step 412, loss: 163.531647\n","epoch 6, step 413, loss: 399.031189\n","epoch 6, step 414, loss: 49.034161\n","epoch 6, step 415, loss: 263.666168\n","epoch 6, step 416, loss: 76.428589\n","epoch 6, step 417, loss: 4.822498\n","epoch 6, step 418, loss: 209.988129\n","epoch 6, step 419, loss: 338.357758\n","epoch 6, step 420, loss: 360.822632\n","epoch 6, step 421, loss: 3.841293\n","epoch 6, step 422, loss: 155.698181\n","epoch 6, step 423, loss: 380.953125\n","epoch 6, step 424, loss: 151.385422\n","epoch 6, step 425, loss: 376.343903\n","epoch 6, step 426, loss: 441.939758\n","epoch 6, step 427, loss: 609.120544\n","epoch 6, step 428, loss: 0.000000\n","epoch 6, step 429, loss: 719.801331\n","epoch 6, step 430, loss: 256.928406\n","epoch 6, step 431, loss: 50.889149\n","epoch 6, step 432, loss: 383.461151\n","epoch 6, step 433, loss: 341.670441\n","epoch 6, step 434, loss: 330.837036\n","epoch 6, step 435, loss: 1249.143311\n","epoch 6, step 436, loss: 976.911926\n","epoch 6, step 437, loss: 338.671631\n","epoch 6, step 438, loss: 253.228394\n","epoch 6, step 439, loss: 435.570007\n","epoch 6, step 440, loss: 789.318237\n","epoch 7, step 441, loss: 58.630177\n","epoch 7, step 442, loss: 155.699020\n","epoch 7, step 443, loss: 174.995773\n","epoch 7, step 444, loss: 669.477905\n","epoch 7, step 445, loss: 658.296692\n","epoch 7, step 446, loss: 971.195984\n","epoch 7, step 447, loss: 98.014336\n","epoch 7, step 448, loss: 156.279648\n","epoch 7, step 449, loss: 95.601746\n","epoch 7, step 450, loss: 1048.016602\n","epoch 7, step 451, loss: 228.137863\n","epoch 7, step 452, loss: 58.570019\n","epoch 7, step 453, loss: 69.725624\n","epoch 7, step 454, loss: 15.433095\n","epoch 7, step 455, loss: 888.654602\n","epoch 7, step 456, loss: 74.384613\n","epoch 7, step 457, loss: 234.961716\n","epoch 7, step 458, loss: 229.509308\n","epoch 7, step 459, loss: 740.219604\n","epoch 7, step 460, loss: 256.392731\n","epoch 7, step 461, loss: 295.969879\n","epoch 7, step 462, loss: 159.215393\n","epoch 7, step 463, loss: 229.087616\n","epoch 7, step 464, loss: 152.698761\n","epoch 7, step 465, loss: 232.804779\n","epoch 7, step 466, loss: 99.039719\n","epoch 7, step 467, loss: 93.328796\n","epoch 7, step 468, loss: 324.344147\n","epoch 7, step 469, loss: 23.164627\n","epoch 7, step 470, loss: 0.125453\n","epoch 7, step 471, loss: 60.383434\n","epoch 7, step 472, loss: 0.000001\n","epoch 7, step 473, loss: 41.090378\n","epoch 7, step 474, loss: 152.687439\n","epoch 7, step 475, loss: 18.574402\n","epoch 7, step 476, loss: 325.489502\n","epoch 7, step 477, loss: 1.119128\n","epoch 7, step 478, loss: 207.187592\n","epoch 7, step 479, loss: 34.598331\n","epoch 7, step 480, loss: 26.216373\n","epoch 7, step 481, loss: 113.136780\n","epoch 7, step 482, loss: 295.195496\n","epoch 7, step 483, loss: 256.155609\n","epoch 7, step 484, loss: 4.029275\n","epoch 7, step 485, loss: 113.738228\n","epoch 7, step 486, loss: 343.920868\n","epoch 7, step 487, loss: 154.572968\n","epoch 7, step 488, loss: 271.723907\n","epoch 7, step 489, loss: 211.451965\n","epoch 7, step 490, loss: 444.666260\n","epoch 7, step 491, loss: 37.852741\n","epoch 7, step 492, loss: 545.047913\n","epoch 7, step 493, loss: 150.254608\n","epoch 7, step 494, loss: 0.641172\n","epoch 7, step 495, loss: 229.409058\n","epoch 7, step 496, loss: 253.067703\n","epoch 7, step 497, loss: 215.772354\n","epoch 7, step 498, loss: 1185.951416\n","epoch 7, step 499, loss: 734.966553\n","epoch 7, step 500, loss: 222.969604\n","epoch 7, step 501, loss: 144.211761\n","epoch 7, step 502, loss: 361.607849\n","epoch 7, step 503, loss: 713.300537\n","epoch 8, step 504, loss: 27.193949\n","epoch 8, step 505, loss: 162.525803\n","epoch 8, step 506, loss: 119.638123\n","epoch 8, step 507, loss: 667.328857\n","epoch 8, step 508, loss: 452.363525\n","epoch 8, step 509, loss: 767.740540\n","epoch 8, step 510, loss: 0.000000\n","epoch 8, step 511, loss: 82.422523\n","epoch 8, step 512, loss: 52.449749\n","epoch 8, step 513, loss: 893.461914\n","epoch 8, step 514, loss: 146.126633\n","epoch 8, step 515, loss: 47.253658\n","epoch 8, step 516, loss: 33.695744\n","epoch 8, step 517, loss: 28.538586\n","epoch 8, step 518, loss: 742.037170\n","epoch 8, step 519, loss: 19.717627\n","epoch 8, step 520, loss: 201.122559\n","epoch 8, step 521, loss: 98.740837\n","epoch 8, step 522, loss: 605.023193\n","epoch 8, step 523, loss: 226.929672\n","epoch 8, step 524, loss: 225.646454\n","epoch 8, step 525, loss: 53.644661\n","epoch 8, step 526, loss: 114.033806\n","epoch 8, step 527, loss: 71.191780\n","epoch 8, step 528, loss: 200.182404\n","epoch 8, step 529, loss: 90.812057\n","epoch 8, step 530, loss: 106.417976\n","epoch 8, step 531, loss: 223.872391\n","epoch 8, step 532, loss: 0.000000\n","epoch 8, step 533, loss: 0.000419\n","epoch 8, step 534, loss: 33.007565\n","epoch 8, step 535, loss: 0.000000\n","epoch 8, step 536, loss: 9.308985\n","epoch 8, step 537, loss: 96.902306\n","epoch 8, step 538, loss: 1.063256\n","epoch 8, step 539, loss: 304.802368\n","epoch 8, step 540, loss: 0.007952\n","epoch 8, step 541, loss: 199.771942\n","epoch 8, step 542, loss: 21.347853\n","epoch 8, step 543, loss: 0.000000\n","epoch 8, step 544, loss: 101.168465\n","epoch 8, step 545, loss: 226.848724\n","epoch 8, step 546, loss: 234.107117\n","epoch 8, step 547, loss: 0.212397\n","epoch 8, step 548, loss: 19.556337\n","epoch 8, step 549, loss: 132.099426\n","epoch 8, step 550, loss: 19.214008\n","epoch 8, step 551, loss: 155.109116\n","epoch 8, step 552, loss: 118.391922\n","epoch 8, step 553, loss: 337.090485\n","epoch 8, step 554, loss: 54.136665\n","epoch 8, step 555, loss: 431.253540\n","epoch 8, step 556, loss: 140.127747\n","epoch 8, step 557, loss: 8.982663\n","epoch 8, step 558, loss: 258.326111\n","epoch 8, step 559, loss: 266.039398\n","epoch 8, step 560, loss: 156.816483\n","epoch 8, step 561, loss: 893.984253\n","epoch 8, step 562, loss: 486.636932\n","epoch 8, step 563, loss: 104.446548\n","epoch 8, step 564, loss: 68.349632\n","epoch 8, step 565, loss: 312.226379\n","epoch 8, step 566, loss: 661.740234\n","epoch 9, step 567, loss: 66.415611\n","epoch 9, step 568, loss: 81.715027\n","epoch 9, step 569, loss: 120.880615\n","epoch 9, step 570, loss: 551.629578\n","epoch 9, step 571, loss: 429.038971\n","epoch 9, step 572, loss: 652.569214\n","epoch 9, step 573, loss: 41.956284\n","epoch 9, step 574, loss: 65.233765\n","epoch 9, step 575, loss: 5.665230\n","epoch 9, step 576, loss: 672.508423\n","epoch 9, step 577, loss: 190.663681\n","epoch 9, step 578, loss: 42.519173\n","epoch 9, step 579, loss: 0.000000\n","epoch 9, step 580, loss: 6.409984\n","epoch 9, step 581, loss: 696.138672\n","epoch 9, step 582, loss: 11.958037\n","epoch 9, step 583, loss: 174.914719\n","epoch 9, step 584, loss: 103.942444\n","epoch 9, step 585, loss: 503.049713\n","epoch 9, step 586, loss: 177.458923\n","epoch 9, step 587, loss: 186.376038\n","epoch 9, step 588, loss: 28.907410\n","epoch 9, step 589, loss: 77.234642\n","epoch 9, step 590, loss: 51.641518\n","epoch 9, step 591, loss: 80.751648\n","epoch 9, step 592, loss: 120.728317\n","epoch 9, step 593, loss: 112.679153\n","epoch 9, step 594, loss: 185.563141\n","epoch 9, step 595, loss: 29.725765\n","epoch 9, step 596, loss: 12.345680\n","epoch 9, step 597, loss: 39.490288\n","epoch 9, step 598, loss: 8.926394\n","epoch 9, step 599, loss: 7.414531\n","epoch 9, step 600, loss: 55.331600\n","epoch 9, step 601, loss: 27.091228\n","epoch 9, step 602, loss: 246.031265\n","epoch 9, step 603, loss: 2.719744\n","epoch 9, step 604, loss: 196.302628\n","epoch 9, step 605, loss: 28.969124\n","epoch 9, step 606, loss: 0.000000\n","epoch 9, step 607, loss: 78.926788\n","epoch 9, step 608, loss: 207.733368\n","epoch 9, step 609, loss: 256.114685\n","epoch 9, step 610, loss: 17.506439\n","epoch 9, step 611, loss: 35.648533\n","epoch 9, step 612, loss: 42.979565\n","epoch 9, step 613, loss: 26.416006\n","epoch 9, step 614, loss: 150.067902\n","epoch 9, step 615, loss: 81.343285\n","epoch 9, step 616, loss: 296.270142\n","epoch 9, step 617, loss: 0.000000\n","epoch 9, step 618, loss: 231.318649\n","epoch 9, step 619, loss: 44.060371\n","epoch 9, step 620, loss: 0.000000\n","epoch 9, step 621, loss: 65.040321\n","epoch 9, step 622, loss: 93.994728\n","epoch 9, step 623, loss: 76.925964\n","epoch 9, step 624, loss: 734.117615\n","epoch 9, step 625, loss: 388.671692\n","epoch 9, step 626, loss: 229.157761\n","epoch 9, step 627, loss: 4.777065\n","epoch 9, step 628, loss: 303.374390\n","epoch 9, step 629, loss: 526.670532\n","epoch 10, step 630, loss: 126.816238\n","epoch 10, step 631, loss: 92.450569\n","epoch 10, step 632, loss: 105.295181\n","epoch 10, step 633, loss: 455.165771\n","epoch 10, step 634, loss: 334.247925\n","epoch 10, step 635, loss: 482.508118\n","epoch 10, step 636, loss: 0.005723\n","epoch 10, step 637, loss: 24.840235\n","epoch 10, step 638, loss: 0.000067\n","epoch 10, step 639, loss: 598.356445\n","epoch 10, step 640, loss: 128.303726\n","epoch 10, step 641, loss: 45.252209\n","epoch 10, step 642, loss: 0.000000\n","epoch 10, step 643, loss: 0.004222\n","epoch 10, step 644, loss: 658.381714\n","epoch 10, step 645, loss: 0.000000\n","epoch 10, step 646, loss: 210.264999\n","epoch 10, step 647, loss: 160.653473\n","epoch 10, step 648, loss: 605.748413\n","epoch 10, step 649, loss: 200.806473\n","epoch 10, step 650, loss: 216.164780\n","epoch 10, step 651, loss: 15.523382\n","epoch 10, step 652, loss: 98.191513\n","epoch 10, step 653, loss: 0.002971\n","epoch 10, step 654, loss: 3.810335\n","epoch 10, step 655, loss: 31.984900\n","epoch 10, step 656, loss: 15.469723\n","epoch 10, step 657, loss: 23.114799\n","epoch 10, step 658, loss: 0.000109\n","epoch 10, step 659, loss: 1.458251\n","epoch 10, step 660, loss: 0.081257\n","epoch 10, step 661, loss: 22.456457\n","epoch 10, step 662, loss: 91.888550\n","epoch 10, step 663, loss: 25.416584\n","epoch 10, step 664, loss: 48.599571\n","epoch 10, step 665, loss: 238.285950\n","epoch 10, step 666, loss: 64.492760\n","epoch 10, step 667, loss: 170.808029\n","epoch 10, step 668, loss: 30.026848\n","epoch 10, step 669, loss: 15.389755\n","epoch 10, step 670, loss: 57.693321\n","epoch 10, step 671, loss: 168.121094\n","epoch 10, step 672, loss: 192.939484\n","epoch 10, step 673, loss: 0.000024\n","epoch 10, step 674, loss: 0.000063\n","epoch 10, step 675, loss: 20.645901\n","epoch 10, step 676, loss: 0.000000\n","epoch 10, step 677, loss: 75.669342\n","epoch 10, step 678, loss: 4.667078\n","epoch 10, step 679, loss: 236.559631\n","epoch 10, step 680, loss: 0.000000\n","epoch 10, step 681, loss: 182.353027\n","epoch 10, step 682, loss: 46.118340\n","epoch 10, step 683, loss: 0.000000\n","epoch 10, step 684, loss: 11.608286\n","epoch 10, step 685, loss: 57.680275\n","epoch 10, step 686, loss: 26.044306\n","epoch 10, step 687, loss: 676.663147\n","epoch 10, step 688, loss: 321.931519\n","epoch 10, step 689, loss: 0.038007\n","epoch 10, step 690, loss: 7.571340\n","epoch 10, step 691, loss: 192.158020\n","epoch 10, step 692, loss: 427.261383\n","epoch 11, step 693, loss: 40.717907\n","epoch 11, step 694, loss: 111.621155\n","epoch 11, step 695, loss: 35.203690\n","epoch 11, step 696, loss: 333.979797\n","epoch 11, step 697, loss: 294.242737\n","epoch 11, step 698, loss: 270.178040\n","epoch 11, step 699, loss: 15.501556\n","epoch 11, step 700, loss: 36.922382\n","epoch 11, step 701, loss: 24.195910\n","epoch 11, step 702, loss: 563.735352\n","epoch 11, step 703, loss: 46.835571\n","epoch 11, step 704, loss: 4.461778\n","epoch 11, step 705, loss: 0.001422\n","epoch 11, step 706, loss: 23.738144\n","epoch 11, step 707, loss: 426.738068\n","epoch 11, step 708, loss: 3.979642\n","epoch 11, step 709, loss: 142.445435\n","epoch 11, step 710, loss: 8.461061\n","epoch 11, step 711, loss: 383.707825\n","epoch 11, step 712, loss: 140.185806\n","epoch 11, step 713, loss: 123.149887\n","epoch 11, step 714, loss: 8.150091\n","epoch 11, step 715, loss: 41.591267\n","epoch 11, step 716, loss: 3.630958\n","epoch 11, step 717, loss: 11.357451\n","epoch 11, step 718, loss: 19.165068\n","epoch 11, step 719, loss: 19.007521\n","epoch 11, step 720, loss: 6.331377\n","epoch 11, step 721, loss: 0.665811\n","epoch 11, step 722, loss: 0.000000\n","epoch 11, step 723, loss: 0.019114\n","epoch 11, step 724, loss: 0.000000\n","epoch 11, step 725, loss: 0.000000\n","epoch 11, step 726, loss: 5.734246\n","epoch 11, step 727, loss: 0.000000\n","epoch 11, step 728, loss: 126.458893\n","epoch 11, step 729, loss: 0.000000\n","epoch 11, step 730, loss: 129.131073\n","epoch 11, step 731, loss: 11.373136\n","epoch 11, step 732, loss: 20.633018\n","epoch 11, step 733, loss: 0.000009\n","epoch 11, step 734, loss: 107.777153\n","epoch 11, step 735, loss: 112.193192\n","epoch 11, step 736, loss: 2.541787\n","epoch 11, step 737, loss: 14.703695\n","epoch 11, step 738, loss: 38.354153\n","epoch 11, step 739, loss: 0.435502\n","epoch 11, step 740, loss: 9.781961\n","epoch 11, step 741, loss: 26.935322\n","epoch 11, step 742, loss: 170.647232\n","epoch 11, step 743, loss: 5.705923\n","epoch 11, step 744, loss: 139.280212\n","epoch 11, step 745, loss: 15.591007\n","epoch 11, step 746, loss: 0.000000\n","epoch 11, step 747, loss: 43.586525\n","epoch 11, step 748, loss: 9.547674\n","epoch 11, step 749, loss: 0.000067\n","epoch 11, step 750, loss: 541.343567\n","epoch 11, step 751, loss: 247.776917\n","epoch 11, step 752, loss: 0.292825\n","epoch 11, step 753, loss: 0.000025\n","epoch 11, step 754, loss: 134.319443\n","epoch 11, step 755, loss: 369.937286\n","epoch 12, step 756, loss: 58.038143\n","epoch 12, step 757, loss: 24.320332\n","epoch 12, step 758, loss: 5.002239\n","epoch 12, step 759, loss: 205.825577\n","epoch 12, step 760, loss: 246.540863\n","epoch 12, step 761, loss: 204.836853\n","epoch 12, step 762, loss: 0.000000\n","epoch 12, step 763, loss: 0.000000\n","epoch 12, step 764, loss: 0.000155\n","epoch 12, step 765, loss: 380.645508\n","epoch 12, step 766, loss: 2.472800\n","epoch 12, step 767, loss: 32.862019\n","epoch 12, step 768, loss: 0.000288\n","epoch 12, step 769, loss: 0.000002\n","epoch 12, step 770, loss: 352.882751\n","epoch 12, step 771, loss: 0.000000\n","epoch 12, step 772, loss: 124.740234\n","epoch 12, step 773, loss: 22.164757\n","epoch 12, step 774, loss: 275.220917\n","epoch 12, step 775, loss: 119.646965\n","epoch 12, step 776, loss: 83.349426\n","epoch 12, step 777, loss: 0.046843\n","epoch 12, step 778, loss: 26.424980\n","epoch 12, step 779, loss: 0.000000\n","epoch 12, step 780, loss: 0.092566\n","epoch 12, step 781, loss: 27.857080\n","epoch 12, step 782, loss: 0.000000\n","epoch 12, step 783, loss: 0.002214\n","epoch 12, step 784, loss: 22.598417\n","epoch 12, step 785, loss: 0.000000\n","epoch 12, step 786, loss: 0.026246\n","epoch 12, step 787, loss: 0.000000\n","epoch 12, step 788, loss: 0.000016\n","epoch 12, step 789, loss: 0.000000\n","epoch 12, step 790, loss: 0.000000\n","epoch 12, step 791, loss: 67.784821\n","epoch 12, step 792, loss: 0.000000\n","epoch 12, step 793, loss: 113.334221\n","epoch 12, step 794, loss: 0.000010\n","epoch 12, step 795, loss: 0.000022\n","epoch 12, step 796, loss: 11.557818\n","epoch 12, step 797, loss: 104.450958\n","epoch 12, step 798, loss: 64.865761\n","epoch 12, step 799, loss: 0.449696\n","epoch 12, step 800, loss: 0.080184\n","epoch 12, step 801, loss: 18.660004\n","epoch 12, step 802, loss: 0.000094\n","epoch 12, step 803, loss: 0.000079\n","epoch 12, step 804, loss: 11.611167\n","epoch 12, step 805, loss: 107.851967\n","epoch 12, step 806, loss: 0.000004\n","epoch 12, step 807, loss: 64.580902\n","epoch 12, step 808, loss: 0.000607\n","epoch 12, step 809, loss: 0.000010\n","epoch 12, step 810, loss: 0.015391\n","epoch 12, step 811, loss: 1.280479\n","epoch 12, step 812, loss: 6.405085\n","epoch 12, step 813, loss: 392.328217\n","epoch 12, step 814, loss: 186.098770\n","epoch 12, step 815, loss: 18.863010\n","epoch 12, step 816, loss: 19.849449\n","epoch 12, step 817, loss: 73.504478\n","epoch 12, step 818, loss: 291.047974\n","epoch 13, step 819, loss: 0.000004\n","epoch 13, step 820, loss: 0.012049\n","epoch 13, step 821, loss: 13.466323\n","epoch 13, step 822, loss: 268.282410\n","epoch 13, step 823, loss: 197.011551\n","epoch 13, step 824, loss: 120.643036\n","epoch 13, step 825, loss: 0.013696\n","epoch 13, step 826, loss: 3.872709\n","epoch 13, step 827, loss: 2.264016\n","epoch 13, step 828, loss: 305.122467\n","epoch 13, step 829, loss: 25.219730\n","epoch 13, step 830, loss: 1.982144\n","epoch 13, step 831, loss: 0.000953\n","epoch 13, step 832, loss: 0.000000\n","epoch 13, step 833, loss: 228.724304\n","epoch 13, step 834, loss: 0.041868\n","epoch 13, step 835, loss: 109.428513\n","epoch 13, step 836, loss: 0.254546\n","epoch 13, step 837, loss: 249.753952\n","epoch 13, step 838, loss: 91.889366\n","epoch 13, step 839, loss: 46.643379\n","epoch 13, step 840, loss: 0.031855\n","epoch 13, step 841, loss: 32.530483\n","epoch 13, step 842, loss: 0.000000\n","epoch 13, step 843, loss: 0.000178\n","epoch 13, step 844, loss: 0.002627\n","epoch 13, step 845, loss: 0.000000\n","epoch 13, step 846, loss: 1.911744\n","epoch 13, step 847, loss: 0.003269\n","epoch 13, step 848, loss: 0.000000\n","epoch 13, step 849, loss: 4.435965\n","epoch 13, step 850, loss: 21.694241\n","epoch 13, step 851, loss: 0.000000\n","epoch 13, step 852, loss: 0.000000\n","epoch 13, step 853, loss: 2.026689\n","epoch 13, step 854, loss: 48.323048\n","epoch 13, step 855, loss: 0.000000\n","epoch 13, step 856, loss: 108.179810\n","epoch 13, step 857, loss: 0.000001\n","epoch 13, step 858, loss: 0.000000\n","epoch 13, step 859, loss: 0.241103\n","epoch 13, step 860, loss: 87.486763\n","epoch 13, step 861, loss: 26.757610\n","epoch 13, step 862, loss: 0.800208\n","epoch 13, step 863, loss: 0.000010\n","epoch 13, step 864, loss: 0.002953\n","epoch 13, step 865, loss: 0.026537\n","epoch 13, step 866, loss: 5.348274\n","epoch 13, step 867, loss: 0.814052\n","epoch 13, step 868, loss: 29.196810\n","epoch 13, step 869, loss: 0.179282\n","epoch 13, step 870, loss: 4.192563\n","epoch 13, step 871, loss: 5.797658\n","epoch 13, step 872, loss: 5.484498\n","epoch 13, step 873, loss: 33.004471\n","epoch 13, step 874, loss: 44.137047\n","epoch 13, step 875, loss: 1.498819\n","epoch 13, step 876, loss: 302.993774\n","epoch 13, step 877, loss: 121.402138\n","epoch 13, step 878, loss: 7.143171\n","epoch 13, step 879, loss: 0.000000\n","epoch 13, step 880, loss: 25.221912\n","epoch 13, step 881, loss: 239.956482\n","epoch 14, step 882, loss: 0.000083\n","epoch 14, step 883, loss: 0.200388\n","epoch 14, step 884, loss: 0.002689\n","epoch 14, step 885, loss: 121.685165\n","epoch 14, step 886, loss: 170.679886\n","epoch 14, step 887, loss: 56.238911\n","epoch 14, step 888, loss: 0.000018\n","epoch 14, step 889, loss: 0.000312\n","epoch 14, step 890, loss: 0.000221\n","epoch 14, step 891, loss: 239.934891\n","epoch 14, step 892, loss: 9.604840\n","epoch 14, step 893, loss: 3.585712\n","epoch 14, step 894, loss: 0.000067\n","epoch 14, step 895, loss: 0.000000\n","epoch 14, step 896, loss: 134.510178\n","epoch 14, step 897, loss: 18.173546\n","epoch 14, step 898, loss: 81.436806\n","epoch 14, step 899, loss: 1.451867\n","epoch 14, step 900, loss: 236.097580\n","epoch 14, step 901, loss: 29.348497\n","epoch 14, step 902, loss: 20.402109\n","epoch 14, step 903, loss: 6.247997\n","epoch 14, step 904, loss: 0.000882\n","epoch 14, step 905, loss: 0.000541\n","epoch 14, step 906, loss: 33.633492\n","epoch 14, step 907, loss: 15.722795\n","epoch 14, step 908, loss: 0.000000\n","epoch 14, step 909, loss: 12.665437\n","epoch 14, step 910, loss: 0.004115\n","epoch 14, step 911, loss: 0.000000\n","epoch 14, step 912, loss: 0.003115\n","epoch 14, step 913, loss: 0.000000\n","epoch 14, step 914, loss: 0.000031\n","epoch 14, step 915, loss: 0.000000\n","epoch 14, step 916, loss: 0.000025\n","epoch 14, step 917, loss: 8.411613\n","epoch 14, step 918, loss: 0.000690\n","epoch 14, step 919, loss: 81.896591\n","epoch 14, step 920, loss: 0.178511\n","epoch 14, step 921, loss: 0.000000\n","epoch 14, step 922, loss: 15.119802\n","epoch 14, step 923, loss: 76.981941\n","epoch 14, step 924, loss: 10.793525\n","epoch 14, step 925, loss: 0.000006\n","epoch 14, step 926, loss: 0.016342\n","epoch 14, step 927, loss: 0.004773\n","epoch 14, step 928, loss: 0.000300\n","epoch 14, step 929, loss: 0.000046\n","epoch 14, step 930, loss: 0.007875\n","epoch 14, step 931, loss: 0.000010\n","epoch 14, step 932, loss: 0.002504\n","epoch 14, step 933, loss: 22.550156\n","epoch 14, step 934, loss: 19.249210\n","epoch 14, step 935, loss: 0.000000\n","epoch 14, step 936, loss: 0.041562\n","epoch 14, step 937, loss: 0.000000\n","epoch 14, step 938, loss: 0.095227\n","epoch 14, step 939, loss: 189.840729\n","epoch 14, step 940, loss: 95.321083\n","epoch 14, step 941, loss: 9.984501\n","epoch 14, step 942, loss: 16.215252\n","epoch 14, step 943, loss: 34.153202\n","epoch 14, step 944, loss: 197.743729\n","epoch 15, step 945, loss: 188.462021\n","epoch 15, step 946, loss: 48.218636\n","epoch 15, step 947, loss: 17.025454\n","epoch 15, step 948, loss: 100.540512\n","epoch 15, step 949, loss: 157.141678\n","epoch 15, step 950, loss: 3.480144\n","epoch 15, step 951, loss: 0.021874\n","epoch 15, step 952, loss: 0.001296\n","epoch 15, step 953, loss: 0.029939\n","epoch 15, step 954, loss: 201.716949\n","epoch 15, step 955, loss: 3.358906\n","epoch 15, step 956, loss: 29.268984\n","epoch 15, step 957, loss: 0.000001\n","epoch 15, step 958, loss: 0.000000\n","epoch 15, step 959, loss: 101.786728\n","epoch 15, step 960, loss: 0.000000\n","epoch 15, step 961, loss: 66.403816\n","epoch 15, step 962, loss: 0.001236\n","epoch 15, step 963, loss: 220.821533\n","epoch 15, step 964, loss: 60.627277\n","epoch 15, step 965, loss: 25.598661\n","epoch 15, step 966, loss: 0.150198\n","epoch 15, step 967, loss: 2.557060\n","epoch 15, step 968, loss: 0.000000\n","epoch 15, step 969, loss: 3.025479\n","epoch 15, step 970, loss: 12.840384\n","epoch 15, step 971, loss: 0.000187\n","epoch 15, step 972, loss: 0.003049\n","epoch 15, step 973, loss: 0.000194\n","epoch 15, step 974, loss: 0.000000\n","epoch 15, step 975, loss: 0.000000\n","epoch 15, step 976, loss: 0.000000\n","epoch 15, step 977, loss: 0.000875\n","epoch 15, step 978, loss: 0.000000\n","epoch 15, step 979, loss: 12.228608\n","epoch 15, step 980, loss: 0.000003\n","epoch 15, step 981, loss: 0.000000\n","epoch 15, step 982, loss: 67.948853\n","epoch 15, step 983, loss: 0.002232\n","epoch 15, step 984, loss: 0.000000\n","epoch 15, step 985, loss: 0.000000\n","epoch 15, step 986, loss: 78.054001\n","epoch 15, step 987, loss: 0.027026\n","epoch 15, step 988, loss: 0.066730\n","epoch 15, step 989, loss: 7.075987\n","epoch 15, step 990, loss: 0.000020\n","epoch 15, step 991, loss: 0.003345\n","epoch 15, step 992, loss: 0.000187\n","epoch 15, step 993, loss: 0.000001\n","epoch 15, step 994, loss: 0.000653\n","epoch 15, step 995, loss: 0.000000\n","epoch 15, step 996, loss: 0.114348\n","epoch 15, step 997, loss: 5.473352\n","epoch 15, step 998, loss: 0.000000\n","epoch 15, step 999, loss: 0.000483\n","epoch 15, step 1000, loss: 0.000000\n","epoch 15, step 1001, loss: 1.192621\n","epoch 15, step 1002, loss: 152.675415\n","epoch 15, step 1003, loss: 87.783630\n","epoch 15, step 1004, loss: 0.000005\n","epoch 15, step 1005, loss: 0.525964\n","epoch 15, step 1006, loss: 0.002128\n","epoch 15, step 1007, loss: 194.767624\n","epoch 16, step 1008, loss: 0.615690\n","epoch 16, step 1009, loss: 0.253410\n","epoch 16, step 1010, loss: 4.578135\n","epoch 16, step 1011, loss: 160.587234\n","epoch 16, step 1012, loss: 95.900566\n","epoch 16, step 1013, loss: 0.000000\n","epoch 16, step 1014, loss: 12.168027\n","epoch 16, step 1015, loss: 60.220192\n","epoch 16, step 1016, loss: 11.598892\n","epoch 16, step 1017, loss: 151.322021\n","epoch 16, step 1018, loss: 46.601723\n","epoch 16, step 1019, loss: 21.181841\n","epoch 16, step 1020, loss: 20.387156\n","epoch 16, step 1021, loss: 0.000000\n","epoch 16, step 1022, loss: 44.228104\n","epoch 16, step 1023, loss: 0.000002\n","epoch 16, step 1024, loss: 53.721241\n","epoch 16, step 1025, loss: 0.000000\n","epoch 16, step 1026, loss: 207.417999\n","epoch 16, step 1027, loss: 80.001183\n","epoch 16, step 1028, loss: 0.000000\n","epoch 16, step 1029, loss: 0.016143\n","epoch 16, step 1030, loss: 0.000000\n","epoch 16, step 1031, loss: 0.000000\n","epoch 16, step 1032, loss: 0.000002\n","epoch 16, step 1033, loss: 0.000000\n","epoch 16, step 1034, loss: 0.000021\n","epoch 16, step 1035, loss: 23.974966\n","epoch 16, step 1036, loss: 0.000108\n","epoch 16, step 1037, loss: 0.000000\n","epoch 16, step 1038, loss: 0.000018\n","epoch 16, step 1039, loss: 0.000009\n","epoch 16, step 1040, loss: 27.433542\n","epoch 16, step 1041, loss: 0.000000\n","epoch 16, step 1042, loss: 32.346924\n","epoch 16, step 1043, loss: 0.136425\n","epoch 16, step 1044, loss: 0.005351\n","epoch 16, step 1045, loss: 42.552193\n","epoch 16, step 1046, loss: 0.000000\n","epoch 16, step 1047, loss: 0.000000\n","epoch 16, step 1048, loss: 0.000001\n","epoch 16, step 1049, loss: 67.485229\n","epoch 16, step 1050, loss: 0.000001\n","epoch 16, step 1051, loss: 0.000000\n","epoch 16, step 1052, loss: 0.011352\n","epoch 16, step 1053, loss: 0.385041\n","epoch 16, step 1054, loss: 0.000167\n","epoch 16, step 1055, loss: 0.806222\n","epoch 16, step 1056, loss: 0.000160\n","epoch 16, step 1057, loss: 29.906284\n","epoch 16, step 1058, loss: 0.000035\n","epoch 16, step 1059, loss: 0.000008\n","epoch 16, step 1060, loss: 0.000001\n","epoch 16, step 1061, loss: 0.000000\n","epoch 16, step 1062, loss: 0.089661\n","epoch 16, step 1063, loss: 0.000044\n","epoch 16, step 1064, loss: 0.000006\n","epoch 16, step 1065, loss: 62.150269\n","epoch 16, step 1066, loss: 27.926748\n","epoch 16, step 1067, loss: 3.471995\n","epoch 16, step 1068, loss: 0.000000\n","epoch 16, step 1069, loss: 0.000257\n","epoch 16, step 1070, loss: 172.682022\n","epoch 17, step 1071, loss: 2.062550\n","epoch 17, step 1072, loss: 9.807607\n","epoch 17, step 1073, loss: 0.938264\n","epoch 17, step 1074, loss: 31.655848\n","epoch 17, step 1075, loss: 60.859467\n","epoch 17, step 1076, loss: 0.000000\n","epoch 17, step 1077, loss: 0.000066\n","epoch 17, step 1078, loss: 0.001713\n","epoch 17, step 1079, loss: 12.275218\n","epoch 17, step 1080, loss: 96.054932\n","epoch 17, step 1081, loss: 66.202988\n","epoch 17, step 1082, loss: 43.127453\n","epoch 17, step 1083, loss: 14.505396\n","epoch 17, step 1084, loss: 9.537306\n","epoch 17, step 1085, loss: 8.274022\n","epoch 17, step 1086, loss: 6.846303\n","epoch 17, step 1087, loss: 34.194088\n","epoch 17, step 1088, loss: 0.014356\n","epoch 17, step 1089, loss: 190.008087\n","epoch 17, step 1090, loss: 59.308212\n","epoch 17, step 1091, loss: 0.001641\n","epoch 17, step 1092, loss: 0.171322\n","epoch 17, step 1093, loss: 0.000000\n","epoch 17, step 1094, loss: 0.000000\n","epoch 17, step 1095, loss: 3.647392\n","epoch 17, step 1096, loss: 0.000001\n","epoch 17, step 1097, loss: 14.927702\n","epoch 17, step 1098, loss: 3.273180\n","epoch 17, step 1099, loss: 0.027997\n","epoch 17, step 1100, loss: 0.000000\n","epoch 17, step 1101, loss: 0.000438\n","epoch 17, step 1102, loss: 0.000001\n","epoch 17, step 1103, loss: 0.000001\n","epoch 17, step 1104, loss: 0.000021\n","epoch 17, step 1105, loss: 0.000000\n","epoch 17, step 1106, loss: 8.898122\n","epoch 17, step 1107, loss: 0.000000\n","epoch 17, step 1108, loss: 33.848011\n","epoch 17, step 1109, loss: 1.753004\n","epoch 17, step 1110, loss: 0.000000\n","epoch 17, step 1111, loss: 0.653869\n","epoch 17, step 1112, loss: 88.699158\n","epoch 17, step 1113, loss: 18.938013\n","epoch 17, step 1114, loss: 22.073429\n","epoch 17, step 1115, loss: 11.317566\n","epoch 17, step 1116, loss: 0.000000\n","epoch 17, step 1117, loss: 0.000024\n","epoch 17, step 1118, loss: 1.982523\n","epoch 17, step 1119, loss: 0.093443\n","epoch 17, step 1120, loss: 0.000207\n","epoch 17, step 1121, loss: 0.000000\n","epoch 17, step 1122, loss: 14.061522\n","epoch 17, step 1123, loss: 0.000010\n","epoch 17, step 1124, loss: 0.000000\n","epoch 17, step 1125, loss: 1.055882\n","epoch 17, step 1126, loss: 0.000181\n","epoch 17, step 1127, loss: 0.000019\n","epoch 17, step 1128, loss: 16.366117\n","epoch 17, step 1129, loss: 5.972034\n","epoch 17, step 1130, loss: 0.000000\n","epoch 17, step 1131, loss: 0.000000\n","epoch 17, step 1132, loss: 0.001584\n","epoch 17, step 1133, loss: 164.374542\n","epoch 18, step 1134, loss: 35.689556\n","epoch 18, step 1135, loss: 0.000000\n","epoch 18, step 1136, loss: 0.069863\n","epoch 18, step 1137, loss: 14.083384\n","epoch 18, step 1138, loss: 43.693054\n","epoch 18, step 1139, loss: 0.000000\n","epoch 18, step 1140, loss: 1.416710\n","epoch 18, step 1141, loss: 0.001197\n","epoch 18, step 1142, loss: 11.849533\n","epoch 18, step 1143, loss: 86.721771\n","epoch 18, step 1144, loss: 37.004795\n","epoch 18, step 1145, loss: 17.069618\n","epoch 18, step 1146, loss: 0.025747\n","epoch 18, step 1147, loss: 0.857625\n","epoch 18, step 1148, loss: 1.913661\n","epoch 18, step 1149, loss: 0.480183\n","epoch 18, step 1150, loss: 1.770118\n","epoch 18, step 1151, loss: 44.849525\n","epoch 18, step 1152, loss: 172.877365\n","epoch 18, step 1153, loss: 39.889107\n","epoch 18, step 1154, loss: 5.060049\n","epoch 18, step 1155, loss: 1.810491\n","epoch 18, step 1156, loss: 0.052318\n","epoch 18, step 1157, loss: 0.007242\n","epoch 18, step 1158, loss: 9.344646\n","epoch 18, step 1159, loss: 16.094692\n","epoch 18, step 1160, loss: 0.000023\n","epoch 18, step 1161, loss: 0.655898\n","epoch 18, step 1162, loss: 0.000000\n","epoch 18, step 1163, loss: 0.000000\n","epoch 18, step 1164, loss: 0.000009\n","epoch 18, step 1165, loss: 0.000000\n","epoch 18, step 1166, loss: 8.775934\n","epoch 18, step 1167, loss: 0.000000\n","epoch 18, step 1168, loss: 0.000095\n","epoch 18, step 1169, loss: 0.000027\n","epoch 18, step 1170, loss: 0.100220\n","epoch 18, step 1171, loss: 0.082342\n","epoch 18, step 1172, loss: 0.000015\n","epoch 18, step 1173, loss: 0.000000\n","epoch 18, step 1174, loss: 0.002864\n","epoch 18, step 1175, loss: 61.342678\n","epoch 18, step 1176, loss: 0.000000\n","epoch 18, step 1177, loss: 0.000000\n","epoch 18, step 1178, loss: 0.000003\n","epoch 18, step 1179, loss: 0.000000\n","epoch 18, step 1180, loss: 0.000000\n","epoch 18, step 1181, loss: 0.000000\n","epoch 18, step 1182, loss: 12.241868\n","epoch 18, step 1183, loss: 13.713349\n","epoch 18, step 1184, loss: 0.000000\n","epoch 18, step 1185, loss: 0.000000\n","epoch 18, step 1186, loss: 0.741419\n","epoch 18, step 1187, loss: 0.000133\n","epoch 18, step 1188, loss: 0.000000\n","epoch 18, step 1189, loss: 6.171553\n","epoch 18, step 1190, loss: 0.000316\n","epoch 18, step 1191, loss: 10.895980\n","epoch 18, step 1192, loss: 75.119476\n","epoch 18, step 1193, loss: 7.241781\n","epoch 18, step 1194, loss: 0.000000\n","epoch 18, step 1195, loss: 0.000226\n","epoch 18, step 1196, loss: 170.830688\n","epoch 19, step 1197, loss: 0.394231\n","epoch 19, step 1198, loss: 0.000000\n","epoch 19, step 1199, loss: 0.000799\n","epoch 19, step 1200, loss: 4.569418\n","epoch 19, step 1201, loss: 26.575449\n","epoch 19, step 1202, loss: 0.000000\n","epoch 19, step 1203, loss: 0.000005\n","epoch 19, step 1204, loss: 0.000017\n","epoch 19, step 1205, loss: 0.377181\n","epoch 19, step 1206, loss: 46.792652\n","epoch 19, step 1207, loss: 50.389023\n","epoch 19, step 1208, loss: 24.423462\n","epoch 19, step 1209, loss: 0.000111\n","epoch 19, step 1210, loss: 0.000158\n","epoch 19, step 1211, loss: 14.881777\n","epoch 19, step 1212, loss: 0.000020\n","epoch 19, step 1213, loss: 0.012643\n","epoch 19, step 1214, loss: 11.025431\n","epoch 19, step 1215, loss: 165.176270\n","epoch 19, step 1216, loss: 26.507242\n","epoch 19, step 1217, loss: 29.379549\n","epoch 19, step 1218, loss: 6.555775\n","epoch 19, step 1219, loss: 8.147761\n","epoch 19, step 1220, loss: 13.456327\n","epoch 19, step 1221, loss: 36.206745\n","epoch 19, step 1222, loss: 31.856997\n","epoch 19, step 1223, loss: 0.000001\n","epoch 19, step 1224, loss: 1.804138\n","epoch 19, step 1225, loss: 0.000000\n","epoch 19, step 1226, loss: 0.000000\n","epoch 19, step 1227, loss: 0.000001\n","epoch 19, step 1228, loss: 0.000000\n","epoch 19, step 1229, loss: 0.000000\n","epoch 19, step 1230, loss: 0.000000\n","epoch 19, step 1231, loss: 2.941869\n","epoch 19, step 1232, loss: 0.000016\n","epoch 19, step 1233, loss: 0.000519\n","epoch 19, step 1234, loss: 3.347086\n","epoch 19, step 1235, loss: 0.001002\n","epoch 19, step 1236, loss: 0.000000\n","epoch 19, step 1237, loss: 0.000960\n","epoch 19, step 1238, loss: 45.487736\n","epoch 19, step 1239, loss: 0.000000\n","epoch 19, step 1240, loss: 0.356841\n","epoch 19, step 1241, loss: 0.000022\n","epoch 19, step 1242, loss: 0.000000\n","epoch 19, step 1243, loss: 10.702347\n","epoch 19, step 1244, loss: 6.550535\n","epoch 19, step 1245, loss: 0.009091\n","epoch 19, step 1246, loss: 0.000004\n","epoch 19, step 1247, loss: 0.000000\n","epoch 19, step 1248, loss: 11.260874\n","epoch 19, step 1249, loss: 0.010904\n","epoch 19, step 1250, loss: 8.387866\n","epoch 19, step 1251, loss: 0.000000\n","epoch 19, step 1252, loss: 0.000000\n","epoch 19, step 1253, loss: 0.000050\n","epoch 19, step 1254, loss: 0.133156\n","epoch 19, step 1255, loss: 83.642860\n","epoch 19, step 1256, loss: 0.000000\n","epoch 19, step 1257, loss: 0.000000\n","epoch 19, step 1258, loss: 0.056553\n","epoch 19, step 1259, loss: 179.660339\n","epoch 20, step 1260, loss: 92.192627\n","epoch 20, step 1261, loss: 0.000000\n","epoch 20, step 1262, loss: 0.001608\n","epoch 20, step 1263, loss: 51.379101\n","epoch 20, step 1264, loss: 0.001831\n","epoch 20, step 1265, loss: 0.000000\n","epoch 20, step 1266, loss: 0.000322\n","epoch 20, step 1267, loss: 0.000000\n","epoch 20, step 1268, loss: 0.000000\n","epoch 20, step 1269, loss: 28.281208\n","epoch 20, step 1270, loss: 5.593162\n","epoch 20, step 1271, loss: 6.368423\n","epoch 20, step 1272, loss: 0.000000\n","epoch 20, step 1273, loss: 0.000000\n","epoch 20, step 1274, loss: 0.001165\n","epoch 20, step 1275, loss: 0.000000\n","epoch 20, step 1276, loss: 1.695865\n","epoch 20, step 1277, loss: 0.000000\n","epoch 20, step 1278, loss: 147.119690\n","epoch 20, step 1279, loss: 7.565599\n","epoch 20, step 1280, loss: 0.000000\n","epoch 20, step 1281, loss: 0.011026\n","epoch 20, step 1282, loss: 0.000000\n","epoch 20, step 1283, loss: 0.000000\n","epoch 20, step 1284, loss: 0.000014\n","epoch 20, step 1285, loss: 17.241209\n","epoch 20, step 1286, loss: 0.000012\n","epoch 20, step 1287, loss: 0.002273\n","epoch 20, step 1288, loss: 0.000000\n","epoch 20, step 1289, loss: 0.000010\n","epoch 20, step 1290, loss: 0.024140\n","epoch 20, step 1291, loss: 0.000000\n","epoch 20, step 1292, loss: 0.000000\n","epoch 20, step 1293, loss: 0.025446\n","epoch 20, step 1294, loss: 0.000000\n","epoch 20, step 1295, loss: 20.466969\n","epoch 20, step 1296, loss: 14.717635\n","epoch 20, step 1297, loss: 0.000000\n","epoch 20, step 1298, loss: 31.329775\n","epoch 20, step 1299, loss: 1.291722\n","epoch 20, step 1300, loss: 0.000000\n","epoch 20, step 1301, loss: 26.043699\n","epoch 20, step 1302, loss: 0.000001\n","epoch 20, step 1303, loss: 0.000016\n","epoch 20, step 1304, loss: 0.236836\n","epoch 20, step 1305, loss: 0.000026\n","epoch 20, step 1306, loss: 0.001682\n","epoch 20, step 1307, loss: 0.000000\n","epoch 20, step 1308, loss: 0.000208\n","epoch 20, step 1309, loss: 0.000000\n","epoch 20, step 1310, loss: 0.000000\n","epoch 20, step 1311, loss: 0.000000\n","epoch 20, step 1312, loss: 0.014046\n","epoch 20, step 1313, loss: 0.000000\n","epoch 20, step 1314, loss: 0.000000\n","epoch 20, step 1315, loss: 0.000000\n","epoch 20, step 1316, loss: 0.865091\n","epoch 20, step 1317, loss: 21.905788\n","epoch 20, step 1318, loss: 106.244415\n","epoch 20, step 1319, loss: 0.000017\n","epoch 20, step 1320, loss: 0.000000\n","epoch 20, step 1321, loss: 26.868111\n","epoch 20, step 1322, loss: 197.991928\n","epoch 21, step 1323, loss: 0.000001\n","epoch 21, step 1324, loss: 0.000000\n","epoch 21, step 1325, loss: 0.000019\n","epoch 21, step 1326, loss: 260.835999\n","epoch 21, step 1327, loss: 1.366380\n","epoch 21, step 1328, loss: 0.000000\n","epoch 21, step 1329, loss: 0.085204\n","epoch 21, step 1330, loss: 0.000000\n","epoch 21, step 1331, loss: 0.000000\n","epoch 21, step 1332, loss: 7.677805\n","epoch 21, step 1333, loss: 9.331998\n","epoch 21, step 1334, loss: 41.245331\n","epoch 21, step 1335, loss: 0.000000\n","epoch 21, step 1336, loss: 0.000000\n","epoch 21, step 1337, loss: 0.000172\n","epoch 21, step 1338, loss: 0.000001\n","epoch 21, step 1339, loss: 0.003289\n","epoch 21, step 1340, loss: 0.000000\n","epoch 21, step 1341, loss: 130.586182\n","epoch 21, step 1342, loss: 33.554012\n","epoch 21, step 1343, loss: 0.000000\n","epoch 21, step 1344, loss: 0.108819\n","epoch 21, step 1345, loss: 0.000007\n","epoch 21, step 1346, loss: 0.000000\n","epoch 21, step 1347, loss: 0.004549\n","epoch 21, step 1348, loss: 38.391281\n","epoch 21, step 1349, loss: 0.060541\n","epoch 21, step 1350, loss: 3.571688\n","epoch 21, step 1351, loss: 0.000016\n","epoch 21, step 1352, loss: 0.787366\n","epoch 21, step 1353, loss: 0.011145\n","epoch 21, step 1354, loss: 0.000000\n","epoch 21, step 1355, loss: 0.000000\n","epoch 21, step 1356, loss: 0.000035\n","epoch 21, step 1357, loss: 0.000028\n","epoch 21, step 1358, loss: 0.000000\n","epoch 21, step 1359, loss: 0.000075\n","epoch 21, step 1360, loss: 0.000000\n","epoch 21, step 1361, loss: 0.002283\n","epoch 21, step 1362, loss: 0.066734\n","epoch 21, step 1363, loss: 0.000000\n","epoch 21, step 1364, loss: 15.015295\n","epoch 21, step 1365, loss: 0.019130\n","epoch 21, step 1366, loss: 0.000559\n","epoch 21, step 1367, loss: 0.000124\n","epoch 21, step 1368, loss: 0.016336\n","epoch 21, step 1369, loss: 18.565094\n","epoch 21, step 1370, loss: 0.000006\n","epoch 21, step 1371, loss: 0.149038\n","epoch 21, step 1372, loss: 0.576362\n","epoch 21, step 1373, loss: 0.000000\n","epoch 21, step 1374, loss: 0.006132\n","epoch 21, step 1375, loss: 0.000000\n","epoch 21, step 1376, loss: 0.000000\n","epoch 21, step 1377, loss: 0.015603\n","epoch 21, step 1378, loss: 0.000001\n","epoch 21, step 1379, loss: 0.006411\n","epoch 21, step 1380, loss: 0.000000\n","epoch 21, step 1381, loss: 70.094864\n","epoch 21, step 1382, loss: 0.000000\n","epoch 21, step 1383, loss: 0.000000\n","epoch 21, step 1384, loss: 0.000038\n","epoch 21, step 1385, loss: 162.268631\n","epoch 22, step 1386, loss: 0.000000\n","epoch 22, step 1387, loss: 0.000228\n","epoch 22, step 1388, loss: 0.097789\n","epoch 22, step 1389, loss: 60.691071\n","epoch 22, step 1390, loss: 0.000000\n","epoch 22, step 1391, loss: 0.000060\n","epoch 22, step 1392, loss: 0.000000\n","epoch 22, step 1393, loss: 0.000001\n","epoch 22, step 1394, loss: 0.000000\n","epoch 22, step 1395, loss: 6.080047\n","epoch 22, step 1396, loss: 13.446804\n","epoch 22, step 1397, loss: 8.254119\n","epoch 22, step 1398, loss: 0.000001\n","epoch 22, step 1399, loss: 0.000000\n","epoch 22, step 1400, loss: 0.001615\n","epoch 22, step 1401, loss: 2.086135\n","epoch 22, step 1402, loss: 0.000059\n","epoch 22, step 1403, loss: 0.000000\n","epoch 22, step 1404, loss: 113.699142\n","epoch 22, step 1405, loss: 34.184952\n","epoch 22, step 1406, loss: 0.000000\n","epoch 22, step 1407, loss: 0.001452\n","epoch 22, step 1408, loss: 0.000000\n","epoch 22, step 1409, loss: 0.000000\n","epoch 22, step 1410, loss: 0.004024\n","epoch 22, step 1411, loss: 30.215508\n","epoch 22, step 1412, loss: 0.000001\n","epoch 22, step 1413, loss: 0.000522\n","epoch 22, step 1414, loss: 0.000270\n","epoch 22, step 1415, loss: 0.000000\n","epoch 22, step 1416, loss: 0.000405\n","epoch 22, step 1417, loss: 0.000000\n","epoch 22, step 1418, loss: 0.000000\n","epoch 22, step 1419, loss: 0.000001\n","epoch 22, step 1420, loss: 0.000000\n","epoch 22, step 1421, loss: 0.000000\n","epoch 22, step 1422, loss: 0.000191\n","epoch 22, step 1423, loss: 0.000000\n","epoch 22, step 1424, loss: 0.019590\n","epoch 22, step 1425, loss: 0.039593\n","epoch 22, step 1426, loss: 0.000001\n","epoch 22, step 1427, loss: 12.648235\n","epoch 22, step 1428, loss: 0.003555\n","epoch 22, step 1429, loss: 0.000274\n","epoch 22, step 1430, loss: 1.248158\n","epoch 22, step 1431, loss: 1.730989\n","epoch 22, step 1432, loss: 0.000001\n","epoch 22, step 1433, loss: 0.006327\n","epoch 22, step 1434, loss: 0.123962\n","epoch 22, step 1435, loss: 0.000000\n","epoch 22, step 1436, loss: 0.000000\n","epoch 22, step 1437, loss: 0.016150\n","epoch 22, step 1438, loss: 0.000000\n","epoch 22, step 1439, loss: 0.000000\n","epoch 22, step 1440, loss: 5.842899\n","epoch 22, step 1441, loss: 0.000061\n","epoch 22, step 1442, loss: 0.009208\n","epoch 22, step 1443, loss: 0.000014\n","epoch 22, step 1444, loss: 70.318192\n","epoch 22, step 1445, loss: 0.000000\n","epoch 22, step 1446, loss: 0.000000\n","epoch 22, step 1447, loss: 0.006045\n","epoch 22, step 1448, loss: 151.920410\n","epoch 23, step 1449, loss: 101.004044\n","epoch 23, step 1450, loss: 0.000002\n","epoch 23, step 1451, loss: 0.008983\n","epoch 23, step 1452, loss: 54.028126\n","epoch 23, step 1453, loss: 0.000000\n","epoch 23, step 1454, loss: 0.000000\n","epoch 23, step 1455, loss: 0.000000\n","epoch 23, step 1456, loss: 0.000000\n","epoch 23, step 1457, loss: 0.000000\n","epoch 23, step 1458, loss: 0.007062\n","epoch 23, step 1459, loss: 0.000002\n","epoch 23, step 1460, loss: 20.503983\n","epoch 23, step 1461, loss: 0.000000\n","epoch 23, step 1462, loss: 0.000000\n","epoch 23, step 1463, loss: 8.799072\n","epoch 23, step 1464, loss: 27.260120\n","epoch 23, step 1465, loss: 0.000001\n","epoch 23, step 1466, loss: 5.199711\n","epoch 23, step 1467, loss: 110.247322\n","epoch 23, step 1468, loss: 15.044241\n","epoch 23, step 1469, loss: 0.000001\n","epoch 23, step 1470, loss: 0.000148\n","epoch 23, step 1471, loss: 0.000000\n","epoch 23, step 1472, loss: 0.000000\n","epoch 23, step 1473, loss: 0.000045\n","epoch 23, step 1474, loss: 18.902950\n","epoch 23, step 1475, loss: 0.000000\n","epoch 23, step 1476, loss: 5.904619\n","epoch 23, step 1477, loss: 4.204298\n","epoch 23, step 1478, loss: 0.000000\n","epoch 23, step 1479, loss: 0.537217\n","epoch 23, step 1480, loss: 0.000000\n","epoch 23, step 1481, loss: 0.000003\n","epoch 23, step 1482, loss: 0.000048\n","epoch 23, step 1483, loss: 0.000037\n","epoch 23, step 1484, loss: 0.000003\n","epoch 23, step 1485, loss: 0.044323\n","epoch 23, step 1486, loss: 0.000000\n","epoch 23, step 1487, loss: 0.069807\n","epoch 23, step 1488, loss: 10.528499\n","epoch 23, step 1489, loss: 0.000037\n","epoch 23, step 1490, loss: 22.454559\n","epoch 23, step 1491, loss: 4.580088\n","epoch 23, step 1492, loss: 0.000113\n","epoch 23, step 1493, loss: 0.000000\n","epoch 23, step 1494, loss: 0.042794\n","epoch 23, step 1495, loss: 0.000000\n","epoch 23, step 1496, loss: 0.000000\n","epoch 23, step 1497, loss: 7.831903\n","epoch 23, step 1498, loss: 0.000000\n","epoch 23, step 1499, loss: 0.000000\n","epoch 23, step 1500, loss: 0.000120\n","epoch 23, step 1501, loss: 0.000000\n","epoch 23, step 1502, loss: 0.000000\n","epoch 23, step 1503, loss: 0.000000\n","epoch 23, step 1504, loss: 5.729609\n","epoch 23, step 1505, loss: 0.943708\n","epoch 23, step 1506, loss: 79.648460\n","epoch 23, step 1507, loss: 69.941849\n","epoch 23, step 1508, loss: 0.000000\n","epoch 23, step 1509, loss: 0.000000\n","epoch 23, step 1510, loss: 0.000049\n","epoch 23, step 1511, loss: 91.973541\n","epoch 24, step 1512, loss: 0.000000\n","epoch 24, step 1513, loss: 0.000000\n","epoch 24, step 1514, loss: 0.005319\n","epoch 24, step 1515, loss: 194.397995\n","epoch 24, step 1516, loss: 0.000000\n","epoch 24, step 1517, loss: 0.000000\n","epoch 24, step 1518, loss: 0.000233\n","epoch 24, step 1519, loss: 0.000003\n","epoch 24, step 1520, loss: 0.000000\n","epoch 24, step 1521, loss: 5.578499\n","epoch 24, step 1522, loss: 37.334442\n","epoch 24, step 1523, loss: 6.694188\n","epoch 24, step 1524, loss: 0.000003\n","epoch 24, step 1525, loss: 0.000000\n","epoch 24, step 1526, loss: 0.000062\n","epoch 24, step 1527, loss: 0.000000\n","epoch 24, step 1528, loss: 0.000000\n","epoch 24, step 1529, loss: 0.000000\n","epoch 24, step 1530, loss: 104.741158\n","epoch 24, step 1531, loss: 42.211132\n","epoch 24, step 1532, loss: 0.000000\n","epoch 24, step 1533, loss: 0.031015\n","epoch 24, step 1534, loss: 0.000000\n","epoch 24, step 1535, loss: 0.000000\n","epoch 24, step 1536, loss: 0.008311\n","epoch 24, step 1537, loss: 24.597742\n","epoch 24, step 1538, loss: 0.000001\n","epoch 24, step 1539, loss: 0.000002\n","epoch 24, step 1540, loss: 0.000000\n","epoch 24, step 1541, loss: 0.000000\n","epoch 24, step 1542, loss: 0.000000\n","epoch 24, step 1543, loss: 0.000000\n","epoch 24, step 1544, loss: 0.000000\n","epoch 24, step 1545, loss: 0.000000\n","epoch 24, step 1546, loss: 0.000006\n","epoch 24, step 1547, loss: 0.000000\n","epoch 24, step 1548, loss: 0.000004\n","epoch 24, step 1549, loss: 0.000000\n","epoch 24, step 1550, loss: 0.000000\n","epoch 24, step 1551, loss: 0.000000\n","epoch 24, step 1552, loss: 0.000000\n","epoch 24, step 1553, loss: 18.984415\n","epoch 24, step 1554, loss: 0.000000\n","epoch 24, step 1555, loss: 0.000301\n","epoch 24, step 1556, loss: 1.761577\n","epoch 24, step 1557, loss: 0.000018\n","epoch 24, step 1558, loss: 0.000000\n","epoch 24, step 1559, loss: 0.000255\n","epoch 24, step 1560, loss: 0.000238\n","epoch 24, step 1561, loss: 0.000000\n","epoch 24, step 1562, loss: 0.000000\n","epoch 24, step 1563, loss: 0.001573\n","epoch 24, step 1564, loss: 0.000000\n","epoch 24, step 1565, loss: 0.000000\n","epoch 24, step 1566, loss: 0.000000\n","epoch 24, step 1567, loss: 0.000102\n","epoch 24, step 1568, loss: 0.244334\n","epoch 24, step 1569, loss: 0.000162\n","epoch 24, step 1570, loss: 53.122780\n","epoch 24, step 1571, loss: 0.000000\n","epoch 24, step 1572, loss: 0.000000\n","epoch 24, step 1573, loss: 0.148629\n","epoch 24, step 1574, loss: 107.049156\n","epoch 25, step 1575, loss: 0.000441\n","epoch 25, step 1576, loss: 0.000314\n","epoch 25, step 1577, loss: 2.664406\n","epoch 25, step 1578, loss: 1.398261\n","epoch 25, step 1579, loss: 0.000000\n","epoch 25, step 1580, loss: 0.000061\n","epoch 25, step 1581, loss: 3.483750\n","epoch 25, step 1582, loss: 0.000883\n","epoch 25, step 1583, loss: 0.000000\n","epoch 25, step 1584, loss: 3.611375\n","epoch 25, step 1585, loss: 25.250917\n","epoch 25, step 1586, loss: 39.298241\n","epoch 25, step 1587, loss: 0.000102\n","epoch 25, step 1588, loss: 0.000000\n","epoch 25, step 1589, loss: 0.000000\n","epoch 25, step 1590, loss: 1.501047\n","epoch 25, step 1591, loss: 0.000000\n","epoch 25, step 1592, loss: 0.000000\n","epoch 25, step 1593, loss: 77.824585\n","epoch 25, step 1594, loss: 38.339989\n","epoch 25, step 1595, loss: 0.000000\n","epoch 25, step 1596, loss: 0.021244\n","epoch 25, step 1597, loss: 0.000000\n","epoch 25, step 1598, loss: 0.000000\n","epoch 25, step 1599, loss: 0.023352\n","epoch 25, step 1600, loss: 25.251047\n","epoch 25, step 1601, loss: 0.000000\n","epoch 25, step 1602, loss: 0.000379\n","epoch 25, step 1603, loss: 0.000105\n","epoch 25, step 1604, loss: 0.000000\n","epoch 25, step 1605, loss: 0.000000\n","epoch 25, step 1606, loss: 0.000000\n","epoch 25, step 1607, loss: 0.000000\n","epoch 25, step 1608, loss: 0.000000\n","epoch 25, step 1609, loss: 0.000038\n","epoch 25, step 1610, loss: 0.000001\n","epoch 25, step 1611, loss: 0.000059\n","epoch 25, step 1612, loss: 0.000000\n","epoch 25, step 1613, loss: 0.000000\n","epoch 25, step 1614, loss: 0.000002\n","epoch 25, step 1615, loss: 0.000003\n","epoch 25, step 1616, loss: 13.381329\n","epoch 25, step 1617, loss: 0.000000\n","epoch 25, step 1618, loss: 0.000172\n","epoch 25, step 1619, loss: 0.000000\n","epoch 25, step 1620, loss: 0.000015\n","epoch 25, step 1621, loss: 0.000000\n","epoch 25, step 1622, loss: 0.000003\n","epoch 25, step 1623, loss: 0.000208\n","epoch 25, step 1624, loss: 0.000000\n","epoch 25, step 1625, loss: 0.000000\n","epoch 25, step 1626, loss: 0.141250\n","epoch 25, step 1627, loss: 0.000000\n","epoch 25, step 1628, loss: 0.000000\n","epoch 25, step 1629, loss: 0.000000\n","epoch 25, step 1630, loss: 0.000001\n","epoch 25, step 1631, loss: 0.000509\n","epoch 25, step 1632, loss: 0.000246\n","epoch 25, step 1633, loss: 64.155411\n","epoch 25, step 1634, loss: 0.000000\n","epoch 25, step 1635, loss: 0.000000\n","epoch 25, step 1636, loss: 0.008464\n","epoch 25, step 1637, loss: 102.958115\n","epoch 26, step 1638, loss: 0.000101\n","epoch 26, step 1639, loss: 0.000437\n","epoch 26, step 1640, loss: 0.000001\n","epoch 26, step 1641, loss: 1.441432\n","epoch 26, step 1642, loss: 0.000000\n","epoch 26, step 1643, loss: 0.000083\n","epoch 26, step 1644, loss: 0.000000\n","epoch 26, step 1645, loss: 0.000035\n","epoch 26, step 1646, loss: 0.000000\n","epoch 26, step 1647, loss: 5.559463\n","epoch 26, step 1648, loss: 21.029661\n","epoch 26, step 1649, loss: 11.264337\n","epoch 26, step 1650, loss: 0.000035\n","epoch 26, step 1651, loss: 0.000000\n","epoch 26, step 1652, loss: 0.000000\n","epoch 26, step 1653, loss: 4.726866\n","epoch 26, step 1654, loss: 0.000041\n","epoch 26, step 1655, loss: 0.000000\n","epoch 26, step 1656, loss: 57.818794\n","epoch 26, step 1657, loss: 33.635532\n","epoch 26, step 1658, loss: 0.000000\n","epoch 26, step 1659, loss: 0.095074\n","epoch 26, step 1660, loss: 0.000000\n","epoch 26, step 1661, loss: 0.000000\n","epoch 26, step 1662, loss: 0.310322\n","epoch 26, step 1663, loss: 29.403257\n","epoch 26, step 1664, loss: 0.000003\n","epoch 26, step 1665, loss: 0.001885\n","epoch 26, step 1666, loss: 0.000146\n","epoch 26, step 1667, loss: 0.000000\n","epoch 26, step 1668, loss: 0.000000\n","epoch 26, step 1669, loss: 0.000000\n","epoch 26, step 1670, loss: 0.000000\n","epoch 26, step 1671, loss: 0.000000\n","epoch 26, step 1672, loss: 0.000056\n","epoch 26, step 1673, loss: 0.000033\n","epoch 26, step 1674, loss: 0.000087\n","epoch 26, step 1675, loss: 0.000000\n","epoch 26, step 1676, loss: 0.000004\n","epoch 26, step 1677, loss: 0.000001\n","epoch 26, step 1678, loss: 0.000011\n","epoch 26, step 1679, loss: 14.282634\n","epoch 26, step 1680, loss: 0.000000\n","epoch 26, step 1681, loss: 0.000899\n","epoch 26, step 1682, loss: 0.000000\n","epoch 26, step 1683, loss: 0.000103\n","epoch 26, step 1684, loss: 0.000001\n","epoch 26, step 1685, loss: 0.000723\n","epoch 26, step 1686, loss: 0.012919\n","epoch 26, step 1687, loss: 0.000003\n","epoch 26, step 1688, loss: 0.000000\n","epoch 26, step 1689, loss: 0.064549\n","epoch 26, step 1690, loss: 0.000000\n","epoch 26, step 1691, loss: 0.000000\n","epoch 26, step 1692, loss: 0.000000\n","epoch 26, step 1693, loss: 0.000001\n","epoch 26, step 1694, loss: 0.017183\n","epoch 26, step 1695, loss: 0.125729\n","epoch 26, step 1696, loss: 60.851006\n","epoch 26, step 1697, loss: 0.000000\n","epoch 26, step 1698, loss: 0.000000\n","epoch 26, step 1699, loss: 0.000421\n","epoch 26, step 1700, loss: 88.146973\n","epoch 27, step 1701, loss: 0.000000\n","epoch 27, step 1702, loss: 0.021033\n","epoch 27, step 1703, loss: 0.000007\n","epoch 27, step 1704, loss: 33.287045\n","epoch 27, step 1705, loss: 0.000000\n","epoch 27, step 1706, loss: 0.007236\n","epoch 27, step 1707, loss: 0.000003\n","epoch 27, step 1708, loss: 0.001512\n","epoch 27, step 1709, loss: 0.000003\n","epoch 27, step 1710, loss: 11.260117\n","epoch 27, step 1711, loss: 37.018845\n","epoch 27, step 1712, loss: 48.947781\n","epoch 27, step 1713, loss: 0.013342\n","epoch 27, step 1714, loss: 0.000041\n","epoch 27, step 1715, loss: 0.000046\n","epoch 27, step 1716, loss: 6.874992\n","epoch 27, step 1717, loss: 0.016240\n","epoch 27, step 1718, loss: 0.000000\n","epoch 27, step 1719, loss: 39.856499\n","epoch 27, step 1720, loss: 36.019974\n","epoch 27, step 1721, loss: 0.000000\n","epoch 27, step 1722, loss: 0.003571\n","epoch 27, step 1723, loss: 0.000000\n","epoch 27, step 1724, loss: 0.000000\n","epoch 27, step 1725, loss: 0.000000\n","epoch 27, step 1726, loss: 29.772955\n","epoch 27, step 1727, loss: 0.000000\n","epoch 27, step 1728, loss: 0.135184\n","epoch 27, step 1729, loss: 0.000029\n","epoch 27, step 1730, loss: 0.000000\n","epoch 27, step 1731, loss: 5.153674\n","epoch 27, step 1732, loss: 0.000000\n","epoch 27, step 1733, loss: 0.000009\n","epoch 27, step 1734, loss: 0.000000\n","epoch 27, step 1735, loss: 0.000003\n","epoch 27, step 1736, loss: 0.000001\n","epoch 27, step 1737, loss: 0.000165\n","epoch 27, step 1738, loss: 0.000000\n","epoch 27, step 1739, loss: 0.000000\n","epoch 27, step 1740, loss: 0.000000\n","epoch 27, step 1741, loss: 0.000000\n","epoch 27, step 1742, loss: 21.755699\n","epoch 27, step 1743, loss: 0.000000\n","epoch 27, step 1744, loss: 0.000042\n","epoch 27, step 1745, loss: 0.000000\n","epoch 27, step 1746, loss: 0.000000\n","epoch 27, step 1747, loss: 0.000000\n","epoch 27, step 1748, loss: 0.000000\n","epoch 27, step 1749, loss: 20.432392\n","epoch 27, step 1750, loss: 0.000000\n","epoch 27, step 1751, loss: 0.000000\n","epoch 27, step 1752, loss: 0.000022\n","epoch 27, step 1753, loss: 0.000000\n","epoch 27, step 1754, loss: 0.000000\n","epoch 27, step 1755, loss: 0.000000\n","epoch 27, step 1756, loss: 0.000000\n","epoch 27, step 1757, loss: 0.000182\n","epoch 27, step 1758, loss: 0.000000\n","epoch 27, step 1759, loss: 75.108482\n","epoch 27, step 1760, loss: 0.000000\n","epoch 27, step 1761, loss: 0.000000\n","epoch 27, step 1762, loss: 0.019764\n","epoch 27, step 1763, loss: 82.282051\n","epoch 28, step 1764, loss: 102.757065\n","epoch 28, step 1765, loss: 0.000421\n","epoch 28, step 1766, loss: 0.000001\n","epoch 28, step 1767, loss: 46.728058\n","epoch 28, step 1768, loss: 0.000000\n","epoch 28, step 1769, loss: 0.000001\n","epoch 28, step 1770, loss: 0.000000\n","epoch 28, step 1771, loss: 0.000000\n","epoch 28, step 1772, loss: 0.000000\n","epoch 28, step 1773, loss: 0.009601\n","epoch 28, step 1774, loss: 7.694186\n","epoch 28, step 1775, loss: 25.669594\n","epoch 28, step 1776, loss: 0.000000\n","epoch 28, step 1777, loss: 0.000000\n","epoch 28, step 1778, loss: 0.000047\n","epoch 28, step 1779, loss: 0.861502\n","epoch 28, step 1780, loss: 0.000297\n","epoch 28, step 1781, loss: 0.000000\n","epoch 28, step 1782, loss: 52.482548\n","epoch 28, step 1783, loss: 7.426616\n","epoch 28, step 1784, loss: 13.276229\n","epoch 28, step 1785, loss: 2.394698\n","epoch 28, step 1786, loss: 0.000000\n","epoch 28, step 1787, loss: 0.000000\n","epoch 28, step 1788, loss: 0.000000\n","epoch 28, step 1789, loss: 0.265658\n","epoch 28, step 1790, loss: 0.000000\n","epoch 28, step 1791, loss: 0.433334\n","epoch 28, step 1792, loss: 0.001213\n","epoch 28, step 1793, loss: 0.000000\n","epoch 28, step 1794, loss: 0.000000\n","epoch 28, step 1795, loss: 0.000000\n","epoch 28, step 1796, loss: 0.000000\n","epoch 28, step 1797, loss: 0.000273\n","epoch 28, step 1798, loss: 0.005191\n","epoch 28, step 1799, loss: 0.000001\n","epoch 28, step 1800, loss: 0.000001\n","epoch 28, step 1801, loss: 0.000000\n","epoch 28, step 1802, loss: 0.006277\n","epoch 28, step 1803, loss: 0.000141\n","epoch 28, step 1804, loss: 0.000017\n","epoch 28, step 1805, loss: 9.068020\n","epoch 28, step 1806, loss: 0.000000\n","epoch 28, step 1807, loss: 0.009521\n","epoch 28, step 1808, loss: 0.000000\n","epoch 28, step 1809, loss: 0.000060\n","epoch 28, step 1810, loss: 0.000000\n","epoch 28, step 1811, loss: 0.002580\n","epoch 28, step 1812, loss: 0.049907\n","epoch 28, step 1813, loss: 0.000007\n","epoch 28, step 1814, loss: 0.000000\n","epoch 28, step 1815, loss: 11.247770\n","epoch 28, step 1816, loss: 0.000000\n","epoch 28, step 1817, loss: 0.000000\n","epoch 28, step 1818, loss: 0.000000\n","epoch 28, step 1819, loss: 0.000000\n","epoch 28, step 1820, loss: 0.001433\n","epoch 28, step 1821, loss: 28.425907\n","epoch 28, step 1822, loss: 31.416151\n","epoch 28, step 1823, loss: 0.000000\n","epoch 28, step 1824, loss: 0.000000\n","epoch 28, step 1825, loss: 0.000032\n","epoch 28, step 1826, loss: 81.009399\n","epoch 29, step 1827, loss: 0.000000\n","epoch 29, step 1828, loss: 0.011644\n","epoch 29, step 1829, loss: 0.000000\n","epoch 29, step 1830, loss: 301.278351\n","epoch 29, step 1831, loss: 0.000000\n","epoch 29, step 1832, loss: 0.000000\n","epoch 29, step 1833, loss: 0.000000\n","epoch 29, step 1834, loss: 0.000000\n","epoch 29, step 1835, loss: 0.000000\n","epoch 29, step 1836, loss: 0.002015\n","epoch 29, step 1837, loss: 20.455040\n","epoch 29, step 1838, loss: 30.646540\n","epoch 29, step 1839, loss: 0.000000\n","epoch 29, step 1840, loss: 0.000000\n","epoch 29, step 1841, loss: 0.000001\n","epoch 29, step 1842, loss: 0.000187\n","epoch 29, step 1843, loss: 0.000001\n","epoch 29, step 1844, loss: 0.000000\n","epoch 29, step 1845, loss: 14.357964\n","epoch 29, step 1846, loss: 30.105202\n","epoch 29, step 1847, loss: 0.000000\n","epoch 29, step 1848, loss: 0.061708\n","epoch 29, step 1849, loss: 0.000000\n","epoch 29, step 1850, loss: 0.000000\n","epoch 29, step 1851, loss: 0.000003\n","epoch 29, step 1852, loss: 72.817039\n","epoch 29, step 1853, loss: 0.000000\n","epoch 29, step 1854, loss: 0.000994\n","epoch 29, step 1855, loss: 0.000563\n","epoch 29, step 1856, loss: 0.000000\n","epoch 29, step 1857, loss: 0.000000\n","epoch 29, step 1858, loss: 0.000000\n","epoch 29, step 1859, loss: 0.000000\n","epoch 29, step 1860, loss: 0.067615\n","epoch 29, step 1861, loss: 0.037481\n","epoch 29, step 1862, loss: 1.001765\n","epoch 29, step 1863, loss: 0.000762\n","epoch 29, step 1864, loss: 0.000000\n","epoch 29, step 1865, loss: 0.000802\n","epoch 29, step 1866, loss: 0.000062\n","epoch 29, step 1867, loss: 0.000201\n","epoch 29, step 1868, loss: 10.028017\n","epoch 29, step 1869, loss: 0.000000\n","epoch 29, step 1870, loss: 0.000277\n","epoch 29, step 1871, loss: 0.000000\n","epoch 29, step 1872, loss: 0.010789\n","epoch 29, step 1873, loss: 0.000000\n","epoch 29, step 1874, loss: 0.002565\n","epoch 29, step 1875, loss: 0.001799\n","epoch 29, step 1876, loss: 0.000000\n","epoch 29, step 1877, loss: 0.000005\n","epoch 29, step 1878, loss: 2.946444\n","epoch 29, step 1879, loss: 0.000000\n","epoch 29, step 1880, loss: 0.000000\n","epoch 29, step 1881, loss: 0.000000\n","epoch 29, step 1882, loss: 0.000044\n","epoch 29, step 1883, loss: 0.000136\n","epoch 29, step 1884, loss: 32.219303\n","epoch 29, step 1885, loss: 24.017191\n","epoch 29, step 1886, loss: 0.000000\n","epoch 29, step 1887, loss: 0.000000\n","epoch 29, step 1888, loss: 0.000045\n","epoch 29, step 1889, loss: 63.943779\n","epoch 30, step 1890, loss: 0.000000\n","epoch 30, step 1891, loss: 0.023266\n","epoch 30, step 1892, loss: 0.000002\n","epoch 30, step 1893, loss: 78.715652\n","epoch 30, step 1894, loss: 0.000000\n","epoch 30, step 1895, loss: 0.000004\n","epoch 30, step 1896, loss: 0.000000\n","epoch 30, step 1897, loss: 0.000127\n","epoch 30, step 1898, loss: 0.000000\n","epoch 30, step 1899, loss: 5.917521\n","epoch 30, step 1900, loss: 31.408209\n","epoch 30, step 1901, loss: 13.592223\n","epoch 30, step 1902, loss: 0.000001\n","epoch 30, step 1903, loss: 0.000000\n","epoch 30, step 1904, loss: 0.000000\n","epoch 30, step 1905, loss: 0.005332\n","epoch 30, step 1906, loss: 0.000003\n","epoch 30, step 1907, loss: 0.000000\n","epoch 30, step 1908, loss: 0.053891\n","epoch 30, step 1909, loss: 29.683006\n","epoch 30, step 1910, loss: 0.000000\n","epoch 30, step 1911, loss: 0.005973\n","epoch 30, step 1912, loss: 0.000000\n","epoch 30, step 1913, loss: 0.000000\n","epoch 30, step 1914, loss: 0.000001\n","epoch 30, step 1915, loss: 83.768326\n","epoch 30, step 1916, loss: 0.000015\n","epoch 30, step 1917, loss: 0.003292\n","epoch 30, step 1918, loss: 0.001816\n","epoch 30, step 1919, loss: 0.000000\n","epoch 30, step 1920, loss: 0.000000\n","epoch 30, step 1921, loss: 0.000000\n","epoch 30, step 1922, loss: 0.000000\n","epoch 30, step 1923, loss: 0.000001\n","epoch 30, step 1924, loss: 2.470589\n","epoch 30, step 1925, loss: 0.000000\n","epoch 30, step 1926, loss: 0.000276\n","epoch 30, step 1927, loss: 0.000000\n","epoch 30, step 1928, loss: 0.000011\n","epoch 30, step 1929, loss: 0.000005\n","epoch 30, step 1930, loss: 0.000000\n","epoch 30, step 1931, loss: 13.292108\n","epoch 30, step 1932, loss: 0.000000\n","epoch 30, step 1933, loss: 0.000047\n","epoch 30, step 1934, loss: 0.000001\n","epoch 30, step 1935, loss: 0.064342\n","epoch 30, step 1936, loss: 0.000000\n","epoch 30, step 1937, loss: 0.000000\n","epoch 30, step 1938, loss: 0.001216\n","epoch 30, step 1939, loss: 0.000000\n","epoch 30, step 1940, loss: 0.000000\n","epoch 30, step 1941, loss: 0.087460\n","epoch 30, step 1942, loss: 0.000000\n","epoch 30, step 1943, loss: 0.000000\n","epoch 30, step 1944, loss: 0.000000\n","epoch 30, step 1945, loss: 0.000001\n","epoch 30, step 1946, loss: 0.000003\n","epoch 30, step 1947, loss: 10.173577\n","epoch 30, step 1948, loss: 29.095045\n","epoch 30, step 1949, loss: 0.000000\n","epoch 30, step 1950, loss: 0.000000\n","epoch 30, step 1951, loss: 0.000001\n","epoch 30, step 1952, loss: 61.092934\n","epoch 31, step 1953, loss: 110.814720\n","epoch 31, step 1954, loss: 0.000001\n","epoch 31, step 1955, loss: 0.000000\n","epoch 31, step 1956, loss: 56.846134\n","epoch 31, step 1957, loss: 0.000000\n","epoch 31, step 1958, loss: 0.000009\n","epoch 31, step 1959, loss: 0.000000\n","epoch 31, step 1960, loss: 0.000000\n","epoch 31, step 1961, loss: 0.000000\n","epoch 31, step 1962, loss: 0.000003\n","epoch 31, step 1963, loss: 0.145591\n","epoch 31, step 1964, loss: 20.916048\n","epoch 31, step 1965, loss: 0.000000\n","epoch 31, step 1966, loss: 0.000000\n","epoch 31, step 1967, loss: 0.818522\n","epoch 31, step 1968, loss: 0.289526\n","epoch 31, step 1969, loss: 0.197189\n","epoch 31, step 1970, loss: 0.000000\n","epoch 31, step 1971, loss: 30.181406\n","epoch 31, step 1972, loss: 71.900566\n","epoch 31, step 1973, loss: 0.000008\n","epoch 31, step 1974, loss: 0.000133\n","epoch 31, step 1975, loss: 0.000000\n","epoch 31, step 1976, loss: 0.000000\n","epoch 31, step 1977, loss: 0.000000\n","epoch 31, step 1978, loss: 42.204758\n","epoch 31, step 1979, loss: 0.000000\n","epoch 31, step 1980, loss: 0.565166\n","epoch 31, step 1981, loss: 10.110640\n","epoch 31, step 1982, loss: 0.000000\n","epoch 31, step 1983, loss: 0.000000\n","epoch 31, step 1984, loss: 0.000000\n","epoch 31, step 1985, loss: 0.000000\n","epoch 31, step 1986, loss: 0.000056\n","epoch 31, step 1987, loss: 0.000058\n","epoch 31, step 1988, loss: 0.000000\n","epoch 31, step 1989, loss: 0.000039\n","epoch 31, step 1990, loss: 0.000000\n","epoch 31, step 1991, loss: 0.000699\n","epoch 31, step 1992, loss: 0.000000\n","epoch 31, step 1993, loss: 0.000021\n","epoch 31, step 1994, loss: 16.366911\n","epoch 31, step 1995, loss: 0.000000\n","epoch 31, step 1996, loss: 0.000019\n","epoch 31, step 1997, loss: 0.000000\n","epoch 31, step 1998, loss: 0.000324\n","epoch 31, step 1999, loss: 0.000000\n","epoch 31, step 2000, loss: 0.000003\n","epoch 31, step 2001, loss: 0.990218\n","epoch 31, step 2002, loss: 0.000092\n","epoch 31, step 2003, loss: 0.000000\n","epoch 31, step 2004, loss: 4.282104\n","epoch 31, step 2005, loss: 0.000000\n","epoch 31, step 2006, loss: 0.000000\n","epoch 31, step 2007, loss: 0.000000\n","epoch 31, step 2008, loss: 0.002844\n","epoch 31, step 2009, loss: 0.000189\n","epoch 31, step 2010, loss: 12.115887\n","epoch 31, step 2011, loss: 28.168282\n","epoch 31, step 2012, loss: 0.000000\n","epoch 31, step 2013, loss: 0.000000\n","epoch 31, step 2014, loss: 0.000007\n","epoch 31, step 2015, loss: 2.005736\n","epoch 32, step 2016, loss: 0.000000\n","epoch 32, step 2017, loss: 0.000080\n","epoch 32, step 2018, loss: 0.000006\n","epoch 32, step 2019, loss: 238.956085\n","epoch 32, step 2020, loss: 0.000000\n","epoch 32, step 2021, loss: 0.000001\n","epoch 32, step 2022, loss: 0.000000\n","epoch 32, step 2023, loss: 0.000002\n","epoch 32, step 2024, loss: 0.000000\n","epoch 32, step 2025, loss: 0.000071\n","epoch 32, step 2026, loss: 36.949242\n","epoch 32, step 2027, loss: 9.277518\n","epoch 32, step 2028, loss: 0.000005\n","epoch 32, step 2029, loss: 0.000000\n","epoch 32, step 2030, loss: 0.000001\n","epoch 32, step 2031, loss: 0.011232\n","epoch 32, step 2032, loss: 0.000008\n","epoch 32, step 2033, loss: 0.000000\n","epoch 32, step 2034, loss: 4.687383\n","epoch 32, step 2035, loss: 32.269787\n","epoch 32, step 2036, loss: 0.000002\n","epoch 32, step 2037, loss: 0.466052\n","epoch 32, step 2038, loss: 0.003034\n","epoch 32, step 2039, loss: 0.000000\n","epoch 32, step 2040, loss: 0.000043\n","epoch 32, step 2041, loss: 96.707642\n","epoch 32, step 2042, loss: 0.000004\n","epoch 32, step 2043, loss: 0.002560\n","epoch 32, step 2044, loss: 0.001012\n","epoch 32, step 2045, loss: 0.000000\n","epoch 32, step 2046, loss: 0.000000\n","epoch 32, step 2047, loss: 0.000000\n","epoch 32, step 2048, loss: 0.000001\n","epoch 32, step 2049, loss: 0.000205\n","epoch 32, step 2050, loss: 0.000002\n","epoch 32, step 2051, loss: 0.000000\n","epoch 32, step 2052, loss: 0.000904\n","epoch 32, step 2053, loss: 0.000004\n","epoch 32, step 2054, loss: 0.000052\n","epoch 32, step 2055, loss: 0.000009\n","epoch 32, step 2056, loss: 0.000012\n","epoch 32, step 2057, loss: 12.031978\n","epoch 32, step 2058, loss: 0.000000\n","epoch 32, step 2059, loss: 0.000018\n","epoch 32, step 2060, loss: 0.000001\n","epoch 32, step 2061, loss: 0.003661\n","epoch 32, step 2062, loss: 0.000000\n","epoch 32, step 2063, loss: 0.000003\n","epoch 32, step 2064, loss: 0.013251\n","epoch 32, step 2065, loss: 0.000000\n","epoch 32, step 2066, loss: 0.000001\n","epoch 32, step 2067, loss: 1.807128\n","epoch 32, step 2068, loss: 0.000000\n","epoch 32, step 2069, loss: 0.000000\n","epoch 32, step 2070, loss: 0.000000\n","epoch 32, step 2071, loss: 0.000369\n","epoch 32, step 2072, loss: 0.000020\n","epoch 32, step 2073, loss: 19.288385\n","epoch 32, step 2074, loss: 32.119579\n","epoch 32, step 2075, loss: 0.000000\n","epoch 32, step 2076, loss: 0.000000\n","epoch 32, step 2077, loss: 0.000000\n","epoch 32, step 2078, loss: 53.598186\n","epoch 33, step 2079, loss: 0.000000\n","epoch 33, step 2080, loss: 0.000014\n","epoch 33, step 2081, loss: 0.000053\n","epoch 33, step 2082, loss: 34.940891\n","epoch 33, step 2083, loss: 0.000000\n","epoch 33, step 2084, loss: 0.000002\n","epoch 33, step 2085, loss: 0.000000\n","epoch 33, step 2086, loss: 0.000034\n","epoch 33, step 2087, loss: 0.000000\n","epoch 33, step 2088, loss: 0.000013\n","epoch 33, step 2089, loss: 23.620579\n","epoch 33, step 2090, loss: 44.055351\n","epoch 33, step 2091, loss: 0.000005\n","epoch 33, step 2092, loss: 0.000004\n","epoch 33, step 2093, loss: 0.000144\n","epoch 33, step 2094, loss: 0.017504\n","epoch 33, step 2095, loss: 0.000000\n","epoch 33, step 2096, loss: 0.000000\n","epoch 33, step 2097, loss: 0.001735\n","epoch 33, step 2098, loss: 24.880198\n","epoch 33, step 2099, loss: 0.000000\n","epoch 33, step 2100, loss: 0.000220\n","epoch 33, step 2101, loss: 0.000004\n","epoch 33, step 2102, loss: 0.000000\n","epoch 33, step 2103, loss: 0.000174\n","epoch 33, step 2104, loss: 90.872963\n","epoch 33, step 2105, loss: 0.000052\n","epoch 33, step 2106, loss: 0.025245\n","epoch 33, step 2107, loss: 0.011647\n","epoch 33, step 2108, loss: 0.000000\n","epoch 33, step 2109, loss: 0.000000\n","epoch 33, step 2110, loss: 0.000000\n","epoch 33, step 2111, loss: 8.753184\n","epoch 33, step 2112, loss: 0.000427\n","epoch 33, step 2113, loss: 0.000001\n","epoch 33, step 2114, loss: 0.000000\n","epoch 33, step 2115, loss: 0.003066\n","epoch 33, step 2116, loss: 0.001406\n","epoch 33, step 2117, loss: 0.000004\n","epoch 33, step 2118, loss: 0.000271\n","epoch 33, step 2119, loss: 0.000000\n","epoch 33, step 2120, loss: 12.544911\n","epoch 33, step 2121, loss: 0.000000\n","epoch 33, step 2122, loss: 0.000029\n","epoch 33, step 2123, loss: 0.000000\n","epoch 33, step 2124, loss: 0.001004\n","epoch 33, step 2125, loss: 0.000000\n","epoch 33, step 2126, loss: 0.000000\n","epoch 33, step 2127, loss: 0.000333\n","epoch 33, step 2128, loss: 0.000000\n","epoch 33, step 2129, loss: 0.000003\n","epoch 33, step 2130, loss: 0.624643\n","epoch 33, step 2131, loss: 0.000000\n","epoch 33, step 2132, loss: 0.000000\n","epoch 33, step 2133, loss: 0.000000\n","epoch 33, step 2134, loss: 0.000000\n","epoch 33, step 2135, loss: 0.000000\n","epoch 33, step 2136, loss: 7.758537\n","epoch 33, step 2137, loss: 35.845341\n","epoch 33, step 2138, loss: 0.000021\n","epoch 33, step 2139, loss: 0.000000\n","epoch 33, step 2140, loss: 0.000009\n","epoch 33, step 2141, loss: 63.323048\n","epoch 34, step 2142, loss: 126.237946\n","epoch 34, step 2143, loss: 0.000000\n","epoch 34, step 2144, loss: 0.000000\n","epoch 34, step 2145, loss: 78.695038\n","epoch 34, step 2146, loss: 0.000000\n","epoch 34, step 2147, loss: 20.109875\n","epoch 34, step 2148, loss: 0.000000\n","epoch 34, step 2149, loss: 0.000000\n","epoch 34, step 2150, loss: 0.000000\n","epoch 34, step 2151, loss: 0.000018\n","epoch 34, step 2152, loss: 0.000001\n","epoch 34, step 2153, loss: 31.503866\n","epoch 34, step 2154, loss: 0.000000\n","epoch 34, step 2155, loss: 0.000000\n","epoch 34, step 2156, loss: 2.599808\n","epoch 34, step 2157, loss: 1.879139\n","epoch 34, step 2158, loss: 0.000000\n","epoch 34, step 2159, loss: 0.000000\n","epoch 34, step 2160, loss: 0.001299\n","epoch 34, step 2161, loss: 57.678745\n","epoch 34, step 2162, loss: 0.000000\n","epoch 34, step 2163, loss: 0.000061\n","epoch 34, step 2164, loss: 0.000000\n","epoch 34, step 2165, loss: 0.000000\n","epoch 34, step 2166, loss: 0.000000\n","epoch 34, step 2167, loss: 54.371876\n","epoch 34, step 2168, loss: 0.000000\n","epoch 34, step 2169, loss: 0.000239\n","epoch 34, step 2170, loss: 0.000105\n","epoch 34, step 2171, loss: 0.000000\n","epoch 34, step 2172, loss: 0.000000\n","epoch 34, step 2173, loss: 0.000000\n","epoch 34, step 2174, loss: 0.000000\n","epoch 34, step 2175, loss: 0.000018\n","epoch 34, step 2176, loss: 0.000000\n","epoch 34, step 2177, loss: 0.000000\n","epoch 34, step 2178, loss: 0.000018\n","epoch 34, step 2179, loss: 0.000000\n","epoch 34, step 2180, loss: 0.000011\n","epoch 34, step 2181, loss: 0.000001\n","epoch 34, step 2182, loss: 0.000819\n","epoch 34, step 2183, loss: 14.550791\n","epoch 34, step 2184, loss: 0.000000\n","epoch 34, step 2185, loss: 0.000032\n","epoch 34, step 2186, loss: 0.000000\n","epoch 34, step 2187, loss: 0.000104\n","epoch 34, step 2188, loss: 0.000000\n","epoch 34, step 2189, loss: 0.000000\n","epoch 34, step 2190, loss: 0.000000\n","epoch 34, step 2191, loss: 0.000001\n","epoch 34, step 2192, loss: 0.000000\n","epoch 34, step 2193, loss: 0.677619\n","epoch 34, step 2194, loss: 0.000072\n","epoch 34, step 2195, loss: 0.000000\n","epoch 34, step 2196, loss: 0.000000\n","epoch 34, step 2197, loss: 0.000000\n","epoch 34, step 2198, loss: 0.000053\n","epoch 34, step 2199, loss: 3.034341\n","epoch 34, step 2200, loss: 43.633305\n","epoch 34, step 2201, loss: 0.000000\n","epoch 34, step 2202, loss: 0.000000\n","epoch 34, step 2203, loss: 0.000000\n","epoch 34, step 2204, loss: 0.067651\n","epoch 35, step 2205, loss: 0.000000\n","epoch 35, step 2206, loss: 1.587565\n","epoch 35, step 2207, loss: 0.000000\n","epoch 35, step 2208, loss: 280.845947\n","epoch 35, step 2209, loss: 0.000000\n","epoch 35, step 2210, loss: 0.000000\n","epoch 35, step 2211, loss: 0.000000\n","epoch 35, step 2212, loss: 0.000000\n","epoch 35, step 2213, loss: 0.000000\n","epoch 35, step 2214, loss: 0.000015\n","epoch 35, step 2215, loss: 26.951176\n","epoch 35, step 2216, loss: 33.501095\n","epoch 35, step 2217, loss: 0.000003\n","epoch 35, step 2218, loss: 0.000000\n","epoch 35, step 2219, loss: 0.000000\n","epoch 35, step 2220, loss: 0.197121\n","epoch 35, step 2221, loss: 0.000003\n","epoch 35, step 2222, loss: 0.000000\n","epoch 35, step 2223, loss: 0.000014\n","epoch 35, step 2224, loss: 56.280117\n","epoch 35, step 2225, loss: 0.000000\n","epoch 35, step 2226, loss: 0.000249\n","epoch 35, step 2227, loss: 0.000000\n","epoch 35, step 2228, loss: 0.000000\n","epoch 35, step 2229, loss: 0.000000\n","epoch 35, step 2230, loss: 60.899097\n","epoch 35, step 2231, loss: 0.000000\n","epoch 35, step 2232, loss: 0.012503\n","epoch 35, step 2233, loss: 0.007171\n","epoch 35, step 2234, loss: 0.000000\n","epoch 35, step 2235, loss: 0.000000\n","epoch 35, step 2236, loss: 0.000000\n","epoch 35, step 2237, loss: 0.000000\n","epoch 35, step 2238, loss: 0.000000\n","epoch 35, step 2239, loss: 0.000001\n","epoch 35, step 2240, loss: 0.000001\n","epoch 35, step 2241, loss: 0.000009\n","epoch 35, step 2242, loss: 0.000000\n","epoch 35, step 2243, loss: 0.000000\n","epoch 35, step 2244, loss: 0.000203\n","epoch 35, step 2245, loss: 0.000658\n","epoch 35, step 2246, loss: 8.817141\n","epoch 35, step 2247, loss: 0.000000\n","epoch 35, step 2248, loss: 0.000025\n","epoch 35, step 2249, loss: 0.000000\n","epoch 35, step 2250, loss: 0.000323\n","epoch 35, step 2251, loss: 0.000000\n","epoch 35, step 2252, loss: 0.000000\n","epoch 35, step 2253, loss: 0.000000\n","epoch 35, step 2254, loss: 0.000039\n","epoch 35, step 2255, loss: 0.000000\n","epoch 35, step 2256, loss: 6.435677\n","epoch 35, step 2257, loss: 0.000003\n","epoch 35, step 2258, loss: 0.000000\n","epoch 35, step 2259, loss: 0.000000\n","epoch 35, step 2260, loss: 0.000000\n","epoch 35, step 2261, loss: 0.000007\n","epoch 35, step 2262, loss: 6.219724\n","epoch 35, step 2263, loss: 44.232300\n","epoch 35, step 2264, loss: 0.000000\n","epoch 35, step 2265, loss: 0.000000\n","epoch 35, step 2266, loss: 0.000000\n","epoch 35, step 2267, loss: 69.825157\n","epoch 36, step 2268, loss: 0.000000\n","epoch 36, step 2269, loss: 0.000000\n","epoch 36, step 2270, loss: 0.000000\n","epoch 36, step 2271, loss: 65.106209\n","epoch 36, step 2272, loss: 0.000000\n","epoch 36, step 2273, loss: 0.000000\n","epoch 36, step 2274, loss: 0.000000\n","epoch 36, step 2275, loss: 0.000000\n","epoch 36, step 2276, loss: 0.000000\n","epoch 36, step 2277, loss: 0.000020\n","epoch 36, step 2278, loss: 21.293835\n","epoch 36, step 2279, loss: 14.479259\n","epoch 36, step 2280, loss: 0.000008\n","epoch 36, step 2281, loss: 0.000001\n","epoch 36, step 2282, loss: 0.000001\n","epoch 36, step 2283, loss: 0.113649\n","epoch 36, step 2284, loss: 0.000062\n","epoch 36, step 2285, loss: 0.000000\n","epoch 36, step 2286, loss: 0.020144\n","epoch 36, step 2287, loss: 30.272867\n","epoch 36, step 2288, loss: 0.000000\n","epoch 36, step 2289, loss: 0.060773\n","epoch 36, step 2290, loss: 0.000009\n","epoch 36, step 2291, loss: 0.000034\n","epoch 36, step 2292, loss: 0.000018\n","epoch 36, step 2293, loss: 101.454178\n","epoch 36, step 2294, loss: 0.000886\n","epoch 36, step 2295, loss: 0.098185\n","epoch 36, step 2296, loss: 0.023957\n","epoch 36, step 2297, loss: 0.000582\n","epoch 36, step 2298, loss: 0.000025\n","epoch 36, step 2299, loss: 0.000000\n","epoch 36, step 2300, loss: 0.002136\n","epoch 36, step 2301, loss: 0.003343\n","epoch 36, step 2302, loss: 0.000017\n","epoch 36, step 2303, loss: 1.186786\n","epoch 36, step 2304, loss: 0.054332\n","epoch 36, step 2305, loss: 0.016044\n","epoch 36, step 2306, loss: 0.010491\n","epoch 36, step 2307, loss: 4.030203\n","epoch 36, step 2308, loss: 0.000978\n","epoch 36, step 2309, loss: 9.976933\n","epoch 36, step 2310, loss: 0.000000\n","epoch 36, step 2311, loss: 0.001256\n","epoch 36, step 2312, loss: 0.000002\n","epoch 36, step 2313, loss: 1.225140\n","epoch 36, step 2314, loss: 0.002835\n","epoch 36, step 2315, loss: 0.000000\n","epoch 36, step 2316, loss: 0.067360\n","epoch 36, step 2317, loss: 0.000000\n","epoch 36, step 2318, loss: 0.000178\n","epoch 36, step 2319, loss: 2.810713\n","epoch 36, step 2320, loss: 0.000000\n","epoch 36, step 2321, loss: 0.000000\n","epoch 36, step 2322, loss: 0.000000\n","epoch 36, step 2323, loss: 0.000000\n","epoch 36, step 2324, loss: 0.000005\n","epoch 36, step 2325, loss: 15.446420\n","epoch 36, step 2326, loss: 21.869764\n","epoch 36, step 2327, loss: 0.000000\n","epoch 36, step 2328, loss: 0.000000\n","epoch 36, step 2329, loss: 0.000000\n","epoch 36, step 2330, loss: 51.786991\n","epoch 37, step 2331, loss: 137.006699\n","epoch 37, step 2332, loss: 0.000000\n","epoch 37, step 2333, loss: 0.000001\n","epoch 37, step 2334, loss: 71.746620\n","epoch 37, step 2335, loss: 0.000000\n","epoch 37, step 2336, loss: 0.000000\n","epoch 37, step 2337, loss: 0.000000\n","epoch 37, step 2338, loss: 0.000000\n","epoch 37, step 2339, loss: 0.000000\n","epoch 37, step 2340, loss: 0.000002\n","epoch 37, step 2341, loss: 0.005582\n","epoch 37, step 2342, loss: 27.164665\n","epoch 37, step 2343, loss: 0.000000\n","epoch 37, step 2344, loss: 0.000000\n","epoch 37, step 2345, loss: 0.170196\n","epoch 37, step 2346, loss: 0.000000\n","epoch 37, step 2347, loss: 12.085187\n","epoch 37, step 2348, loss: 0.000000\n","epoch 37, step 2349, loss: 0.379513\n","epoch 37, step 2350, loss: 70.902016\n","epoch 37, step 2351, loss: 0.000000\n","epoch 37, step 2352, loss: 0.000000\n","epoch 37, step 2353, loss: 0.000000\n","epoch 37, step 2354, loss: 0.000000\n","epoch 37, step 2355, loss: 0.000000\n","epoch 37, step 2356, loss: 52.572388\n","epoch 37, step 2357, loss: 0.000000\n","epoch 37, step 2358, loss: 0.001016\n","epoch 37, step 2359, loss: 0.000292\n","epoch 37, step 2360, loss: 0.000000\n","epoch 37, step 2361, loss: 0.000000\n","epoch 37, step 2362, loss: 0.000000\n","epoch 37, step 2363, loss: 0.000000\n","epoch 37, step 2364, loss: 0.000002\n","epoch 37, step 2365, loss: 0.000000\n","epoch 37, step 2366, loss: 0.000000\n","epoch 37, step 2367, loss: 0.003200\n","epoch 37, step 2368, loss: 1.223663\n","epoch 37, step 2369, loss: 0.000043\n","epoch 37, step 2370, loss: 0.000141\n","epoch 37, step 2371, loss: 0.000587\n","epoch 37, step 2372, loss: 8.700031\n","epoch 37, step 2373, loss: 0.000001\n","epoch 37, step 2374, loss: 0.000005\n","epoch 37, step 2375, loss: 0.000000\n","epoch 37, step 2376, loss: 0.000002\n","epoch 37, step 2377, loss: 0.000000\n","epoch 37, step 2378, loss: 0.000000\n","epoch 37, step 2379, loss: 0.000000\n","epoch 37, step 2380, loss: 0.000001\n","epoch 37, step 2381, loss: 0.000000\n","epoch 37, step 2382, loss: 8.522242\n","epoch 37, step 2383, loss: 0.000204\n","epoch 37, step 2384, loss: 0.000000\n","epoch 37, step 2385, loss: 0.000000\n","epoch 37, step 2386, loss: 0.000000\n","epoch 37, step 2387, loss: 0.000080\n","epoch 37, step 2388, loss: 5.829950\n","epoch 37, step 2389, loss: 35.704235\n","epoch 37, step 2390, loss: 0.000000\n","epoch 37, step 2391, loss: 0.000000\n","epoch 37, step 2392, loss: 0.000000\n","epoch 37, step 2393, loss: 0.015771\n","epoch 38, step 2394, loss: 0.000017\n","epoch 38, step 2395, loss: 0.000013\n","epoch 38, step 2396, loss: 0.000000\n","epoch 38, step 2397, loss: 312.804169\n","epoch 38, step 2398, loss: 0.000000\n","epoch 38, step 2399, loss: 0.000113\n","epoch 38, step 2400, loss: 0.000000\n","epoch 38, step 2401, loss: 0.000000\n","epoch 38, step 2402, loss: 0.000000\n","epoch 38, step 2403, loss: 0.000012\n","epoch 38, step 2404, loss: 36.098736\n","epoch 38, step 2405, loss: 26.830603\n","epoch 38, step 2406, loss: 0.000000\n","epoch 38, step 2407, loss: 0.000000\n","epoch 38, step 2408, loss: 0.000000\n","epoch 38, step 2409, loss: 0.112211\n","epoch 38, step 2410, loss: 0.000128\n","epoch 38, step 2411, loss: 0.000000\n","epoch 38, step 2412, loss: 0.000191\n","epoch 38, step 2413, loss: 45.054356\n","epoch 38, step 2414, loss: 0.000000\n","epoch 38, step 2415, loss: 0.000526\n","epoch 38, step 2416, loss: 0.000008\n","epoch 38, step 2417, loss: 0.000021\n","epoch 38, step 2418, loss: 0.000000\n","epoch 38, step 2419, loss: 90.735329\n","epoch 38, step 2420, loss: 0.000002\n","epoch 38, step 2421, loss: 0.030625\n","epoch 38, step 2422, loss: 0.016947\n","epoch 38, step 2423, loss: 0.000000\n","epoch 38, step 2424, loss: 0.000000\n","epoch 38, step 2425, loss: 0.000000\n","epoch 38, step 2426, loss: 0.000000\n","epoch 38, step 2427, loss: 0.000000\n","epoch 38, step 2428, loss: 0.000009\n","epoch 38, step 2429, loss: 0.000000\n","epoch 38, step 2430, loss: 0.000000\n","epoch 38, step 2431, loss: 0.000005\n","epoch 38, step 2432, loss: 0.427931\n","epoch 38, step 2433, loss: 0.000264\n","epoch 38, step 2434, loss: 0.000887\n","epoch 38, step 2435, loss: 8.979873\n","epoch 38, step 2436, loss: 0.000000\n","epoch 38, step 2437, loss: 0.000060\n","epoch 38, step 2438, loss: 0.000001\n","epoch 38, step 2439, loss: 0.000048\n","epoch 38, step 2440, loss: 0.000000\n","epoch 38, step 2441, loss: 0.000000\n","epoch 38, step 2442, loss: 0.000004\n","epoch 38, step 2443, loss: 0.000001\n","epoch 38, step 2444, loss: 0.000003\n","epoch 38, step 2445, loss: 6.011707\n","epoch 38, step 2446, loss: 0.000000\n","epoch 38, step 2447, loss: 0.000000\n","epoch 38, step 2448, loss: 0.000000\n","epoch 38, step 2449, loss: 0.000072\n","epoch 38, step 2450, loss: 0.000024\n","epoch 38, step 2451, loss: 14.869758\n","epoch 38, step 2452, loss: 19.077112\n","epoch 38, step 2453, loss: 0.000000\n","epoch 38, step 2454, loss: 0.000000\n","epoch 38, step 2455, loss: 0.000003\n","epoch 38, step 2456, loss: 55.012920\n","epoch 39, step 2457, loss: 0.000000\n","epoch 39, step 2458, loss: 0.000009\n","epoch 39, step 2459, loss: 0.000007\n","epoch 39, step 2460, loss: 114.493759\n","epoch 39, step 2461, loss: 0.000000\n","epoch 39, step 2462, loss: 0.000000\n","epoch 39, step 2463, loss: 0.000000\n","epoch 39, step 2464, loss: 0.000089\n","epoch 39, step 2465, loss: 0.000000\n","epoch 39, step 2466, loss: 0.015034\n","epoch 39, step 2467, loss: 29.992268\n","epoch 39, step 2468, loss: 43.618568\n","epoch 39, step 2469, loss: 0.000762\n","epoch 39, step 2470, loss: 0.000001\n","epoch 39, step 2471, loss: 0.001557\n","epoch 39, step 2472, loss: 0.003578\n","epoch 39, step 2473, loss: 0.000002\n","epoch 39, step 2474, loss: 0.000000\n","epoch 39, step 2475, loss: 0.007678\n","epoch 39, step 2476, loss: 34.282051\n","epoch 39, step 2477, loss: 0.000000\n","epoch 39, step 2478, loss: 0.004467\n","epoch 39, step 2479, loss: 0.000008\n","epoch 39, step 2480, loss: 0.000212\n","epoch 39, step 2481, loss: 0.000000\n","epoch 39, step 2482, loss: 112.683678\n","epoch 39, step 2483, loss: 0.000006\n","epoch 39, step 2484, loss: 0.021443\n","epoch 39, step 2485, loss: 0.002003\n","epoch 39, step 2486, loss: 0.000016\n","epoch 39, step 2487, loss: 0.000005\n","epoch 39, step 2488, loss: 0.000000\n","epoch 39, step 2489, loss: 0.000029\n","epoch 39, step 2490, loss: 0.000006\n","epoch 39, step 2491, loss: 0.000029\n","epoch 39, step 2492, loss: 0.000000\n","epoch 39, step 2493, loss: 0.000031\n","epoch 39, step 2494, loss: 0.000003\n","epoch 39, step 2495, loss: 0.000202\n","epoch 39, step 2496, loss: 0.000100\n","epoch 39, step 2497, loss: 0.053737\n","epoch 39, step 2498, loss: 12.052212\n","epoch 39, step 2499, loss: 0.000027\n","epoch 39, step 2500, loss: 0.000134\n","epoch 39, step 2501, loss: 0.000161\n","epoch 39, step 2502, loss: 0.000040\n","epoch 39, step 2503, loss: 0.000000\n","epoch 39, step 2504, loss: 0.000065\n","epoch 39, step 2505, loss: 0.000242\n","epoch 39, step 2506, loss: 0.000000\n","epoch 39, step 2507, loss: 0.000398\n","epoch 39, step 2508, loss: 3.298197\n","epoch 39, step 2509, loss: 0.000000\n","epoch 39, step 2510, loss: 0.000000\n","epoch 39, step 2511, loss: 0.000001\n","epoch 39, step 2512, loss: 0.002445\n","epoch 39, step 2513, loss: 0.000000\n","epoch 39, step 2514, loss: 17.154123\n","epoch 39, step 2515, loss: 11.490180\n","epoch 39, step 2516, loss: 0.000000\n","epoch 39, step 2517, loss: 0.000000\n","epoch 39, step 2518, loss: 0.000000\n","epoch 39, step 2519, loss: 46.267601\n","epoch 40, step 2520, loss: 60.196106\n","epoch 40, step 2521, loss: 0.000000\n","epoch 40, step 2522, loss: 0.000008\n","epoch 40, step 2523, loss: 11.923594\n","epoch 40, step 2524, loss: 0.000000\n","epoch 40, step 2525, loss: 0.000000\n","epoch 40, step 2526, loss: 0.000000\n","epoch 40, step 2527, loss: 0.000000\n","epoch 40, step 2528, loss: 0.000000\n","epoch 40, step 2529, loss: 0.000000\n","epoch 40, step 2530, loss: 2.600326\n","epoch 40, step 2531, loss: 30.808167\n","epoch 40, step 2532, loss: 0.000000\n","epoch 40, step 2533, loss: 0.000000\n","epoch 40, step 2534, loss: 0.000007\n","epoch 40, step 2535, loss: 0.000001\n","epoch 40, step 2536, loss: 0.000000\n","epoch 40, step 2537, loss: 0.000000\n","epoch 40, step 2538, loss: 1.529420\n","epoch 40, step 2539, loss: 67.599869\n","epoch 40, step 2540, loss: 0.000031\n","epoch 40, step 2541, loss: 0.000001\n","epoch 40, step 2542, loss: 0.000000\n","epoch 40, step 2543, loss: 0.000003\n","epoch 40, step 2544, loss: 0.000000\n","epoch 40, step 2545, loss: 48.493382\n","epoch 40, step 2546, loss: 0.000043\n","epoch 40, step 2547, loss: 0.000021\n","epoch 40, step 2548, loss: 0.000001\n","epoch 40, step 2549, loss: 0.000000\n","epoch 40, step 2550, loss: 0.000000\n","epoch 40, step 2551, loss: 0.000000\n","epoch 40, step 2552, loss: 0.000000\n","epoch 40, step 2553, loss: 0.767407\n","epoch 40, step 2554, loss: 0.000000\n","epoch 40, step 2555, loss: 0.000000\n","epoch 40, step 2556, loss: 0.049171\n","epoch 40, step 2557, loss: 0.001209\n","epoch 40, step 2558, loss: 0.000000\n","epoch 40, step 2559, loss: 0.000000\n","epoch 40, step 2560, loss: 0.002292\n","epoch 40, step 2561, loss: 14.808208\n","epoch 40, step 2562, loss: 0.019038\n","epoch 40, step 2563, loss: 0.000028\n","epoch 40, step 2564, loss: 0.000000\n","epoch 40, step 2565, loss: 0.000001\n","epoch 40, step 2566, loss: 0.000000\n","epoch 40, step 2567, loss: 0.000000\n","epoch 40, step 2568, loss: 0.000000\n","epoch 40, step 2569, loss: 0.000000\n","epoch 40, step 2570, loss: 0.000000\n","epoch 40, step 2571, loss: 3.131335\n","epoch 40, step 2572, loss: 0.064194\n","epoch 40, step 2573, loss: 0.000000\n","epoch 40, step 2574, loss: 0.000000\n","epoch 40, step 2575, loss: 0.000000\n","epoch 40, step 2576, loss: 0.000016\n","epoch 40, step 2577, loss: 0.070275\n","epoch 40, step 2578, loss: 52.385239\n","epoch 40, step 2579, loss: 0.000000\n","epoch 40, step 2580, loss: 0.000000\n","epoch 40, step 2581, loss: 0.000002\n","epoch 40, step 2582, loss: 60.073727\n","epoch 41, step 2583, loss: 0.000024\n","epoch 41, step 2584, loss: 0.000013\n","epoch 41, step 2585, loss: 0.000000\n","epoch 41, step 2586, loss: 304.269135\n","epoch 41, step 2587, loss: 0.000000\n","epoch 41, step 2588, loss: 0.000021\n","epoch 41, step 2589, loss: 0.000000\n","epoch 41, step 2590, loss: 0.000000\n","epoch 41, step 2591, loss: 0.000000\n","epoch 41, step 2592, loss: 28.311310\n","epoch 41, step 2593, loss: 30.692272\n","epoch 41, step 2594, loss: 36.285603\n","epoch 41, step 2595, loss: 0.000025\n","epoch 41, step 2596, loss: 0.000000\n","epoch 41, step 2597, loss: 0.000001\n","epoch 41, step 2598, loss: 0.013623\n","epoch 41, step 2599, loss: 0.000002\n","epoch 41, step 2600, loss: 0.000000\n","epoch 41, step 2601, loss: 0.000001\n","epoch 41, step 2602, loss: 32.782700\n","epoch 41, step 2603, loss: 0.000000\n","epoch 41, step 2604, loss: 0.000420\n","epoch 41, step 2605, loss: 0.000010\n","epoch 41, step 2606, loss: 0.000002\n","epoch 41, step 2607, loss: 0.000000\n","epoch 41, step 2608, loss: 84.111000\n","epoch 41, step 2609, loss: 0.000001\n","epoch 41, step 2610, loss: 0.004274\n","epoch 41, step 2611, loss: 0.002457\n","epoch 41, step 2612, loss: 0.000000\n","epoch 41, step 2613, loss: 0.000000\n","epoch 41, step 2614, loss: 0.000000\n","epoch 41, step 2615, loss: 0.000000\n","epoch 41, step 2616, loss: 0.000000\n","epoch 41, step 2617, loss: 0.000633\n","epoch 41, step 2618, loss: 0.000000\n","epoch 41, step 2619, loss: 0.000004\n","epoch 41, step 2620, loss: 0.000001\n","epoch 41, step 2621, loss: 0.000000\n","epoch 41, step 2622, loss: 0.000080\n","epoch 41, step 2623, loss: 0.010885\n","epoch 41, step 2624, loss: 9.744054\n","epoch 41, step 2625, loss: 0.000000\n","epoch 41, step 2626, loss: 0.000100\n","epoch 41, step 2627, loss: 0.000065\n","epoch 41, step 2628, loss: 0.000012\n","epoch 41, step 2629, loss: 0.000000\n","epoch 41, step 2630, loss: 0.000000\n","epoch 41, step 2631, loss: 0.000000\n","epoch 41, step 2632, loss: 0.000000\n","epoch 41, step 2633, loss: 0.000000\n","epoch 41, step 2634, loss: 7.530443\n","epoch 41, step 2635, loss: 0.000001\n","epoch 41, step 2636, loss: 0.000000\n","epoch 41, step 2637, loss: 0.000000\n","epoch 41, step 2638, loss: 0.000004\n","epoch 41, step 2639, loss: 0.000003\n","epoch 41, step 2640, loss: 1.223947\n","epoch 41, step 2641, loss: 32.436756\n","epoch 41, step 2642, loss: 0.000000\n","epoch 41, step 2643, loss: 0.000000\n","epoch 41, step 2644, loss: 0.000017\n","epoch 41, step 2645, loss: 48.532810\n","epoch 42, step 2646, loss: 0.000040\n","epoch 42, step 2647, loss: 0.000014\n","epoch 42, step 2648, loss: 0.000017\n","epoch 42, step 2649, loss: 135.168533\n","epoch 42, step 2650, loss: 0.000000\n","epoch 42, step 2651, loss: 0.000004\n","epoch 42, step 2652, loss: 0.000000\n","epoch 42, step 2653, loss: 0.000231\n","epoch 42, step 2654, loss: 0.000000\n","epoch 42, step 2655, loss: 0.000295\n","epoch 42, step 2656, loss: 40.040565\n","epoch 42, step 2657, loss: 23.266739\n","epoch 42, step 2658, loss: 0.000369\n","epoch 42, step 2659, loss: 0.000001\n","epoch 42, step 2660, loss: 0.001834\n","epoch 42, step 2661, loss: 0.148608\n","epoch 42, step 2662, loss: 0.000114\n","epoch 42, step 2663, loss: 0.000240\n","epoch 42, step 2664, loss: 5.033523\n","epoch 42, step 2665, loss: 18.201452\n","epoch 42, step 2666, loss: 0.000000\n","epoch 42, step 2667, loss: 0.049240\n","epoch 42, step 2668, loss: 0.000129\n","epoch 42, step 2669, loss: 0.000007\n","epoch 42, step 2670, loss: 0.000029\n","epoch 42, step 2671, loss: 110.216042\n","epoch 42, step 2672, loss: 0.000135\n","epoch 42, step 2673, loss: 0.008462\n","epoch 42, step 2674, loss: 0.000831\n","epoch 42, step 2675, loss: 0.000000\n","epoch 42, step 2676, loss: 0.000000\n","epoch 42, step 2677, loss: 0.000000\n","epoch 42, step 2678, loss: 0.000010\n","epoch 42, step 2679, loss: 0.000156\n","epoch 42, step 2680, loss: 0.000005\n","epoch 42, step 2681, loss: 0.000000\n","epoch 42, step 2682, loss: 0.000166\n","epoch 42, step 2683, loss: 0.000241\n","epoch 42, step 2684, loss: 0.000000\n","epoch 42, step 2685, loss: 0.000010\n","epoch 42, step 2686, loss: 0.040196\n","epoch 42, step 2687, loss: 12.172736\n","epoch 42, step 2688, loss: 0.000136\n","epoch 42, step 2689, loss: 0.002111\n","epoch 42, step 2690, loss: 0.000065\n","epoch 42, step 2691, loss: 0.064757\n","epoch 42, step 2692, loss: 0.000000\n","epoch 42, step 2693, loss: 0.000021\n","epoch 42, step 2694, loss: 0.000048\n","epoch 42, step 2695, loss: 0.000012\n","epoch 42, step 2696, loss: 0.000062\n","epoch 42, step 2697, loss: 5.058951\n","epoch 42, step 2698, loss: 0.000000\n","epoch 42, step 2699, loss: 0.000000\n","epoch 42, step 2700, loss: 0.000002\n","epoch 42, step 2701, loss: 0.001800\n","epoch 42, step 2702, loss: 0.000004\n","epoch 42, step 2703, loss: 38.172428\n","epoch 42, step 2704, loss: 14.385056\n","epoch 42, step 2705, loss: 0.000000\n","epoch 42, step 2706, loss: 0.000000\n","epoch 42, step 2707, loss: 0.000474\n","epoch 42, step 2708, loss: 38.103050\n","epoch 43, step 2709, loss: 15.234113\n","epoch 43, step 2710, loss: 0.000001\n","epoch 43, step 2711, loss: 0.000167\n","epoch 43, step 2712, loss: 26.867382\n","epoch 43, step 2713, loss: 0.000037\n","epoch 43, step 2714, loss: 0.000000\n","epoch 43, step 2715, loss: 0.000000\n","epoch 43, step 2716, loss: 0.000025\n","epoch 43, step 2717, loss: 0.000000\n","epoch 43, step 2718, loss: 0.000221\n","epoch 43, step 2719, loss: 35.454010\n","epoch 43, step 2720, loss: 35.219154\n","epoch 43, step 2721, loss: 0.000008\n","epoch 43, step 2722, loss: 0.000000\n","epoch 43, step 2723, loss: 0.000004\n","epoch 43, step 2724, loss: 0.000089\n","epoch 43, step 2725, loss: 0.000000\n","epoch 43, step 2726, loss: 0.000000\n","epoch 43, step 2727, loss: 0.000001\n","epoch 43, step 2728, loss: 27.954218\n","epoch 43, step 2729, loss: 0.000000\n","epoch 43, step 2730, loss: 0.000011\n","epoch 43, step 2731, loss: 0.000000\n","epoch 43, step 2732, loss: 0.000000\n","epoch 43, step 2733, loss: 0.000000\n","epoch 43, step 2734, loss: 93.519241\n","epoch 43, step 2735, loss: 0.000000\n","epoch 43, step 2736, loss: 0.000486\n","epoch 43, step 2737, loss: 0.000035\n","epoch 43, step 2738, loss: 0.000000\n","epoch 43, step 2739, loss: 0.000000\n","epoch 43, step 2740, loss: 0.000000\n","epoch 43, step 2741, loss: 0.000000\n","epoch 43, step 2742, loss: 0.000001\n","epoch 43, step 2743, loss: 0.000001\n","epoch 43, step 2744, loss: 0.000000\n","epoch 43, step 2745, loss: 0.000016\n","epoch 43, step 2746, loss: 0.000000\n","epoch 43, step 2747, loss: 0.000000\n","epoch 43, step 2748, loss: 0.000001\n","epoch 43, step 2749, loss: 0.000006\n","epoch 43, step 2750, loss: 13.781315\n","epoch 43, step 2751, loss: 0.000000\n","epoch 43, step 2752, loss: 0.000120\n","epoch 43, step 2753, loss: 0.000002\n","epoch 43, step 2754, loss: 0.000002\n","epoch 43, step 2755, loss: 0.000000\n","epoch 43, step 2756, loss: 0.000000\n","epoch 43, step 2757, loss: 0.000000\n","epoch 43, step 2758, loss: 0.000000\n","epoch 43, step 2759, loss: 0.000001\n","epoch 43, step 2760, loss: 4.376279\n","epoch 43, step 2761, loss: 0.000000\n","epoch 43, step 2762, loss: 0.000000\n","epoch 43, step 2763, loss: 0.000000\n","epoch 43, step 2764, loss: 0.000171\n","epoch 43, step 2765, loss: 0.000000\n","epoch 43, step 2766, loss: 30.277132\n","epoch 43, step 2767, loss: 25.226122\n","epoch 43, step 2768, loss: 0.000000\n","epoch 43, step 2769, loss: 0.000000\n","epoch 43, step 2770, loss: 0.000007\n","epoch 43, step 2771, loss: 44.043751\n","epoch 44, step 2772, loss: 7.088662\n","epoch 44, step 2773, loss: 0.000000\n","epoch 44, step 2774, loss: 0.000016\n","epoch 44, step 2775, loss: 35.035580\n","epoch 44, step 2776, loss: 0.000001\n","epoch 44, step 2777, loss: 0.000000\n","epoch 44, step 2778, loss: 0.000000\n","epoch 44, step 2779, loss: 0.000002\n","epoch 44, step 2780, loss: 0.000000\n","epoch 44, step 2781, loss: 0.000001\n","epoch 44, step 2782, loss: 30.372570\n","epoch 44, step 2783, loss: 30.735886\n","epoch 44, step 2784, loss: 0.000000\n","epoch 44, step 2785, loss: 0.000000\n","epoch 44, step 2786, loss: 0.000002\n","epoch 44, step 2787, loss: 0.000100\n","epoch 44, step 2788, loss: 0.000000\n","epoch 44, step 2789, loss: 0.000000\n","epoch 44, step 2790, loss: 0.000001\n","epoch 44, step 2791, loss: 27.273695\n","epoch 44, step 2792, loss: 0.000000\n","epoch 44, step 2793, loss: 0.000023\n","epoch 44, step 2794, loss: 0.000000\n","epoch 44, step 2795, loss: 0.000000\n","epoch 44, step 2796, loss: 0.000000\n","epoch 44, step 2797, loss: 99.825897\n","epoch 44, step 2798, loss: 0.000001\n","epoch 44, step 2799, loss: 0.000730\n","epoch 44, step 2800, loss: 0.000087\n","epoch 44, step 2801, loss: 0.000000\n","epoch 44, step 2802, loss: 0.000000\n","epoch 44, step 2803, loss: 0.000000\n","epoch 44, step 2804, loss: 0.000000\n","epoch 44, step 2805, loss: 0.000028\n","epoch 44, step 2806, loss: 0.000001\n","epoch 44, step 2807, loss: 0.000000\n","epoch 44, step 2808, loss: 0.000033\n","epoch 44, step 2809, loss: 0.000042\n","epoch 44, step 2810, loss: 0.000000\n","epoch 44, step 2811, loss: 0.000004\n","epoch 44, step 2812, loss: 0.000004\n","epoch 44, step 2813, loss: 12.713725\n","epoch 44, step 2814, loss: 0.000038\n","epoch 44, step 2815, loss: 0.001493\n","epoch 44, step 2816, loss: 0.000032\n","epoch 44, step 2817, loss: 0.000548\n","epoch 44, step 2818, loss: 0.000000\n","epoch 44, step 2819, loss: 0.000007\n","epoch 44, step 2820, loss: 0.000056\n","epoch 44, step 2821, loss: 0.000003\n","epoch 44, step 2822, loss: 0.000044\n","epoch 44, step 2823, loss: 5.851344\n","epoch 44, step 2824, loss: 0.000000\n","epoch 44, step 2825, loss: 0.000000\n","epoch 44, step 2826, loss: 0.000002\n","epoch 44, step 2827, loss: 0.001684\n","epoch 44, step 2828, loss: 0.000004\n","epoch 44, step 2829, loss: 37.776245\n","epoch 44, step 2830, loss: 16.146460\n","epoch 44, step 2831, loss: 0.000000\n","epoch 44, step 2832, loss: 0.000000\n","epoch 44, step 2833, loss: 0.000340\n","epoch 44, step 2834, loss: 37.418190\n","epoch 45, step 2835, loss: 14.906718\n","epoch 45, step 2836, loss: 0.000001\n","epoch 45, step 2837, loss: 0.000174\n","epoch 45, step 2838, loss: 27.466299\n","epoch 45, step 2839, loss: 0.000043\n","epoch 45, step 2840, loss: 0.000000\n","epoch 45, step 2841, loss: 0.000000\n","epoch 45, step 2842, loss: 0.000040\n","epoch 45, step 2843, loss: 0.000000\n","epoch 45, step 2844, loss: 0.000516\n","epoch 45, step 2845, loss: 37.570419\n","epoch 45, step 2846, loss: 36.003094\n","epoch 45, step 2847, loss: 0.000008\n","epoch 45, step 2848, loss: 0.000000\n","epoch 45, step 2849, loss: 0.000005\n","epoch 45, step 2850, loss: 0.000141\n","epoch 45, step 2851, loss: 0.000000\n","epoch 45, step 2852, loss: 0.000000\n","epoch 45, step 2853, loss: 0.000001\n","epoch 45, step 2854, loss: 28.990141\n","epoch 45, step 2855, loss: 0.000000\n","epoch 45, step 2856, loss: 0.000013\n","epoch 45, step 2857, loss: 0.000000\n","epoch 45, step 2858, loss: 0.000000\n","epoch 45, step 2859, loss: 0.000000\n","epoch 45, step 2860, loss: 94.879158\n","epoch 45, step 2861, loss: 0.000000\n","epoch 45, step 2862, loss: 0.000474\n","epoch 45, step 2863, loss: 0.000053\n","epoch 45, step 2864, loss: 0.000000\n","epoch 45, step 2865, loss: 0.000000\n","epoch 45, step 2866, loss: 0.000000\n","epoch 45, step 2867, loss: 0.000000\n","epoch 45, step 2868, loss: 0.000001\n","epoch 45, step 2869, loss: 0.000001\n","epoch 45, step 2870, loss: 0.000000\n","epoch 45, step 2871, loss: 0.000017\n","epoch 45, step 2872, loss: 0.000000\n","epoch 45, step 2873, loss: 0.000000\n","epoch 45, step 2874, loss: 0.000002\n","epoch 45, step 2875, loss: 0.000004\n","epoch 45, step 2876, loss: 13.476001\n","epoch 45, step 2877, loss: 0.000000\n","epoch 45, step 2878, loss: 0.000111\n","epoch 45, step 2879, loss: 0.000002\n","epoch 45, step 2880, loss: 0.000002\n","epoch 45, step 2881, loss: 0.000000\n","epoch 45, step 2882, loss: 0.000000\n","epoch 45, step 2883, loss: 0.000000\n","epoch 45, step 2884, loss: 0.000000\n","epoch 45, step 2885, loss: 0.000001\n","epoch 45, step 2886, loss: 5.391613\n","epoch 45, step 2887, loss: 0.000000\n","epoch 45, step 2888, loss: 0.000000\n","epoch 45, step 2889, loss: 0.000000\n","epoch 45, step 2890, loss: 0.000204\n","epoch 45, step 2891, loss: 0.000000\n","epoch 45, step 2892, loss: 29.902092\n","epoch 45, step 2893, loss: 26.842722\n","epoch 45, step 2894, loss: 0.000000\n","epoch 45, step 2895, loss: 0.000000\n","epoch 45, step 2896, loss: 0.000005\n","epoch 45, step 2897, loss: 43.372482\n","epoch 46, step 2898, loss: 6.623173\n","epoch 46, step 2899, loss: 0.000000\n","epoch 46, step 2900, loss: 0.000016\n","epoch 46, step 2901, loss: 35.874355\n","epoch 46, step 2902, loss: 0.000003\n","epoch 46, step 2903, loss: 0.000000\n","epoch 46, step 2904, loss: 0.000000\n","epoch 46, step 2905, loss: 0.000002\n","epoch 46, step 2906, loss: 0.000000\n","epoch 46, step 2907, loss: 0.000001\n","epoch 46, step 2908, loss: 32.271240\n","epoch 46, step 2909, loss: 31.823051\n","epoch 46, step 2910, loss: 0.000000\n","epoch 46, step 2911, loss: 0.000000\n","epoch 46, step 2912, loss: 0.000002\n","epoch 46, step 2913, loss: 0.000127\n","epoch 46, step 2914, loss: 0.000000\n","epoch 46, step 2915, loss: 0.000000\n","epoch 46, step 2916, loss: 0.000001\n","epoch 46, step 2917, loss: 28.320478\n","epoch 46, step 2918, loss: 0.000000\n","epoch 46, step 2919, loss: 0.000028\n","epoch 46, step 2920, loss: 0.000000\n","epoch 46, step 2921, loss: 0.000000\n","epoch 46, step 2922, loss: 0.000000\n","epoch 46, step 2923, loss: 101.237770\n","epoch 46, step 2924, loss: 0.000001\n","epoch 46, step 2925, loss: 0.000703\n","epoch 46, step 2926, loss: 0.000110\n","epoch 46, step 2927, loss: 0.000000\n","epoch 46, step 2928, loss: 0.000000\n","epoch 46, step 2929, loss: 0.000000\n","epoch 46, step 2930, loss: 0.000000\n","epoch 46, step 2931, loss: 0.000038\n","epoch 46, step 2932, loss: 0.000000\n","epoch 46, step 2933, loss: 0.000000\n","epoch 46, step 2934, loss: 0.000034\n","epoch 46, step 2935, loss: 0.000078\n","epoch 46, step 2936, loss: 0.000000\n","epoch 46, step 2937, loss: 0.000005\n","epoch 46, step 2938, loss: 0.000003\n","epoch 46, step 2939, loss: 12.607524\n","epoch 46, step 2940, loss: 0.000040\n","epoch 46, step 2941, loss: 0.001524\n","epoch 46, step 2942, loss: 0.000039\n","epoch 46, step 2943, loss: 0.000661\n","epoch 46, step 2944, loss: 0.000000\n","epoch 46, step 2945, loss: 0.000006\n","epoch 46, step 2946, loss: 0.000111\n","epoch 46, step 2947, loss: 0.000002\n","epoch 46, step 2948, loss: 0.000051\n","epoch 46, step 2949, loss: 6.672928\n","epoch 46, step 2950, loss: 0.000000\n","epoch 46, step 2951, loss: 0.000000\n","epoch 46, step 2952, loss: 0.000003\n","epoch 46, step 2953, loss: 0.002054\n","epoch 46, step 2954, loss: 0.000004\n","epoch 46, step 2955, loss: 37.589062\n","epoch 46, step 2956, loss: 17.492352\n","epoch 46, step 2957, loss: 0.000000\n","epoch 46, step 2958, loss: 0.000000\n","epoch 46, step 2959, loss: 0.000290\n","epoch 46, step 2960, loss: 36.579903\n","epoch 47, step 2961, loss: 14.740375\n","epoch 47, step 2962, loss: 0.000001\n","epoch 47, step 2963, loss: 0.000192\n","epoch 47, step 2964, loss: 28.065796\n","epoch 47, step 2965, loss: 0.000056\n","epoch 47, step 2966, loss: 0.000000\n","epoch 47, step 2967, loss: 0.000000\n","epoch 47, step 2968, loss: 0.000063\n","epoch 47, step 2969, loss: 0.000000\n","epoch 47, step 2970, loss: 0.001270\n","epoch 47, step 2971, loss: 39.651703\n","epoch 47, step 2972, loss: 36.713116\n","epoch 47, step 2973, loss: 0.000007\n","epoch 47, step 2974, loss: 0.000000\n","epoch 47, step 2975, loss: 0.000005\n","epoch 47, step 2976, loss: 0.000170\n","epoch 47, step 2977, loss: 0.000000\n","epoch 47, step 2978, loss: 0.000000\n","epoch 47, step 2979, loss: 0.000001\n","epoch 47, step 2980, loss: 30.092789\n","epoch 47, step 2981, loss: 0.000000\n","epoch 47, step 2982, loss: 0.000015\n","epoch 47, step 2983, loss: 0.000000\n","epoch 47, step 2984, loss: 0.000000\n","epoch 47, step 2985, loss: 0.000000\n","epoch 47, step 2986, loss: 96.025581\n","epoch 47, step 2987, loss: 0.000000\n","epoch 47, step 2988, loss: 0.000437\n","epoch 47, step 2989, loss: 0.000062\n","epoch 47, step 2990, loss: 0.000000\n","epoch 47, step 2991, loss: 0.000000\n","epoch 47, step 2992, loss: 0.000000\n","epoch 47, step 2993, loss: 0.000000\n","epoch 47, step 2994, loss: 0.000002\n","epoch 47, step 2995, loss: 0.000001\n","epoch 47, step 2996, loss: 0.000000\n","epoch 47, step 2997, loss: 0.000017\n","epoch 47, step 2998, loss: 0.000000\n","epoch 47, step 2999, loss: 0.000000\n","epoch 47, step 3000, loss: 0.000002\n","epoch 47, step 3001, loss: 0.000003\n","epoch 47, step 3002, loss: 13.457419\n","epoch 47, step 3003, loss: 0.000000\n","epoch 47, step 3004, loss: 0.000099\n","epoch 47, step 3005, loss: 0.000003\n","epoch 47, step 3006, loss: 0.000002\n","epoch 47, step 3007, loss: 0.000000\n","epoch 47, step 3008, loss: 0.000000\n","epoch 47, step 3009, loss: 0.000000\n","epoch 47, step 3010, loss: 0.000000\n","epoch 47, step 3011, loss: 0.000001\n","epoch 47, step 3012, loss: 6.120198\n","epoch 47, step 3013, loss: 0.000000\n","epoch 47, step 3014, loss: 0.000000\n","epoch 47, step 3015, loss: 0.000000\n","epoch 47, step 3016, loss: 0.000216\n","epoch 47, step 3017, loss: 0.000000\n","epoch 47, step 3018, loss: 29.428514\n","epoch 47, step 3019, loss: 28.515047\n","epoch 47, step 3020, loss: 0.000000\n","epoch 47, step 3021, loss: 0.000000\n","epoch 47, step 3022, loss: 0.000004\n","epoch 47, step 3023, loss: 42.750336\n","epoch 48, step 3024, loss: 6.186683\n","epoch 48, step 3025, loss: 0.000000\n","epoch 48, step 3026, loss: 0.000016\n","epoch 48, step 3027, loss: 36.742290\n","epoch 48, step 3028, loss: 0.000008\n","epoch 48, step 3029, loss: 0.000000\n","epoch 48, step 3030, loss: 0.000000\n","epoch 48, step 3031, loss: 0.000003\n","epoch 48, step 3032, loss: 0.000000\n","epoch 48, step 3033, loss: 0.000002\n","epoch 48, step 3034, loss: 34.123547\n","epoch 48, step 3035, loss: 32.941204\n","epoch 48, step 3036, loss: 0.000000\n","epoch 48, step 3037, loss: 0.000000\n","epoch 48, step 3038, loss: 0.000002\n","epoch 48, step 3039, loss: 0.000142\n","epoch 48, step 3040, loss: 0.000000\n","epoch 48, step 3041, loss: 0.000000\n","epoch 48, step 3042, loss: 0.000001\n","epoch 48, step 3043, loss: 29.380772\n","epoch 48, step 3044, loss: 0.000000\n","epoch 48, step 3045, loss: 0.000033\n","epoch 48, step 3046, loss: 0.000000\n","epoch 48, step 3047, loss: 0.000000\n","epoch 48, step 3048, loss: 0.000000\n","epoch 48, step 3049, loss: 102.522514\n","epoch 48, step 3050, loss: 0.000001\n","epoch 48, step 3051, loss: 0.000667\n","epoch 48, step 3052, loss: 0.000123\n","epoch 48, step 3053, loss: 0.000000\n","epoch 48, step 3054, loss: 0.000000\n","epoch 48, step 3055, loss: 0.000000\n","epoch 48, step 3056, loss: 0.000001\n","epoch 48, step 3057, loss: 0.000053\n","epoch 48, step 3058, loss: 0.000000\n","epoch 48, step 3059, loss: 0.000000\n","epoch 48, step 3060, loss: 0.000035\n","epoch 48, step 3061, loss: 0.000146\n","epoch 48, step 3062, loss: 0.000000\n","epoch 48, step 3063, loss: 0.000006\n","epoch 48, step 3064, loss: 0.000002\n","epoch 48, step 3065, loss: 12.616302\n","epoch 48, step 3066, loss: 0.000041\n","epoch 48, step 3067, loss: 0.001532\n","epoch 48, step 3068, loss: 0.000049\n","epoch 48, step 3069, loss: 0.000781\n","epoch 48, step 3070, loss: 0.000000\n","epoch 48, step 3071, loss: 0.000005\n","epoch 48, step 3072, loss: 0.000224\n","epoch 48, step 3073, loss: 0.000002\n","epoch 48, step 3074, loss: 0.000059\n","epoch 48, step 3075, loss: 7.376207\n","epoch 48, step 3076, loss: 0.000000\n","epoch 48, step 3077, loss: 0.000000\n","epoch 48, step 3078, loss: 0.000004\n","epoch 48, step 3079, loss: 0.002411\n","epoch 48, step 3080, loss: 0.000004\n","epoch 48, step 3081, loss: 37.370308\n","epoch 48, step 3082, loss: 18.800907\n","epoch 48, step 3083, loss: 0.000000\n","epoch 48, step 3084, loss: 0.000000\n","epoch 48, step 3085, loss: 0.000251\n","epoch 48, step 3086, loss: 35.717861\n","epoch 49, step 3087, loss: 14.705219\n","epoch 49, step 3088, loss: 0.000001\n","epoch 49, step 3089, loss: 0.000212\n","epoch 49, step 3090, loss: 28.594635\n","epoch 49, step 3091, loss: 0.000072\n","epoch 49, step 3092, loss: 0.000001\n","epoch 49, step 3093, loss: 0.000000\n","epoch 49, step 3094, loss: 0.000100\n","epoch 49, step 3095, loss: 0.000000\n","epoch 49, step 3096, loss: 0.003079\n","epoch 49, step 3097, loss: 41.704170\n","epoch 49, step 3098, loss: 37.375481\n","epoch 49, step 3099, loss: 0.000006\n","epoch 49, step 3100, loss: 0.000000\n","epoch 49, step 3101, loss: 0.000005\n","epoch 49, step 3102, loss: 0.000187\n","epoch 49, step 3103, loss: 0.000000\n","epoch 49, step 3104, loss: 0.000000\n","epoch 49, step 3105, loss: 0.000001\n","epoch 49, step 3106, loss: 31.198782\n","epoch 49, step 3107, loss: 0.000000\n","epoch 49, step 3108, loss: 0.000017\n","epoch 49, step 3109, loss: 0.000000\n","epoch 49, step 3110, loss: 0.000000\n","epoch 49, step 3111, loss: 0.000000\n","epoch 49, step 3112, loss: 97.052223\n","epoch 49, step 3113, loss: 0.000000\n","epoch 49, step 3114, loss: 0.000393\n","epoch 49, step 3115, loss: 0.000065\n","epoch 49, step 3116, loss: 0.000000\n","epoch 49, step 3117, loss: 0.000000\n","epoch 49, step 3118, loss: 0.000000\n","epoch 49, step 3119, loss: 0.000000\n","epoch 49, step 3120, loss: 0.000002\n","epoch 49, step 3121, loss: 0.000001\n","epoch 49, step 3122, loss: 0.000000\n","epoch 49, step 3123, loss: 0.000018\n","epoch 49, step 3124, loss: 0.000000\n","epoch 49, step 3125, loss: 0.000000\n","epoch 49, step 3126, loss: 0.000002\n","epoch 49, step 3127, loss: 0.000002\n","epoch 49, step 3128, loss: 13.529301\n","epoch 49, step 3129, loss: 0.000000\n","epoch 49, step 3130, loss: 0.000086\n","epoch 49, step 3131, loss: 0.000003\n","epoch 49, step 3132, loss: 0.000002\n","epoch 49, step 3133, loss: 0.000000\n","epoch 49, step 3134, loss: 0.000000\n","epoch 49, step 3135, loss: 0.000000\n","epoch 49, step 3136, loss: 0.000000\n","epoch 49, step 3137, loss: 0.000001\n","epoch 49, step 3138, loss: 6.754128\n","epoch 49, step 3139, loss: 0.000000\n","epoch 49, step 3140, loss: 0.000000\n","epoch 49, step 3141, loss: 0.000000\n","epoch 49, step 3142, loss: 0.000219\n","epoch 49, step 3143, loss: 0.000000\n","epoch 49, step 3144, loss: 28.926754\n","epoch 49, step 3145, loss: 30.140354\n","epoch 49, step 3146, loss: 0.000000\n","epoch 49, step 3147, loss: 0.000000\n","epoch 49, step 3148, loss: 0.000002\n","epoch 49, step 3149, loss: 42.103848\n","epoch 50, step 3150, loss: 5.865664\n","epoch 50, step 3151, loss: 0.000000\n","epoch 50, step 3152, loss: 0.000016\n","epoch 50, step 3153, loss: 37.531898\n","epoch 50, step 3154, loss: 0.000024\n","epoch 50, step 3155, loss: 0.000000\n","epoch 50, step 3156, loss: 0.000000\n","epoch 50, step 3157, loss: 0.000005\n","epoch 50, step 3158, loss: 0.000000\n","epoch 50, step 3159, loss: 0.000003\n","epoch 50, step 3160, loss: 35.947357\n","epoch 50, step 3161, loss: 34.071301\n","epoch 50, step 3162, loss: 0.000000\n","epoch 50, step 3163, loss: 0.000000\n","epoch 50, step 3164, loss: 0.000002\n","epoch 50, step 3165, loss: 0.000150\n","epoch 50, step 3166, loss: 0.000000\n","epoch 50, step 3167, loss: 0.000000\n","epoch 50, step 3168, loss: 0.000001\n","epoch 50, step 3169, loss: 30.438662\n","epoch 50, step 3170, loss: 0.000000\n","epoch 50, step 3171, loss: 0.000038\n","epoch 50, step 3172, loss: 0.000000\n","epoch 50, step 3173, loss: 0.000000\n","epoch 50, step 3174, loss: 0.000000\n","epoch 50, step 3175, loss: 103.691345\n","epoch 50, step 3176, loss: 0.000002\n","epoch 50, step 3177, loss: 0.000645\n","epoch 50, step 3178, loss: 0.000130\n","epoch 50, step 3179, loss: 0.000000\n","epoch 50, step 3180, loss: 0.000000\n","epoch 50, step 3181, loss: 0.000000\n","epoch 50, step 3182, loss: 0.000001\n","epoch 50, step 3183, loss: 0.000073\n","epoch 50, step 3184, loss: 0.000000\n","epoch 50, step 3185, loss: 0.000000\n","epoch 50, step 3186, loss: 0.000036\n","epoch 50, step 3187, loss: 0.000268\n","epoch 50, step 3188, loss: 0.000000\n","epoch 50, step 3189, loss: 0.000006\n","epoch 50, step 3190, loss: 0.000002\n","epoch 50, step 3191, loss: 12.676452\n","epoch 50, step 3192, loss: 0.000040\n","epoch 50, step 3193, loss: 0.001518\n","epoch 50, step 3194, loss: 0.000062\n","epoch 50, step 3195, loss: 0.000902\n","epoch 50, step 3196, loss: 0.000000\n","epoch 50, step 3197, loss: 0.000005\n","epoch 50, step 3198, loss: 0.000461\n","epoch 50, step 3199, loss: 0.000001\n","epoch 50, step 3200, loss: 0.000068\n","epoch 50, step 3201, loss: 8.021820\n","epoch 50, step 3202, loss: 0.000000\n","epoch 50, step 3203, loss: 0.000000\n","epoch 50, step 3204, loss: 0.000005\n","epoch 50, step 3205, loss: 0.002725\n","epoch 50, step 3206, loss: 0.000004\n","epoch 50, step 3207, loss: 37.119427\n","epoch 50, step 3208, loss: 20.063128\n","epoch 50, step 3209, loss: 0.000000\n","epoch 50, step 3210, loss: 0.000000\n","epoch 50, step 3211, loss: 0.000226\n","epoch 50, step 3212, loss: 34.833416\n","epoch 51, step 3213, loss: 14.783049\n","epoch 51, step 3214, loss: 0.000001\n","epoch 51, step 3215, loss: 0.000233\n","epoch 51, step 3216, loss: 29.037180\n","epoch 51, step 3217, loss: 0.000092\n","epoch 51, step 3218, loss: 0.000002\n","epoch 51, step 3219, loss: 0.000000\n","epoch 51, step 3220, loss: 0.000161\n","epoch 51, step 3221, loss: 0.000000\n","epoch 51, step 3222, loss: 0.006735\n","epoch 51, step 3223, loss: 43.716305\n","epoch 51, step 3224, loss: 37.979820\n","epoch 51, step 3225, loss: 0.000005\n","epoch 51, step 3226, loss: 0.000000\n","epoch 51, step 3227, loss: 0.000005\n","epoch 51, step 3228, loss: 0.000196\n","epoch 51, step 3229, loss: 0.000000\n","epoch 51, step 3230, loss: 0.000000\n","epoch 51, step 3231, loss: 0.000001\n","epoch 51, step 3232, loss: 32.308105\n","epoch 51, step 3233, loss: 0.000000\n","epoch 51, step 3234, loss: 0.000019\n","epoch 51, step 3235, loss: 0.000000\n","epoch 51, step 3236, loss: 0.000000\n","epoch 51, step 3237, loss: 0.000000\n","epoch 51, step 3238, loss: 97.950378\n","epoch 51, step 3239, loss: 0.000000\n","epoch 51, step 3240, loss: 0.000347\n","epoch 51, step 3241, loss: 0.000065\n","epoch 51, step 3242, loss: 0.000000\n","epoch 51, step 3243, loss: 0.000000\n","epoch 51, step 3244, loss: 0.000000\n","epoch 51, step 3245, loss: 0.000000\n","epoch 51, step 3246, loss: 0.000002\n","epoch 51, step 3247, loss: 0.000001\n","epoch 51, step 3248, loss: 0.000000\n","epoch 51, step 3249, loss: 0.000018\n","epoch 51, step 3250, loss: 0.000001\n","epoch 51, step 3251, loss: 0.000000\n","epoch 51, step 3252, loss: 0.000002\n","epoch 51, step 3253, loss: 0.000002\n","epoch 51, step 3254, loss: 13.651047\n","epoch 51, step 3255, loss: 0.000000\n","epoch 51, step 3256, loss: 0.000073\n","epoch 51, step 3257, loss: 0.000003\n","epoch 51, step 3258, loss: 0.000002\n","epoch 51, step 3259, loss: 0.000000\n","epoch 51, step 3260, loss: 0.000000\n","epoch 51, step 3261, loss: 0.000001\n","epoch 51, step 3262, loss: 0.000000\n","epoch 51, step 3263, loss: 0.000001\n","epoch 51, step 3264, loss: 7.331171\n","epoch 51, step 3265, loss: 0.000000\n","epoch 51, step 3266, loss: 0.000000\n","epoch 51, step 3267, loss: 0.000000\n","epoch 51, step 3268, loss: 0.000212\n","epoch 51, step 3269, loss: 0.000000\n","epoch 51, step 3270, loss: 28.389147\n","epoch 51, step 3271, loss: 31.725109\n","epoch 51, step 3272, loss: 0.000000\n","epoch 51, step 3273, loss: 0.000000\n","epoch 51, step 3274, loss: 0.000001\n","epoch 51, step 3275, loss: 41.445000\n","epoch 52, step 3276, loss: 5.630061\n","epoch 52, step 3277, loss: 0.000000\n","epoch 52, step 3278, loss: 0.000015\n","epoch 52, step 3279, loss: 38.251762\n","epoch 52, step 3280, loss: 0.000074\n","epoch 52, step 3281, loss: 0.000000\n","epoch 52, step 3282, loss: 0.000000\n","epoch 52, step 3283, loss: 0.000007\n","epoch 52, step 3284, loss: 0.000000\n","epoch 52, step 3285, loss: 0.000003\n","epoch 52, step 3286, loss: 37.721931\n","epoch 52, step 3287, loss: 35.225899\n","epoch 52, step 3288, loss: 0.000000\n","epoch 52, step 3289, loss: 0.000000\n","epoch 52, step 3290, loss: 0.000002\n","epoch 52, step 3291, loss: 0.000151\n","epoch 52, step 3292, loss: 0.000000\n","epoch 52, step 3293, loss: 0.000000\n","epoch 52, step 3294, loss: 0.000002\n","epoch 52, step 3295, loss: 31.500244\n","epoch 52, step 3296, loss: 0.000000\n","epoch 52, step 3297, loss: 0.000045\n","epoch 52, step 3298, loss: 0.000001\n","epoch 52, step 3299, loss: 0.000000\n","epoch 52, step 3300, loss: 0.000000\n","epoch 52, step 3301, loss: 104.725838\n","epoch 52, step 3302, loss: 0.000003\n","epoch 52, step 3303, loss: 0.000663\n","epoch 52, step 3304, loss: 0.000132\n","epoch 52, step 3305, loss: 0.000000\n","epoch 52, step 3306, loss: 0.000000\n","epoch 52, step 3307, loss: 0.000000\n","epoch 52, step 3308, loss: 0.000001\n","epoch 52, step 3309, loss: 0.000100\n","epoch 52, step 3310, loss: 0.000000\n","epoch 52, step 3311, loss: 0.000000\n","epoch 52, step 3312, loss: 0.000037\n","epoch 52, step 3313, loss: 0.000466\n","epoch 52, step 3314, loss: 0.000000\n","epoch 52, step 3315, loss: 0.000007\n","epoch 52, step 3316, loss: 0.000001\n","epoch 52, step 3317, loss: 12.771391\n","epoch 52, step 3318, loss: 0.000038\n","epoch 52, step 3319, loss: 0.001472\n","epoch 52, step 3320, loss: 0.000077\n","epoch 52, step 3321, loss: 0.001009\n","epoch 52, step 3322, loss: 0.000000\n","epoch 52, step 3323, loss: 0.000005\n","epoch 52, step 3324, loss: 0.000934\n","epoch 52, step 3325, loss: 0.000001\n","epoch 52, step 3326, loss: 0.000079\n","epoch 52, step 3327, loss: 8.624730\n","epoch 52, step 3328, loss: 0.000000\n","epoch 52, step 3329, loss: 0.000000\n","epoch 52, step 3330, loss: 0.000008\n","epoch 52, step 3331, loss: 0.002931\n","epoch 52, step 3332, loss: 0.000004\n","epoch 52, step 3333, loss: 36.822235\n","epoch 52, step 3334, loss: 21.296129\n","epoch 52, step 3335, loss: 0.000000\n","epoch 52, step 3336, loss: 0.000000\n","epoch 52, step 3337, loss: 0.000220\n","epoch 52, step 3338, loss: 33.944504\n","epoch 53, step 3339, loss: 14.926598\n","epoch 53, step 3340, loss: 0.000001\n","epoch 53, step 3341, loss: 0.000255\n","epoch 53, step 3342, loss: 29.420940\n","epoch 53, step 3343, loss: 0.000114\n","epoch 53, step 3344, loss: 0.000004\n","epoch 53, step 3345, loss: 0.000000\n","epoch 53, step 3346, loss: 0.000256\n","epoch 53, step 3347, loss: 0.000000\n","epoch 53, step 3348, loss: 0.011293\n","epoch 53, step 3349, loss: 45.665009\n","epoch 53, step 3350, loss: 38.507061\n","epoch 53, step 3351, loss: 0.000004\n","epoch 53, step 3352, loss: 0.000000\n","epoch 53, step 3353, loss: 0.000005\n","epoch 53, step 3354, loss: 0.000196\n","epoch 53, step 3355, loss: 0.000000\n","epoch 53, step 3356, loss: 0.000000\n","epoch 53, step 3357, loss: 0.000001\n","epoch 53, step 3358, loss: 33.426605\n","epoch 53, step 3359, loss: 0.000000\n","epoch 53, step 3360, loss: 0.000021\n","epoch 53, step 3361, loss: 0.000000\n","epoch 53, step 3362, loss: 0.000000\n","epoch 53, step 3363, loss: 0.000000\n","epoch 53, step 3364, loss: 98.707207\n","epoch 53, step 3365, loss: 0.000000\n","epoch 53, step 3366, loss: 0.000301\n","epoch 53, step 3367, loss: 0.000063\n","epoch 53, step 3368, loss: 0.000000\n","epoch 53, step 3369, loss: 0.000000\n","epoch 53, step 3370, loss: 0.000000\n","epoch 53, step 3371, loss: 0.000000\n","epoch 53, step 3372, loss: 0.000003\n","epoch 53, step 3373, loss: 0.000001\n","epoch 53, step 3374, loss: 0.000000\n","epoch 53, step 3375, loss: 0.000018\n","epoch 53, step 3376, loss: 0.000001\n","epoch 53, step 3377, loss: 0.000000\n","epoch 53, step 3378, loss: 0.000001\n","epoch 53, step 3379, loss: 0.000001\n","epoch 53, step 3380, loss: 13.807947\n","epoch 53, step 3381, loss: 0.000000\n","epoch 53, step 3382, loss: 0.000061\n","epoch 53, step 3383, loss: 0.000003\n","epoch 53, step 3384, loss: 0.000001\n","epoch 53, step 3385, loss: 0.000000\n","epoch 53, step 3386, loss: 0.000000\n","epoch 53, step 3387, loss: 0.000001\n","epoch 53, step 3388, loss: 0.000000\n","epoch 53, step 3389, loss: 0.000001\n","epoch 53, step 3390, loss: 7.863925\n","epoch 53, step 3391, loss: 0.000000\n","epoch 53, step 3392, loss: 0.000000\n","epoch 53, step 3393, loss: 0.000000\n","epoch 53, step 3394, loss: 0.000195\n","epoch 53, step 3395, loss: 0.000000\n","epoch 53, step 3396, loss: 27.806499\n","epoch 53, step 3397, loss: 33.277477\n","epoch 53, step 3398, loss: 0.000000\n","epoch 53, step 3399, loss: 0.000000\n","epoch 53, step 3400, loss: 0.000001\n","epoch 53, step 3401, loss: 40.776432\n","epoch 54, step 3402, loss: 5.448299\n","epoch 54, step 3403, loss: 0.000000\n","epoch 54, step 3404, loss: 0.000015\n","epoch 54, step 3405, loss: 38.916832\n","epoch 54, step 3406, loss: 0.000224\n","epoch 54, step 3407, loss: 0.000000\n","epoch 54, step 3408, loss: 0.000000\n","epoch 54, step 3409, loss: 0.000010\n","epoch 54, step 3410, loss: 0.000000\n","epoch 54, step 3411, loss: 0.000003\n","epoch 54, step 3412, loss: 39.446014\n","epoch 54, step 3413, loss: 36.412296\n","epoch 54, step 3414, loss: 0.000000\n","epoch 54, step 3415, loss: 0.000000\n","epoch 54, step 3416, loss: 0.000002\n","epoch 54, step 3417, loss: 0.000144\n","epoch 54, step 3418, loss: 0.000000\n","epoch 54, step 3419, loss: 0.000000\n","epoch 54, step 3420, loss: 0.000002\n","epoch 54, step 3421, loss: 32.586422\n","epoch 54, step 3422, loss: 0.000000\n","epoch 54, step 3423, loss: 0.000052\n","epoch 54, step 3424, loss: 0.000002\n","epoch 54, step 3425, loss: 0.000000\n","epoch 54, step 3426, loss: 0.000000\n","epoch 54, step 3427, loss: 105.610733\n","epoch 54, step 3428, loss: 0.000004\n","epoch 54, step 3429, loss: 0.000755\n","epoch 54, step 3430, loss: 0.000128\n","epoch 54, step 3431, loss: 0.000000\n","epoch 54, step 3432, loss: 0.000000\n","epoch 54, step 3433, loss: 0.000000\n","epoch 54, step 3434, loss: 0.000002\n","epoch 54, step 3435, loss: 0.000134\n","epoch 54, step 3436, loss: 0.000000\n","epoch 54, step 3437, loss: 0.000000\n","epoch 54, step 3438, loss: 0.000037\n","epoch 54, step 3439, loss: 0.000743\n","epoch 54, step 3440, loss: 0.000000\n","epoch 54, step 3441, loss: 0.000008\n","epoch 54, step 3442, loss: 0.000001\n","epoch 54, step 3443, loss: 12.911953\n","epoch 54, step 3444, loss: 0.000034\n","epoch 54, step 3445, loss: 0.001382\n","epoch 54, step 3446, loss: 0.000096\n","epoch 54, step 3447, loss: 0.001085\n","epoch 54, step 3448, loss: 0.000000\n","epoch 54, step 3449, loss: 0.000005\n","epoch 54, step 3450, loss: 0.001811\n","epoch 54, step 3451, loss: 0.000001\n","epoch 54, step 3452, loss: 0.000089\n","epoch 54, step 3453, loss: 9.171656\n","epoch 54, step 3454, loss: 0.000000\n","epoch 54, step 3455, loss: 0.000000\n","epoch 54, step 3456, loss: 0.000011\n","epoch 54, step 3457, loss: 0.003009\n","epoch 54, step 3458, loss: 0.000004\n","epoch 54, step 3459, loss: 36.450390\n","epoch 54, step 3460, loss: 22.513956\n","epoch 54, step 3461, loss: 0.000000\n","epoch 54, step 3462, loss: 0.000000\n","epoch 54, step 3463, loss: 0.000238\n","epoch 54, step 3464, loss: 33.048317\n","epoch 55, step 3465, loss: 15.100684\n","epoch 55, step 3466, loss: 0.000001\n","epoch 55, step 3467, loss: 0.000274\n","epoch 55, step 3468, loss: 29.767670\n","epoch 55, step 3469, loss: 0.000136\n","epoch 55, step 3470, loss: 0.000009\n","epoch 55, step 3471, loss: 0.000000\n","epoch 55, step 3472, loss: 0.000401\n","epoch 55, step 3473, loss: 0.000000\n","epoch 55, step 3474, loss: 0.013005\n","epoch 55, step 3475, loss: 47.551861\n","epoch 55, step 3476, loss: 38.949013\n","epoch 55, step 3477, loss: 0.000003\n","epoch 55, step 3478, loss: 0.000000\n","epoch 55, step 3479, loss: 0.000006\n","epoch 55, step 3480, loss: 0.000188\n","epoch 55, step 3481, loss: 0.000000\n","epoch 55, step 3482, loss: 0.000000\n","epoch 55, step 3483, loss: 0.000001\n","epoch 55, step 3484, loss: 34.561996\n","epoch 55, step 3485, loss: 0.000000\n","epoch 55, step 3486, loss: 0.000023\n","epoch 55, step 3487, loss: 0.000001\n","epoch 55, step 3488, loss: 0.000000\n","epoch 55, step 3489, loss: 0.000000\n","epoch 55, step 3490, loss: 99.342804\n","epoch 55, step 3491, loss: 0.000000\n","epoch 55, step 3492, loss: 0.000256\n","epoch 55, step 3493, loss: 0.000058\n","epoch 55, step 3494, loss: 0.000000\n","epoch 55, step 3495, loss: 0.000000\n","epoch 55, step 3496, loss: 0.000000\n","epoch 55, step 3497, loss: 0.000000\n","epoch 55, step 3498, loss: 0.000003\n","epoch 55, step 3499, loss: 0.000000\n","epoch 55, step 3500, loss: 0.000000\n","epoch 55, step 3501, loss: 0.000018\n","epoch 55, step 3502, loss: 0.000001\n","epoch 55, step 3503, loss: 0.000000\n","epoch 55, step 3504, loss: 0.000001\n","epoch 55, step 3505, loss: 0.000001\n","epoch 55, step 3506, loss: 13.997495\n","epoch 55, step 3507, loss: 0.000000\n","epoch 55, step 3508, loss: 0.000049\n","epoch 55, step 3509, loss: 0.000004\n","epoch 55, step 3510, loss: 0.000001\n","epoch 55, step 3511, loss: 0.000000\n","epoch 55, step 3512, loss: 0.000000\n","epoch 55, step 3513, loss: 0.000001\n","epoch 55, step 3514, loss: 0.000000\n","epoch 55, step 3515, loss: 0.000001\n","epoch 55, step 3516, loss: 8.353919\n","epoch 55, step 3517, loss: 0.000000\n","epoch 55, step 3518, loss: 0.000000\n","epoch 55, step 3519, loss: 0.000000\n","epoch 55, step 3520, loss: 0.000177\n","epoch 55, step 3521, loss: 0.000000\n","epoch 55, step 3522, loss: 27.175150\n","epoch 55, step 3523, loss: 34.763313\n","epoch 55, step 3524, loss: 0.000000\n","epoch 55, step 3525, loss: 0.000000\n","epoch 55, step 3526, loss: 0.000000\n","epoch 55, step 3527, loss: 40.074375\n","epoch 56, step 3528, loss: 5.351136\n","epoch 56, step 3529, loss: 0.000000\n","epoch 56, step 3530, loss: 0.000014\n","epoch 56, step 3531, loss: 39.499920\n","epoch 56, step 3532, loss: 0.000539\n","epoch 56, step 3533, loss: 0.000000\n","epoch 56, step 3534, loss: 0.000000\n","epoch 56, step 3535, loss: 0.000013\n","epoch 56, step 3536, loss: 0.000000\n","epoch 56, step 3537, loss: 0.000003\n","epoch 56, step 3538, loss: 41.121174\n","epoch 56, step 3539, loss: 37.618896\n","epoch 56, step 3540, loss: 0.000000\n","epoch 56, step 3541, loss: 0.000000\n","epoch 56, step 3542, loss: 0.000002\n","epoch 56, step 3543, loss: 0.000136\n","epoch 56, step 3544, loss: 0.000000\n","epoch 56, step 3545, loss: 0.000000\n","epoch 56, step 3546, loss: 0.000002\n","epoch 56, step 3547, loss: 33.668362\n","epoch 56, step 3548, loss: 0.000000\n","epoch 56, step 3549, loss: 0.000060\n","epoch 56, step 3550, loss: 0.000005\n","epoch 56, step 3551, loss: 0.000000\n","epoch 56, step 3552, loss: 0.000000\n","epoch 56, step 3553, loss: 106.846954\n","epoch 56, step 3554, loss: 0.000006\n","epoch 56, step 3555, loss: 0.001292\n","epoch 56, step 3556, loss: 0.000121\n","epoch 56, step 3557, loss: 0.000000\n","epoch 56, step 3558, loss: 0.000000\n","epoch 56, step 3559, loss: 0.000000\n","epoch 56, step 3560, loss: 0.000002\n","epoch 56, step 3561, loss: 0.000179\n","epoch 56, step 3562, loss: 0.000000\n","epoch 56, step 3563, loss: 0.000000\n","epoch 56, step 3564, loss: 0.000037\n","epoch 56, step 3565, loss: 0.001047\n","epoch 56, step 3566, loss: 0.000000\n","epoch 56, step 3567, loss: 0.000009\n","epoch 56, step 3568, loss: 0.000001\n","epoch 56, step 3569, loss: 13.065249\n","epoch 56, step 3570, loss: 0.000028\n","epoch 56, step 3571, loss: 0.001274\n","epoch 56, step 3572, loss: 0.000120\n","epoch 56, step 3573, loss: 0.001307\n","epoch 56, step 3574, loss: 0.000000\n","epoch 56, step 3575, loss: 0.000009\n","epoch 56, step 3576, loss: 0.003654\n","epoch 56, step 3577, loss: 0.000001\n","epoch 56, step 3578, loss: 0.000097\n","epoch 56, step 3579, loss: 9.693071\n","epoch 56, step 3580, loss: 0.000000\n","epoch 56, step 3581, loss: 0.000000\n","epoch 56, step 3582, loss: 0.000017\n","epoch 56, step 3583, loss: 0.002965\n","epoch 56, step 3584, loss: 0.000004\n","epoch 56, step 3585, loss: 36.020309\n","epoch 56, step 3586, loss: 23.097738\n","epoch 56, step 3587, loss: 0.000000\n","epoch 56, step 3588, loss: 0.000000\n","epoch 56, step 3589, loss: 0.000288\n","epoch 56, step 3590, loss: 32.159081\n","epoch 57, step 3591, loss: 15.263824\n","epoch 57, step 3592, loss: 0.000001\n","epoch 57, step 3593, loss: 0.000290\n","epoch 57, step 3594, loss: 30.108894\n","epoch 57, step 3595, loss: 0.000213\n","epoch 57, step 3596, loss: 0.000021\n","epoch 57, step 3597, loss: 0.000000\n","epoch 57, step 3598, loss: 0.000595\n","epoch 57, step 3599, loss: 0.000000\n","epoch 57, step 3600, loss: 0.016148\n","epoch 57, step 3601, loss: 49.350288\n","epoch 57, step 3602, loss: 39.301369\n","epoch 57, step 3603, loss: 0.000002\n","epoch 57, step 3604, loss: 0.000000\n","epoch 57, step 3605, loss: 0.000006\n","epoch 57, step 3606, loss: 0.000175\n","epoch 57, step 3607, loss: 0.000000\n","epoch 57, step 3608, loss: 0.000000\n","epoch 57, step 3609, loss: 0.000001\n","epoch 57, step 3610, loss: 35.720005\n","epoch 57, step 3611, loss: 0.000000\n","epoch 57, step 3612, loss: 0.000024\n","epoch 57, step 3613, loss: 0.000001\n","epoch 57, step 3614, loss: 0.000000\n","epoch 57, step 3615, loss: 0.000000\n","epoch 57, step 3616, loss: 100.433296\n","epoch 57, step 3617, loss: 0.000000\n","epoch 57, step 3618, loss: 0.000213\n","epoch 57, step 3619, loss: 0.000053\n","epoch 57, step 3620, loss: 0.000000\n","epoch 57, step 3621, loss: 0.000000\n","epoch 57, step 3622, loss: 0.000000\n","epoch 57, step 3623, loss: 0.000000\n","epoch 57, step 3624, loss: 0.000004\n","epoch 57, step 3625, loss: 0.000000\n","epoch 57, step 3626, loss: 0.000000\n","epoch 57, step 3627, loss: 0.000017\n","epoch 57, step 3628, loss: 0.000001\n","epoch 57, step 3629, loss: 0.000000\n","epoch 57, step 3630, loss: 0.000001\n","epoch 57, step 3631, loss: 0.000001\n","epoch 57, step 3632, loss: 14.212010\n","epoch 57, step 3633, loss: 0.000000\n","epoch 57, step 3634, loss: 0.000038\n","epoch 57, step 3635, loss: 0.000004\n","epoch 57, step 3636, loss: 0.000001\n","epoch 57, step 3637, loss: 0.000000\n","epoch 57, step 3638, loss: 0.000000\n","epoch 57, step 3639, loss: 0.000001\n","epoch 57, step 3640, loss: 0.000000\n","epoch 57, step 3641, loss: 0.000001\n","epoch 57, step 3642, loss: 8.805545\n","epoch 57, step 3643, loss: 0.000000\n","epoch 57, step 3644, loss: 0.000000\n","epoch 57, step 3645, loss: 0.000000\n","epoch 57, step 3646, loss: 0.000154\n","epoch 57, step 3647, loss: 0.000000\n","epoch 57, step 3648, loss: 26.477226\n","epoch 57, step 3649, loss: 35.632595\n","epoch 57, step 3650, loss: 0.000000\n","epoch 57, step 3651, loss: 0.000000\n","epoch 57, step 3652, loss: 0.000000\n","epoch 57, step 3653, loss: 39.390522\n","epoch 58, step 3654, loss: 5.219535\n","epoch 58, step 3655, loss: 0.000000\n","epoch 58, step 3656, loss: 0.000013\n","epoch 58, step 3657, loss: 40.083130\n","epoch 58, step 3658, loss: 0.000743\n","epoch 58, step 3659, loss: 0.000000\n","epoch 58, step 3660, loss: 0.000000\n","epoch 58, step 3661, loss: 0.000017\n","epoch 58, step 3662, loss: 0.000000\n","epoch 58, step 3663, loss: 0.000003\n","epoch 58, step 3664, loss: 42.705448\n","epoch 58, step 3665, loss: 38.847698\n","epoch 58, step 3666, loss: 0.000000\n","epoch 58, step 3667, loss: 0.000000\n","epoch 58, step 3668, loss: 0.000003\n","epoch 58, step 3669, loss: 0.000125\n","epoch 58, step 3670, loss: 0.000000\n","epoch 58, step 3671, loss: 0.000000\n","epoch 58, step 3672, loss: 0.000003\n","epoch 58, step 3673, loss: 34.773632\n","epoch 58, step 3674, loss: 0.000000\n","epoch 58, step 3675, loss: 0.000067\n","epoch 58, step 3676, loss: 0.000011\n","epoch 58, step 3677, loss: 0.000000\n","epoch 58, step 3678, loss: 0.000000\n","epoch 58, step 3679, loss: 107.576088\n","epoch 58, step 3680, loss: 0.000010\n","epoch 58, step 3681, loss: 0.001834\n","epoch 58, step 3682, loss: 0.000113\n","epoch 58, step 3683, loss: 0.000000\n","epoch 58, step 3684, loss: 0.000000\n","epoch 58, step 3685, loss: 0.000000\n","epoch 58, step 3686, loss: 0.000004\n","epoch 58, step 3687, loss: 0.000228\n","epoch 58, step 3688, loss: 0.000000\n","epoch 58, step 3689, loss: 0.000000\n","epoch 58, step 3690, loss: 0.000036\n","epoch 58, step 3691, loss: 0.001240\n","epoch 58, step 3692, loss: 0.000000\n","epoch 58, step 3693, loss: 0.000010\n","epoch 58, step 3694, loss: 0.000000\n","epoch 58, step 3695, loss: 13.240496\n","epoch 58, step 3696, loss: 0.000022\n","epoch 58, step 3697, loss: 0.001137\n","epoch 58, step 3698, loss: 0.000148\n","epoch 58, step 3699, loss: 0.001274\n","epoch 58, step 3700, loss: 0.000000\n","epoch 58, step 3701, loss: 0.000016\n","epoch 58, step 3702, loss: 0.005553\n","epoch 58, step 3703, loss: 0.000000\n","epoch 58, step 3704, loss: 0.000101\n","epoch 58, step 3705, loss: 10.179342\n","epoch 58, step 3706, loss: 0.000000\n","epoch 58, step 3707, loss: 0.000000\n","epoch 58, step 3708, loss: 0.000025\n","epoch 58, step 3709, loss: 0.002806\n","epoch 58, step 3710, loss: 0.000004\n","epoch 58, step 3711, loss: 35.517307\n","epoch 58, step 3712, loss: 24.278543\n","epoch 58, step 3713, loss: 0.000000\n","epoch 58, step 3714, loss: 0.000000\n","epoch 58, step 3715, loss: 0.000387\n","epoch 58, step 3716, loss: 31.295353\n","epoch 59, step 3717, loss: 15.436231\n","epoch 59, step 3718, loss: 0.000001\n","epoch 59, step 3719, loss: 0.000302\n","epoch 59, step 3720, loss: 30.420940\n","epoch 59, step 3721, loss: 0.000210\n","epoch 59, step 3722, loss: 0.000044\n","epoch 59, step 3723, loss: 0.000000\n","epoch 59, step 3724, loss: 0.000865\n","epoch 59, step 3725, loss: 0.000000\n","epoch 59, step 3726, loss: 0.010752\n","epoch 59, step 3727, loss: 51.057442\n","epoch 59, step 3728, loss: 39.567474\n","epoch 59, step 3729, loss: 0.000002\n","epoch 59, step 3730, loss: 0.000000\n","epoch 59, step 3731, loss: 0.000006\n","epoch 59, step 3732, loss: 0.000163\n","epoch 59, step 3733, loss: 0.000000\n","epoch 59, step 3734, loss: 0.000000\n","epoch 59, step 3735, loss: 0.000002\n","epoch 59, step 3736, loss: 36.854584\n","epoch 59, step 3737, loss: 0.000000\n","epoch 59, step 3738, loss: 0.000026\n","epoch 59, step 3739, loss: 0.000003\n","epoch 59, step 3740, loss: 0.000000\n","epoch 59, step 3741, loss: 0.000000\n","epoch 59, step 3742, loss: 100.841171\n","epoch 59, step 3743, loss: 0.000000\n","epoch 59, step 3744, loss: 0.000180\n","epoch 59, step 3745, loss: 0.000048\n","epoch 59, step 3746, loss: 0.000000\n","epoch 59, step 3747, loss: 0.000000\n","epoch 59, step 3748, loss: 0.000000\n","epoch 59, step 3749, loss: 0.000000\n","epoch 59, step 3750, loss: 0.000004\n","epoch 59, step 3751, loss: 0.000000\n","epoch 59, step 3752, loss: 0.000000\n","epoch 59, step 3753, loss: 0.000017\n","epoch 59, step 3754, loss: 0.000001\n","epoch 59, step 3755, loss: 0.000000\n","epoch 59, step 3756, loss: 0.000001\n","epoch 59, step 3757, loss: 0.000002\n","epoch 59, step 3758, loss: 14.406364\n","epoch 59, step 3759, loss: 0.000000\n","epoch 59, step 3760, loss: 0.000031\n","epoch 59, step 3761, loss: 0.000004\n","epoch 59, step 3762, loss: 0.000001\n","epoch 59, step 3763, loss: 0.000000\n","epoch 59, step 3764, loss: 0.000000\n","epoch 59, step 3765, loss: 0.000001\n","epoch 59, step 3766, loss: 0.000000\n","epoch 59, step 3767, loss: 0.000001\n","epoch 59, step 3768, loss: 9.264003\n","epoch 59, step 3769, loss: 0.000000\n","epoch 59, step 3770, loss: 0.000000\n","epoch 59, step 3771, loss: 0.000000\n","epoch 59, step 3772, loss: 0.000140\n","epoch 59, step 3773, loss: 0.000000\n","epoch 59, step 3774, loss: 25.781588\n","epoch 59, step 3775, loss: 36.957478\n","epoch 59, step 3776, loss: 0.000000\n","epoch 59, step 3777, loss: 0.000000\n","epoch 59, step 3778, loss: 0.000000\n","epoch 59, step 3779, loss: 38.633026\n","epoch 60, step 3780, loss: 5.267692\n","epoch 60, step 3781, loss: 0.000000\n","epoch 60, step 3782, loss: 0.000013\n","epoch 60, step 3783, loss: 40.510860\n","epoch 60, step 3784, loss: 0.000620\n","epoch 60, step 3785, loss: 0.000001\n","epoch 60, step 3786, loss: 0.000000\n","epoch 60, step 3787, loss: 0.000022\n","epoch 60, step 3788, loss: 0.000000\n","epoch 60, step 3789, loss: 0.000003\n","epoch 60, step 3790, loss: 44.268349\n","epoch 60, step 3791, loss: 40.047749\n","epoch 60, step 3792, loss: 0.000000\n","epoch 60, step 3793, loss: 0.000000\n","epoch 60, step 3794, loss: 0.000004\n","epoch 60, step 3795, loss: 0.000117\n","epoch 60, step 3796, loss: 0.000000\n","epoch 60, step 3797, loss: 0.000000\n","epoch 60, step 3798, loss: 0.000004\n","epoch 60, step 3799, loss: 35.856457\n","epoch 60, step 3800, loss: 0.000000\n","epoch 60, step 3801, loss: 0.000077\n","epoch 60, step 3802, loss: 0.000029\n","epoch 60, step 3803, loss: 0.000000\n","epoch 60, step 3804, loss: 0.000000\n","epoch 60, step 3805, loss: 108.074585\n","epoch 60, step 3806, loss: 0.000017\n","epoch 60, step 3807, loss: 0.002336\n","epoch 60, step 3808, loss: 0.000105\n","epoch 60, step 3809, loss: 0.000001\n","epoch 60, step 3810, loss: 0.000000\n","epoch 60, step 3811, loss: 0.000000\n","epoch 60, step 3812, loss: 0.000006\n","epoch 60, step 3813, loss: 0.000300\n","epoch 60, step 3814, loss: 0.000000\n","epoch 60, step 3815, loss: 0.000000\n","epoch 60, step 3816, loss: 0.000035\n","epoch 60, step 3817, loss: 0.001380\n","epoch 60, step 3818, loss: 0.000000\n","epoch 60, step 3819, loss: 0.000012\n","epoch 60, step 3820, loss: 0.000000\n","epoch 60, step 3821, loss: 13.402315\n","epoch 60, step 3822, loss: 0.000017\n","epoch 60, step 3823, loss: 0.001031\n","epoch 60, step 3824, loss: 0.000188\n","epoch 60, step 3825, loss: 0.001240\n","epoch 60, step 3826, loss: 0.000000\n","epoch 60, step 3827, loss: 0.000037\n","epoch 60, step 3828, loss: 0.007474\n","epoch 60, step 3829, loss: 0.000000\n","epoch 60, step 3830, loss: 0.000105\n","epoch 60, step 3831, loss: 10.663232\n","epoch 60, step 3832, loss: 0.000000\n","epoch 60, step 3833, loss: 0.000000\n","epoch 60, step 3834, loss: 0.000040\n","epoch 60, step 3835, loss: 0.002710\n","epoch 60, step 3836, loss: 0.000004\n","epoch 60, step 3837, loss: 34.978764\n","epoch 60, step 3838, loss: 25.369009\n","epoch 60, step 3839, loss: 0.000000\n","epoch 60, step 3840, loss: 0.000000\n","epoch 60, step 3841, loss: 0.000604\n","epoch 60, step 3842, loss: 30.386513\n","epoch 61, step 3843, loss: 15.583057\n","epoch 61, step 3844, loss: 0.000001\n","epoch 61, step 3845, loss: 0.000320\n","epoch 61, step 3846, loss: 30.729765\n","epoch 61, step 3847, loss: 0.000209\n","epoch 61, step 3848, loss: 0.000097\n","epoch 61, step 3849, loss: 0.000000\n","epoch 61, step 3850, loss: 0.001228\n","epoch 61, step 3851, loss: 0.000001\n","epoch 61, step 3852, loss: 0.010736\n","epoch 61, step 3853, loss: 52.727699\n","epoch 61, step 3854, loss: 39.793171\n","epoch 61, step 3855, loss: 0.000001\n","epoch 61, step 3856, loss: 0.000000\n","epoch 61, step 3857, loss: 0.000006\n","epoch 61, step 3858, loss: 0.000149\n","epoch 61, step 3859, loss: 0.000000\n","epoch 61, step 3860, loss: 0.000000\n","epoch 61, step 3861, loss: 0.000003\n","epoch 61, step 3862, loss: 38.008392\n","epoch 61, step 3863, loss: 0.000000\n","epoch 61, step 3864, loss: 0.000028\n","epoch 61, step 3865, loss: 0.000007\n","epoch 61, step 3866, loss: 0.000000\n","epoch 61, step 3867, loss: 0.000000\n","epoch 61, step 3868, loss: 101.108505\n","epoch 61, step 3869, loss: 0.000000\n","epoch 61, step 3870, loss: 0.000150\n","epoch 61, step 3871, loss: 0.000042\n","epoch 61, step 3872, loss: 0.000000\n","epoch 61, step 3873, loss: 0.000000\n","epoch 61, step 3874, loss: 0.000000\n","epoch 61, step 3875, loss: 0.000000\n","epoch 61, step 3876, loss: 0.000005\n","epoch 61, step 3877, loss: 0.000000\n","epoch 61, step 3878, loss: 0.000000\n","epoch 61, step 3879, loss: 0.000016\n","epoch 61, step 3880, loss: 0.000000\n","epoch 61, step 3881, loss: 0.000000\n","epoch 61, step 3882, loss: 0.000001\n","epoch 61, step 3883, loss: 0.000003\n","epoch 61, step 3884, loss: 14.623465\n","epoch 61, step 3885, loss: 0.000000\n","epoch 61, step 3886, loss: 0.000024\n","epoch 61, step 3887, loss: 0.000005\n","epoch 61, step 3888, loss: 0.000001\n","epoch 61, step 3889, loss: 0.000000\n","epoch 61, step 3890, loss: 0.000000\n","epoch 61, step 3891, loss: 0.000001\n","epoch 61, step 3892, loss: 0.000000\n","epoch 61, step 3893, loss: 0.000001\n","epoch 61, step 3894, loss: 9.682941\n","epoch 61, step 3895, loss: 0.000000\n","epoch 61, step 3896, loss: 0.000000\n","epoch 61, step 3897, loss: 0.000000\n","epoch 61, step 3898, loss: 0.000129\n","epoch 61, step 3899, loss: 0.000000\n","epoch 61, step 3900, loss: 25.006948\n","epoch 61, step 3901, loss: 38.250195\n","epoch 61, step 3902, loss: 0.000001\n","epoch 61, step 3903, loss: 0.000000\n","epoch 61, step 3904, loss: 0.000000\n","epoch 61, step 3905, loss: 37.871872\n","epoch 62, step 3906, loss: 5.202178\n","epoch 62, step 3907, loss: 0.000000\n","epoch 62, step 3908, loss: 0.000013\n","epoch 62, step 3909, loss: 40.990940\n","epoch 62, step 3910, loss: 0.000684\n","epoch 62, step 3911, loss: 0.000001\n","epoch 62, step 3912, loss: 0.000000\n","epoch 62, step 3913, loss: 0.000026\n","epoch 62, step 3914, loss: 0.000000\n","epoch 62, step 3915, loss: 0.000004\n","epoch 62, step 3916, loss: 45.767944\n","epoch 62, step 3917, loss: 41.255310\n","epoch 62, step 3918, loss: 0.000000\n","epoch 62, step 3919, loss: 0.000000\n","epoch 62, step 3920, loss: 0.000006\n","epoch 62, step 3921, loss: 0.000106\n","epoch 62, step 3922, loss: 0.000000\n","epoch 62, step 3923, loss: 0.000000\n","epoch 62, step 3924, loss: 0.000006\n","epoch 62, step 3925, loss: 36.960602\n","epoch 62, step 3926, loss: 0.000000\n","epoch 62, step 3927, loss: 0.000087\n","epoch 62, step 3928, loss: 0.000078\n","epoch 62, step 3929, loss: 0.000000\n","epoch 62, step 3930, loss: 0.000000\n","epoch 62, step 3931, loss: 108.436104\n","epoch 62, step 3932, loss: 0.000032\n","epoch 62, step 3933, loss: 0.002400\n","epoch 62, step 3934, loss: 0.000096\n","epoch 62, step 3935, loss: 0.000001\n","epoch 62, step 3936, loss: 0.000000\n","epoch 62, step 3937, loss: 0.000000\n","epoch 62, step 3938, loss: 0.000009\n","epoch 62, step 3939, loss: 0.000389\n","epoch 62, step 3940, loss: 0.000000\n","epoch 62, step 3941, loss: 0.000000\n","epoch 62, step 3942, loss: 0.000034\n","epoch 62, step 3943, loss: 0.001408\n","epoch 62, step 3944, loss: 0.000000\n","epoch 62, step 3945, loss: 0.000014\n","epoch 62, step 3946, loss: 0.000000\n","epoch 62, step 3947, loss: 13.580487\n","epoch 62, step 3948, loss: 0.000012\n","epoch 62, step 3949, loss: 0.000921\n","epoch 62, step 3950, loss: 0.000239\n","epoch 62, step 3951, loss: 0.001180\n","epoch 62, step 3952, loss: 0.000000\n","epoch 62, step 3953, loss: 0.000093\n","epoch 62, step 3954, loss: 0.008443\n","epoch 62, step 3955, loss: 0.000000\n","epoch 62, step 3956, loss: 0.000105\n","epoch 62, step 3957, loss: 11.114155\n","epoch 62, step 3958, loss: 0.000000\n","epoch 62, step 3959, loss: 0.000000\n","epoch 62, step 3960, loss: 0.000068\n","epoch 62, step 3961, loss: 0.002660\n","epoch 62, step 3962, loss: 0.000003\n","epoch 62, step 3963, loss: 34.379925\n","epoch 62, step 3964, loss: 26.397484\n","epoch 62, step 3965, loss: 0.000000\n","epoch 62, step 3966, loss: 0.000000\n","epoch 62, step 3967, loss: 0.000959\n","epoch 62, step 3968, loss: 29.456251\n","epoch 63, step 3969, loss: 15.753255\n","epoch 63, step 3970, loss: 0.000001\n","epoch 63, step 3971, loss: 0.000339\n","epoch 63, step 3972, loss: 30.983891\n","epoch 63, step 3973, loss: 0.000206\n","epoch 63, step 3974, loss: 0.000217\n","epoch 63, step 3975, loss: 0.000000\n","epoch 63, step 3976, loss: 0.001667\n","epoch 63, step 3977, loss: 0.000001\n","epoch 63, step 3978, loss: 0.010268\n","epoch 63, step 3979, loss: 54.330059\n","epoch 63, step 3980, loss: 39.958694\n","epoch 63, step 3981, loss: 0.000001\n","epoch 63, step 3982, loss: 0.000000\n","epoch 63, step 3983, loss: 0.000007\n","epoch 63, step 3984, loss: 0.000138\n","epoch 63, step 3985, loss: 0.000000\n","epoch 63, step 3986, loss: 0.000000\n","epoch 63, step 3987, loss: 0.000005\n","epoch 63, step 3988, loss: 39.139229\n","epoch 63, step 3989, loss: 0.000000\n","epoch 63, step 3990, loss: 0.000031\n","epoch 63, step 3991, loss: 0.000017\n","epoch 63, step 3992, loss: 0.000000\n","epoch 63, step 3993, loss: 0.000000\n","epoch 63, step 3994, loss: 101.279083\n","epoch 63, step 3995, loss: 0.000000\n","epoch 63, step 3996, loss: 0.000125\n","epoch 63, step 3997, loss: 0.000038\n","epoch 63, step 3998, loss: 0.000000\n","epoch 63, step 3999, loss: 0.000000\n","epoch 63, step 4000, loss: 0.000000\n","epoch 63, step 4001, loss: 0.000000\n","epoch 63, step 4002, loss: 0.000006\n","epoch 63, step 4003, loss: 0.000000\n","epoch 63, step 4004, loss: 0.000000\n","epoch 63, step 4005, loss: 0.000016\n","epoch 63, step 4006, loss: 0.000000\n","epoch 63, step 4007, loss: 0.000000\n","epoch 63, step 4008, loss: 0.000001\n","epoch 63, step 4009, loss: 0.000004\n","epoch 63, step 4010, loss: 14.821484\n","epoch 63, step 4011, loss: 0.000000\n","epoch 63, step 4012, loss: 0.000019\n","epoch 63, step 4013, loss: 0.000006\n","epoch 63, step 4014, loss: 0.000001\n","epoch 63, step 4015, loss: 0.000000\n","epoch 63, step 4016, loss: 0.000001\n","epoch 63, step 4017, loss: 0.000001\n","epoch 63, step 4018, loss: 0.000000\n","epoch 63, step 4019, loss: 0.000001\n","epoch 63, step 4020, loss: 10.102414\n","epoch 63, step 4021, loss: 0.000000\n","epoch 63, step 4022, loss: 0.000000\n","epoch 63, step 4023, loss: 0.000001\n","epoch 63, step 4024, loss: 0.000125\n","epoch 63, step 4025, loss: 0.000000\n","epoch 63, step 4026, loss: 24.213623\n","epoch 63, step 4027, loss: 39.452431\n","epoch 63, step 4028, loss: 0.000002\n","epoch 63, step 4029, loss: 0.000000\n","epoch 63, step 4030, loss: 0.000000\n","epoch 63, step 4031, loss: 37.088047\n","epoch 64, step 4032, loss: 5.180464\n","epoch 64, step 4033, loss: 0.000000\n","epoch 64, step 4034, loss: 0.000012\n","epoch 64, step 4035, loss: 41.407051\n","epoch 64, step 4036, loss: 0.000631\n","epoch 64, step 4037, loss: 0.000002\n","epoch 64, step 4038, loss: 0.000000\n","epoch 64, step 4039, loss: 0.000030\n","epoch 64, step 4040, loss: 0.000000\n","epoch 64, step 4041, loss: 0.000005\n","epoch 64, step 4042, loss: 47.203747\n","epoch 64, step 4043, loss: 42.449444\n","epoch 64, step 4044, loss: 0.000000\n","epoch 64, step 4045, loss: 0.000000\n","epoch 64, step 4046, loss: 0.000009\n","epoch 64, step 4047, loss: 0.000097\n","epoch 64, step 4048, loss: 0.000000\n","epoch 64, step 4049, loss: 0.000000\n","epoch 64, step 4050, loss: 0.000010\n","epoch 64, step 4051, loss: 38.046150\n","epoch 64, step 4052, loss: 0.000000\n","epoch 64, step 4053, loss: 0.000098\n","epoch 64, step 4054, loss: 0.000211\n","epoch 64, step 4055, loss: 0.000000\n","epoch 64, step 4056, loss: 0.000000\n","epoch 64, step 4057, loss: 108.681305\n","epoch 64, step 4058, loss: 0.000067\n","epoch 64, step 4059, loss: 0.002276\n","epoch 64, step 4060, loss: 0.000088\n","epoch 64, step 4061, loss: 0.000002\n","epoch 64, step 4062, loss: 0.000000\n","epoch 64, step 4063, loss: 0.000000\n","epoch 64, step 4064, loss: 0.000014\n","epoch 64, step 4065, loss: 0.000493\n","epoch 64, step 4066, loss: 0.000000\n","epoch 64, step 4067, loss: 0.000000\n","epoch 64, step 4068, loss: 0.000033\n","epoch 64, step 4069, loss: 0.001356\n","epoch 64, step 4070, loss: 0.000000\n","epoch 64, step 4071, loss: 0.000017\n","epoch 64, step 4072, loss: 0.000000\n","epoch 64, step 4073, loss: 13.742915\n","epoch 64, step 4074, loss: 0.000009\n","epoch 64, step 4075, loss: 0.000822\n","epoch 64, step 4076, loss: 0.000304\n","epoch 64, step 4077, loss: 0.001097\n","epoch 64, step 4078, loss: 0.000000\n","epoch 64, step 4079, loss: 0.000247\n","epoch 64, step 4080, loss: 0.008692\n","epoch 64, step 4081, loss: 0.000000\n","epoch 64, step 4082, loss: 0.000104\n","epoch 64, step 4083, loss: 11.561730\n","epoch 64, step 4084, loss: 0.000000\n","epoch 64, step 4085, loss: 0.000000\n","epoch 64, step 4086, loss: 0.000118\n","epoch 64, step 4087, loss: 0.002600\n","epoch 64, step 4088, loss: 0.000003\n","epoch 64, step 4089, loss: 33.748425\n","epoch 64, step 4090, loss: 27.358238\n","epoch 64, step 4091, loss: 0.000000\n","epoch 64, step 4092, loss: 0.000000\n","epoch 64, step 4093, loss: 0.001483\n","epoch 64, step 4094, loss: 28.509909\n","epoch 65, step 4095, loss: 15.915358\n","epoch 65, step 4096, loss: 0.000001\n","epoch 65, step 4097, loss: 0.000359\n","epoch 65, step 4098, loss: 31.212990\n","epoch 65, step 4099, loss: 0.000198\n","epoch 65, step 4100, loss: 0.000473\n","epoch 65, step 4101, loss: 0.000000\n","epoch 65, step 4102, loss: 0.002125\n","epoch 65, step 4103, loss: 0.000002\n","epoch 65, step 4104, loss: 0.009844\n","epoch 65, step 4105, loss: 55.870941\n","epoch 65, step 4106, loss: 40.068192\n","epoch 65, step 4107, loss: 0.000000\n","epoch 65, step 4108, loss: 0.000000\n","epoch 65, step 4109, loss: 0.000007\n","epoch 65, step 4110, loss: 0.000127\n","epoch 65, step 4111, loss: 0.000000\n","epoch 65, step 4112, loss: 0.000000\n","epoch 65, step 4113, loss: 0.000008\n","epoch 65, step 4114, loss: 40.258465\n","epoch 65, step 4115, loss: 0.000000\n","epoch 65, step 4116, loss: 0.000033\n","epoch 65, step 4117, loss: 0.000041\n","epoch 65, step 4118, loss: 0.000000\n","epoch 65, step 4119, loss: 0.000000\n","epoch 65, step 4120, loss: 101.344391\n","epoch 65, step 4121, loss: 0.000000\n","epoch 65, step 4122, loss: 0.000104\n","epoch 65, step 4123, loss: 0.000034\n","epoch 65, step 4124, loss: 0.000000\n","epoch 65, step 4125, loss: 0.000000\n","epoch 65, step 4126, loss: 0.000000\n","epoch 65, step 4127, loss: 0.000000\n","epoch 65, step 4128, loss: 0.000006\n","epoch 65, step 4129, loss: 0.000000\n","epoch 65, step 4130, loss: 0.000000\n","epoch 65, step 4131, loss: 0.000016\n","epoch 65, step 4132, loss: 0.000000\n","epoch 65, step 4133, loss: 0.000000\n","epoch 65, step 4134, loss: 0.000000\n","epoch 65, step 4135, loss: 0.000008\n","epoch 65, step 4136, loss: 15.021500\n","epoch 65, step 4137, loss: 0.000000\n","epoch 65, step 4138, loss: 0.000015\n","epoch 65, step 4139, loss: 0.000007\n","epoch 65, step 4140, loss: 0.000000\n","epoch 65, step 4141, loss: 0.000000\n","epoch 65, step 4142, loss: 0.000001\n","epoch 65, step 4143, loss: 0.000001\n","epoch 65, step 4144, loss: 0.000000\n","epoch 65, step 4145, loss: 0.000001\n","epoch 65, step 4146, loss: 10.500047\n","epoch 65, step 4147, loss: 0.000000\n","epoch 65, step 4148, loss: 0.000000\n","epoch 65, step 4149, loss: 0.000001\n","epoch 65, step 4150, loss: 0.000133\n","epoch 65, step 4151, loss: 0.000000\n","epoch 65, step 4152, loss: 23.385128\n","epoch 65, step 4153, loss: 40.575493\n","epoch 65, step 4154, loss: 0.000004\n","epoch 65, step 4155, loss: 0.000000\n","epoch 65, step 4156, loss: 0.000000\n","epoch 65, step 4157, loss: 36.274311\n","epoch 66, step 4158, loss: 5.161942\n","epoch 66, step 4159, loss: 0.000000\n","epoch 66, step 4160, loss: 0.000012\n","epoch 66, step 4161, loss: 41.786961\n","epoch 66, step 4162, loss: 0.000641\n","epoch 66, step 4163, loss: 0.000003\n","epoch 66, step 4164, loss: 0.000000\n","epoch 66, step 4165, loss: 0.000032\n","epoch 66, step 4166, loss: 0.000000\n","epoch 66, step 4167, loss: 0.000007\n","epoch 66, step 4168, loss: 48.588688\n","epoch 66, step 4169, loss: 43.625172\n","epoch 66, step 4170, loss: 0.000000\n","epoch 66, step 4171, loss: 0.000000\n","epoch 66, step 4172, loss: 0.000016\n","epoch 66, step 4173, loss: 0.000089\n","epoch 66, step 4174, loss: 0.000000\n","epoch 66, step 4175, loss: 0.000000\n","epoch 66, step 4176, loss: 0.000018\n","epoch 66, step 4177, loss: 39.122974\n","epoch 66, step 4178, loss: 0.000000\n","epoch 66, step 4179, loss: 0.000110\n","epoch 66, step 4180, loss: 0.000563\n","epoch 66, step 4181, loss: 0.000000\n","epoch 66, step 4182, loss: 0.000000\n","epoch 66, step 4183, loss: 108.811325\n","epoch 66, step 4184, loss: 0.000147\n","epoch 66, step 4185, loss: 0.002122\n","epoch 66, step 4186, loss: 0.000081\n","epoch 66, step 4187, loss: 0.000004\n","epoch 66, step 4188, loss: 0.000000\n","epoch 66, step 4189, loss: 0.000000\n","epoch 66, step 4190, loss: 0.000021\n","epoch 66, step 4191, loss: 0.000607\n","epoch 66, step 4192, loss: 0.000000\n","epoch 66, step 4193, loss: 0.000000\n","epoch 66, step 4194, loss: 0.000032\n","epoch 66, step 4195, loss: 0.001285\n","epoch 66, step 4196, loss: 0.000000\n","epoch 66, step 4197, loss: 0.000020\n","epoch 66, step 4198, loss: 0.000000\n","epoch 66, step 4199, loss: 13.909211\n","epoch 66, step 4200, loss: 0.000007\n","epoch 66, step 4201, loss: 0.000729\n","epoch 66, step 4202, loss: 0.000388\n","epoch 66, step 4203, loss: 0.001000\n","epoch 66, step 4204, loss: 0.000000\n","epoch 66, step 4205, loss: 0.000637\n","epoch 66, step 4206, loss: 0.008650\n","epoch 66, step 4207, loss: 0.000000\n","epoch 66, step 4208, loss: 0.000103\n","epoch 66, step 4209, loss: 11.983197\n","epoch 66, step 4210, loss: 0.000000\n","epoch 66, step 4211, loss: 0.000000\n","epoch 66, step 4212, loss: 0.000210\n","epoch 66, step 4213, loss: 0.002520\n","epoch 66, step 4214, loss: 0.000003\n","epoch 66, step 4215, loss: 33.066517\n","epoch 66, step 4216, loss: 28.263920\n","epoch 66, step 4217, loss: 0.000000\n","epoch 66, step 4218, loss: 0.000000\n","epoch 66, step 4219, loss: 0.002110\n","epoch 66, step 4220, loss: 27.546923\n","epoch 67, step 4221, loss: 16.059952\n","epoch 67, step 4222, loss: 0.000001\n","epoch 67, step 4223, loss: 0.000378\n","epoch 67, step 4224, loss: 31.425804\n","epoch 67, step 4225, loss: 0.000184\n","epoch 67, step 4226, loss: 0.000953\n","epoch 67, step 4227, loss: 0.000000\n","epoch 67, step 4228, loss: 0.002511\n","epoch 67, step 4229, loss: 0.000004\n","epoch 67, step 4230, loss: 0.009335\n","epoch 67, step 4231, loss: 57.346134\n","epoch 67, step 4232, loss: 40.119446\n","epoch 67, step 4233, loss: 0.000000\n","epoch 67, step 4234, loss: 0.000000\n","epoch 67, step 4235, loss: 0.000008\n","epoch 67, step 4236, loss: 0.000116\n","epoch 67, step 4237, loss: 0.000000\n","epoch 67, step 4238, loss: 0.000000\n","epoch 67, step 4239, loss: 0.000013\n","epoch 67, step 4240, loss: 41.373096\n","epoch 67, step 4241, loss: 0.000000\n","epoch 67, step 4242, loss: 0.000035\n","epoch 67, step 4243, loss: 0.000091\n","epoch 67, step 4244, loss: 0.000000\n","epoch 67, step 4245, loss: 0.000000\n","epoch 67, step 4246, loss: 101.297569\n","epoch 67, step 4247, loss: 0.000001\n","epoch 67, step 4248, loss: 0.000086\n","epoch 67, step 4249, loss: 0.000030\n","epoch 67, step 4250, loss: 0.000000\n","epoch 67, step 4251, loss: 0.000000\n","epoch 67, step 4252, loss: 0.000000\n","epoch 67, step 4253, loss: 0.000001\n","epoch 67, step 4254, loss: 0.000007\n","epoch 67, step 4255, loss: 0.000000\n","epoch 67, step 4256, loss: 0.000000\n","epoch 67, step 4257, loss: 0.000016\n","epoch 67, step 4258, loss: 0.000000\n","epoch 67, step 4259, loss: 0.000000\n","epoch 67, step 4260, loss: 0.000000\n","epoch 67, step 4261, loss: 0.000015\n","epoch 67, step 4262, loss: 15.232841\n","epoch 67, step 4263, loss: 0.000000\n","epoch 67, step 4264, loss: 0.000012\n","epoch 67, step 4265, loss: 0.000009\n","epoch 67, step 4266, loss: 0.000000\n","epoch 67, step 4267, loss: 0.000000\n","epoch 67, step 4268, loss: 0.000002\n","epoch 67, step 4269, loss: 0.000001\n","epoch 67, step 4270, loss: 0.000000\n","epoch 67, step 4271, loss: 0.000000\n","epoch 67, step 4272, loss: 10.864714\n","epoch 67, step 4273, loss: 0.000000\n","epoch 67, step 4274, loss: 0.000000\n","epoch 67, step 4275, loss: 0.000002\n","epoch 67, step 4276, loss: 0.000166\n","epoch 67, step 4277, loss: 0.000000\n","epoch 67, step 4278, loss: 22.515491\n","epoch 67, step 4279, loss: 41.705952\n","epoch 67, step 4280, loss: 0.000009\n","epoch 67, step 4281, loss: 0.000000\n","epoch 67, step 4282, loss: 0.000000\n","epoch 67, step 4283, loss: 35.389637\n","epoch 68, step 4284, loss: 4.744251\n","epoch 68, step 4285, loss: 0.000000\n","epoch 68, step 4286, loss: 0.000011\n","epoch 68, step 4287, loss: 42.521370\n","epoch 68, step 4288, loss: 0.000361\n","epoch 68, step 4289, loss: 0.000005\n","epoch 68, step 4290, loss: 0.000000\n","epoch 68, step 4291, loss: 0.000032\n","epoch 68, step 4292, loss: 0.000000\n","epoch 68, step 4293, loss: 0.000008\n","epoch 68, step 4294, loss: 50.078705\n","epoch 68, step 4295, loss: 44.590939\n","epoch 68, step 4296, loss: 0.000000\n","epoch 68, step 4297, loss: 0.000000\n","epoch 68, step 4298, loss: 0.000015\n","epoch 68, step 4299, loss: 0.000087\n","epoch 68, step 4300, loss: 0.000000\n","epoch 68, step 4301, loss: 0.000000\n","epoch 68, step 4302, loss: 0.000031\n","epoch 68, step 4303, loss: 40.095936\n","epoch 68, step 4304, loss: 0.000000\n","epoch 68, step 4305, loss: 0.000152\n","epoch 68, step 4306, loss: 0.001448\n","epoch 68, step 4307, loss: 0.000000\n","epoch 68, step 4308, loss: 0.000000\n","epoch 68, step 4309, loss: 109.798225\n","epoch 68, step 4310, loss: 0.000319\n","epoch 68, step 4311, loss: 0.002156\n","epoch 68, step 4312, loss: 0.000079\n","epoch 68, step 4313, loss: 0.000009\n","epoch 68, step 4314, loss: 0.000000\n","epoch 68, step 4315, loss: 0.000000\n","epoch 68, step 4316, loss: 0.000030\n","epoch 68, step 4317, loss: 0.000820\n","epoch 68, step 4318, loss: 0.000000\n","epoch 68, step 4319, loss: 0.000000\n","epoch 68, step 4320, loss: 0.000032\n","epoch 68, step 4321, loss: 0.001325\n","epoch 68, step 4322, loss: 0.000000\n","epoch 68, step 4323, loss: 0.000032\n","epoch 68, step 4324, loss: 0.000000\n","epoch 68, step 4325, loss: 14.011091\n","epoch 68, step 4326, loss: 0.000006\n","epoch 68, step 4327, loss: 0.000697\n","epoch 68, step 4328, loss: 0.000402\n","epoch 68, step 4329, loss: 0.001534\n","epoch 68, step 4330, loss: 0.000000\n","epoch 68, step 4331, loss: 0.001053\n","epoch 68, step 4332, loss: 0.007699\n","epoch 68, step 4333, loss: 0.000000\n","epoch 68, step 4334, loss: 0.000110\n","epoch 68, step 4335, loss: 12.448855\n","epoch 68, step 4336, loss: 0.000000\n","epoch 68, step 4337, loss: 0.000000\n","epoch 68, step 4338, loss: 0.000526\n","epoch 68, step 4339, loss: 0.002139\n","epoch 68, step 4340, loss: 0.000003\n","epoch 68, step 4341, loss: 32.342453\n","epoch 68, step 4342, loss: 28.274937\n","epoch 68, step 4343, loss: 0.000000\n","epoch 68, step 4344, loss: 0.000000\n","epoch 68, step 4345, loss: 0.002575\n","epoch 68, step 4346, loss: 26.411556\n","epoch 69, step 4347, loss: 15.777948\n","epoch 69, step 4348, loss: 0.000002\n","epoch 69, step 4349, loss: 0.000345\n","epoch 69, step 4350, loss: 32.017124\n","epoch 69, step 4351, loss: 0.000412\n","epoch 69, step 4352, loss: 0.001778\n","epoch 69, step 4353, loss: 0.000000\n","epoch 69, step 4354, loss: 0.002830\n","epoch 69, step 4355, loss: 0.000007\n","epoch 69, step 4356, loss: 0.009902\n","epoch 69, step 4357, loss: 58.933853\n","epoch 69, step 4358, loss: 40.328449\n","epoch 69, step 4359, loss: 0.000000\n","epoch 69, step 4360, loss: 0.000000\n","epoch 69, step 4361, loss: 0.000009\n","epoch 69, step 4362, loss: 0.000110\n","epoch 69, step 4363, loss: 0.000000\n","epoch 69, step 4364, loss: 0.000000\n","epoch 69, step 4365, loss: 0.000017\n","epoch 69, step 4366, loss: 42.393856\n","epoch 69, step 4367, loss: 0.000000\n","epoch 69, step 4368, loss: 0.000038\n","epoch 69, step 4369, loss: 0.000161\n","epoch 69, step 4370, loss: 0.000000\n","epoch 69, step 4371, loss: 0.000000\n","epoch 69, step 4372, loss: 102.086098\n","epoch 69, step 4373, loss: 0.000001\n","epoch 69, step 4374, loss: 0.000072\n","epoch 69, step 4375, loss: 0.000027\n","epoch 69, step 4376, loss: 0.000000\n","epoch 69, step 4377, loss: 0.000000\n","epoch 69, step 4378, loss: 0.000000\n","epoch 69, step 4379, loss: 0.000001\n","epoch 69, step 4380, loss: 0.000007\n","epoch 69, step 4381, loss: 0.000000\n","epoch 69, step 4382, loss: 0.000000\n","epoch 69, step 4383, loss: 0.000016\n","epoch 69, step 4384, loss: 0.000000\n","epoch 69, step 4385, loss: 0.000000\n","epoch 69, step 4386, loss: 0.000000\n","epoch 69, step 4387, loss: 0.000039\n","epoch 69, step 4388, loss: 15.419640\n","epoch 69, step 4389, loss: 0.000000\n","epoch 69, step 4390, loss: 0.000009\n","epoch 69, step 4391, loss: 0.000013\n","epoch 69, step 4392, loss: 0.000000\n","epoch 69, step 4393, loss: 0.000000\n","epoch 69, step 4394, loss: 0.000002\n","epoch 69, step 4395, loss: 0.000000\n","epoch 69, step 4396, loss: 0.000000\n","epoch 69, step 4397, loss: 0.000000\n","epoch 69, step 4398, loss: 11.110127\n","epoch 69, step 4399, loss: 0.000000\n","epoch 69, step 4400, loss: 0.000000\n","epoch 69, step 4401, loss: 0.000005\n","epoch 69, step 4402, loss: 0.000344\n","epoch 69, step 4403, loss: 0.000000\n","epoch 69, step 4404, loss: 19.474115\n","epoch 69, step 4405, loss: 43.122501\n","epoch 69, step 4406, loss: 0.000024\n","epoch 69, step 4407, loss: 0.000000\n","epoch 69, step 4408, loss: 0.000000\n","epoch 69, step 4409, loss: 35.883156\n","epoch 70, step 4410, loss: 5.033258\n","epoch 70, step 4411, loss: 0.000002\n","epoch 70, step 4412, loss: 0.000005\n","epoch 70, step 4413, loss: 42.466766\n","epoch 70, step 4414, loss: 0.006451\n","epoch 70, step 4415, loss: 0.000000\n","epoch 70, step 4416, loss: 0.000000\n","epoch 70, step 4417, loss: 0.000024\n","epoch 70, step 4418, loss: 0.000000\n","epoch 70, step 4419, loss: 0.000209\n","epoch 70, step 4420, loss: 49.554245\n","epoch 70, step 4421, loss: 46.859653\n","epoch 70, step 4422, loss: 0.000000\n","epoch 70, step 4423, loss: 0.000000\n","epoch 70, step 4424, loss: 0.002147\n","epoch 70, step 4425, loss: 0.000929\n","epoch 70, step 4426, loss: 0.000000\n","epoch 70, step 4427, loss: 0.000000\n","epoch 70, step 4428, loss: 0.000000\n","epoch 70, step 4429, loss: 45.427063\n","epoch 70, step 4430, loss: 0.000000\n","epoch 70, step 4431, loss: 0.000095\n","epoch 70, step 4432, loss: 0.006104\n","epoch 70, step 4433, loss: 0.000001\n","epoch 70, step 4434, loss: 0.000000\n","epoch 70, step 4435, loss: 107.347313\n","epoch 70, step 4436, loss: 0.000292\n","epoch 70, step 4437, loss: 0.013665\n","epoch 70, step 4438, loss: 0.006054\n","epoch 70, step 4439, loss: 0.000002\n","epoch 70, step 4440, loss: 0.000000\n","epoch 70, step 4441, loss: 0.000000\n","epoch 70, step 4442, loss: 0.095776\n","epoch 70, step 4443, loss: 0.000134\n","epoch 70, step 4444, loss: 0.000000\n","epoch 70, step 4445, loss: 0.000000\n","epoch 70, step 4446, loss: 0.000251\n","epoch 70, step 4447, loss: 0.000138\n","epoch 70, step 4448, loss: 0.000000\n","epoch 70, step 4449, loss: 0.000044\n","epoch 70, step 4450, loss: 0.000002\n","epoch 70, step 4451, loss: 10.496957\n","epoch 70, step 4452, loss: 0.000001\n","epoch 70, step 4453, loss: 0.000228\n","epoch 70, step 4454, loss: 0.000137\n","epoch 70, step 4455, loss: 0.000410\n","epoch 70, step 4456, loss: 0.000000\n","epoch 70, step 4457, loss: 0.000405\n","epoch 70, step 4458, loss: 0.002165\n","epoch 70, step 4459, loss: 0.000000\n","epoch 70, step 4460, loss: 0.000031\n","epoch 70, step 4461, loss: 9.885286\n","epoch 70, step 4462, loss: 0.000000\n","epoch 70, step 4463, loss: 0.000000\n","epoch 70, step 4464, loss: 0.000061\n","epoch 70, step 4465, loss: 0.000219\n","epoch 70, step 4466, loss: 0.000002\n","epoch 70, step 4467, loss: 27.778526\n","epoch 70, step 4468, loss: 33.256943\n","epoch 70, step 4469, loss: 0.000000\n","epoch 70, step 4470, loss: 0.000000\n","epoch 70, step 4471, loss: 0.000013\n","epoch 70, step 4472, loss: 28.868248\n","epoch 71, step 4473, loss: 11.889057\n","epoch 71, step 4474, loss: 0.000004\n","epoch 71, step 4475, loss: 0.000060\n","epoch 71, step 4476, loss: 36.224873\n","epoch 71, step 4477, loss: 0.000003\n","epoch 71, step 4478, loss: 0.000003\n","epoch 71, step 4479, loss: 0.000000\n","epoch 71, step 4480, loss: 0.000423\n","epoch 71, step 4481, loss: 0.000002\n","epoch 71, step 4482, loss: 0.000934\n","epoch 71, step 4483, loss: 56.638733\n","epoch 71, step 4484, loss: 38.532066\n","epoch 71, step 4485, loss: 0.000001\n","epoch 71, step 4486, loss: 0.000000\n","epoch 71, step 4487, loss: 0.000167\n","epoch 71, step 4488, loss: 0.000683\n","epoch 71, step 4489, loss: 0.000000\n","epoch 71, step 4490, loss: 0.000000\n","epoch 71, step 4491, loss: 0.000000\n","epoch 71, step 4492, loss: 49.017048\n","epoch 71, step 4493, loss: 0.000000\n","epoch 71, step 4494, loss: 0.000010\n","epoch 71, step 4495, loss: 0.000032\n","epoch 71, step 4496, loss: 0.000000\n","epoch 71, step 4497, loss: 0.000000\n","epoch 71, step 4498, loss: 97.268372\n","epoch 71, step 4499, loss: 0.000001\n","epoch 71, step 4500, loss: 0.000907\n","epoch 71, step 4501, loss: 0.000411\n","epoch 71, step 4502, loss: 0.000000\n","epoch 71, step 4503, loss: 0.000000\n","epoch 71, step 4504, loss: 0.000000\n","epoch 71, step 4505, loss: 0.000001\n","epoch 71, step 4506, loss: 0.000000\n","epoch 71, step 4507, loss: 0.000000\n","epoch 71, step 4508, loss: 0.000000\n","epoch 71, step 4509, loss: 0.000033\n","epoch 71, step 4510, loss: 0.000000\n","epoch 71, step 4511, loss: 0.000000\n","epoch 71, step 4512, loss: 0.000003\n","epoch 71, step 4513, loss: 0.000001\n","epoch 71, step 4514, loss: 13.112597\n","epoch 71, step 4515, loss: 0.000000\n","epoch 71, step 4516, loss: 0.000003\n","epoch 71, step 4517, loss: 0.000002\n","epoch 71, step 4518, loss: 0.000000\n","epoch 71, step 4519, loss: 0.000000\n","epoch 71, step 4520, loss: 0.000001\n","epoch 71, step 4521, loss: 0.000000\n","epoch 71, step 4522, loss: 0.000000\n","epoch 71, step 4523, loss: 0.000000\n","epoch 71, step 4524, loss: 7.263225\n","epoch 71, step 4525, loss: 0.000000\n","epoch 71, step 4526, loss: 0.000000\n","epoch 71, step 4527, loss: 0.000000\n","epoch 71, step 4528, loss: 0.012055\n","epoch 71, step 4529, loss: 0.000000\n","epoch 71, step 4530, loss: 17.113552\n","epoch 71, step 4531, loss: 46.581863\n","epoch 71, step 4532, loss: 0.000069\n","epoch 71, step 4533, loss: 0.000000\n","epoch 71, step 4534, loss: 0.000001\n","epoch 71, step 4535, loss: 36.750072\n","epoch 72, step 4536, loss: 1.338175\n","epoch 72, step 4537, loss: 0.000001\n","epoch 72, step 4538, loss: 0.000004\n","epoch 72, step 4539, loss: 31.883608\n","epoch 72, step 4540, loss: 0.000005\n","epoch 72, step 4541, loss: 0.000000\n","epoch 72, step 4542, loss: 0.000000\n","epoch 72, step 4543, loss: 0.000097\n","epoch 72, step 4544, loss: 0.000000\n","epoch 72, step 4545, loss: 0.000060\n","epoch 72, step 4546, loss: 53.877987\n","epoch 72, step 4547, loss: 46.201916\n","epoch 72, step 4548, loss: 0.000000\n","epoch 72, step 4549, loss: 0.000000\n","epoch 72, step 4550, loss: 0.026175\n","epoch 72, step 4551, loss: 0.001117\n","epoch 72, step 4552, loss: 0.000000\n","epoch 72, step 4553, loss: 0.000000\n","epoch 72, step 4554, loss: 0.000007\n","epoch 72, step 4555, loss: 42.542011\n","epoch 72, step 4556, loss: 0.000000\n","epoch 72, step 4557, loss: 0.000431\n","epoch 72, step 4558, loss: 0.006703\n","epoch 72, step 4559, loss: 0.000074\n","epoch 72, step 4560, loss: 0.000042\n","epoch 72, step 4561, loss: 114.281960\n","epoch 72, step 4562, loss: 0.024632\n","epoch 72, step 4563, loss: 0.288835\n","epoch 72, step 4564, loss: 0.006534\n","epoch 72, step 4565, loss: 0.000435\n","epoch 72, step 4566, loss: 0.000000\n","epoch 72, step 4567, loss: 0.000000\n","epoch 72, step 4568, loss: 0.000392\n","epoch 72, step 4569, loss: 0.000030\n","epoch 72, step 4570, loss: 0.000001\n","epoch 72, step 4571, loss: 0.000000\n","epoch 72, step 4572, loss: 0.000076\n","epoch 72, step 4573, loss: 0.006612\n","epoch 72, step 4574, loss: 0.000000\n","epoch 72, step 4575, loss: 0.000027\n","epoch 72, step 4576, loss: 0.000002\n","epoch 72, step 4577, loss: 11.710367\n","epoch 72, step 4578, loss: 0.000002\n","epoch 72, step 4579, loss: 0.000033\n","epoch 72, step 4580, loss: 0.001518\n","epoch 72, step 4581, loss: 0.001084\n","epoch 72, step 4582, loss: 0.000000\n","epoch 72, step 4583, loss: 0.000041\n","epoch 72, step 4584, loss: 0.000245\n","epoch 72, step 4585, loss: 0.000000\n","epoch 72, step 4586, loss: 0.000118\n","epoch 72, step 4587, loss: 8.358066\n","epoch 72, step 4588, loss: 0.000000\n","epoch 72, step 4589, loss: 0.000000\n","epoch 72, step 4590, loss: 0.005684\n","epoch 72, step 4591, loss: 0.001278\n","epoch 72, step 4592, loss: 0.000001\n","epoch 72, step 4593, loss: 28.988754\n","epoch 72, step 4594, loss: 28.426237\n","epoch 72, step 4595, loss: 0.000000\n","epoch 72, step 4596, loss: 0.000000\n","epoch 72, step 4597, loss: 0.000143\n","epoch 72, step 4598, loss: 25.316229\n","epoch 73, step 4599, loss: 60.336582\n","epoch 73, step 4600, loss: 0.000005\n","epoch 73, step 4601, loss: 0.000479\n","epoch 73, step 4602, loss: 11.573602\n","epoch 73, step 4603, loss: 0.000000\n","epoch 73, step 4604, loss: 0.000000\n","epoch 73, step 4605, loss: 0.000000\n","epoch 73, step 4606, loss: 0.000000\n","epoch 73, step 4607, loss: 0.000000\n","epoch 73, step 4608, loss: 0.000008\n","epoch 73, step 4609, loss: 32.501591\n","epoch 73, step 4610, loss: 25.693674\n","epoch 73, step 4611, loss: 0.000000\n","epoch 73, step 4612, loss: 0.000027\n","epoch 73, step 4613, loss: 0.038923\n","epoch 73, step 4614, loss: 0.000000\n","epoch 73, step 4615, loss: 0.000000\n","epoch 73, step 4616, loss: 0.000001\n","epoch 73, step 4617, loss: 0.000328\n","epoch 73, step 4618, loss: 83.467896\n","epoch 73, step 4619, loss: 0.088883\n","epoch 73, step 4620, loss: 0.000447\n","epoch 73, step 4621, loss: 8.602894\n","epoch 73, step 4622, loss: 1.706364\n","epoch 73, step 4623, loss: 0.000000\n","epoch 73, step 4624, loss: 41.362701\n","epoch 73, step 4625, loss: 0.949240\n","epoch 73, step 4626, loss: 0.002368\n","epoch 73, step 4627, loss: 0.000010\n","epoch 73, step 4628, loss: 0.000000\n","epoch 73, step 4629, loss: 0.000000\n","epoch 73, step 4630, loss: 0.000000\n","epoch 73, step 4631, loss: 0.000000\n","epoch 73, step 4632, loss: 0.000000\n","epoch 73, step 4633, loss: 0.000599\n","epoch 73, step 4634, loss: 0.000000\n","epoch 73, step 4635, loss: 0.000002\n","epoch 73, step 4636, loss: 0.000000\n","epoch 73, step 4637, loss: 0.000011\n","epoch 73, step 4638, loss: 0.001741\n","epoch 73, step 4639, loss: 4.091414\n","epoch 73, step 4640, loss: 6.266564\n","epoch 73, step 4641, loss: 0.000635\n","epoch 73, step 4642, loss: 1.441782\n","epoch 73, step 4643, loss: 0.000000\n","epoch 73, step 4644, loss: 0.000046\n","epoch 73, step 4645, loss: 0.000000\n","epoch 73, step 4646, loss: 0.000000\n","epoch 73, step 4647, loss: 0.000000\n","epoch 73, step 4648, loss: 0.000000\n","epoch 73, step 4649, loss: 0.000000\n","epoch 73, step 4650, loss: 11.997179\n","epoch 73, step 4651, loss: 0.000171\n","epoch 73, step 4652, loss: 0.000000\n","epoch 73, step 4653, loss: 4.072674\n","epoch 73, step 4654, loss: 2.023054\n","epoch 73, step 4655, loss: 0.090286\n","epoch 73, step 4656, loss: 5.406933\n","epoch 73, step 4657, loss: 70.448311\n","epoch 73, step 4658, loss: 0.000000\n","epoch 73, step 4659, loss: 0.000000\n","epoch 73, step 4660, loss: 1.820539\n","epoch 73, step 4661, loss: 46.002033\n","epoch 74, step 4662, loss: 52.508747\n","epoch 74, step 4663, loss: 0.000058\n","epoch 74, step 4664, loss: 0.000013\n","epoch 74, step 4665, loss: 290.865875\n","epoch 74, step 4666, loss: 0.000000\n","epoch 74, step 4667, loss: 0.844135\n","epoch 74, step 4668, loss: 0.000000\n","epoch 74, step 4669, loss: 0.000001\n","epoch 74, step 4670, loss: 0.000000\n","epoch 74, step 4671, loss: 0.011232\n","epoch 74, step 4672, loss: 38.816902\n","epoch 74, step 4673, loss: 37.254436\n","epoch 74, step 4674, loss: 0.000000\n","epoch 74, step 4675, loss: 0.000000\n","epoch 74, step 4676, loss: 0.000001\n","epoch 74, step 4677, loss: 0.000251\n","epoch 74, step 4678, loss: 5.327023\n","epoch 74, step 4679, loss: 0.000000\n","epoch 74, step 4680, loss: 0.000000\n","epoch 74, step 4681, loss: 88.076019\n","epoch 74, step 4682, loss: 10.944201\n","epoch 74, step 4683, loss: 0.000000\n","epoch 74, step 4684, loss: 0.000000\n","epoch 74, step 4685, loss: 0.000000\n","epoch 74, step 4686, loss: 0.000000\n","epoch 74, step 4687, loss: 66.119217\n","epoch 74, step 4688, loss: 0.000000\n","epoch 74, step 4689, loss: 12.078286\n","epoch 74, step 4690, loss: 0.155786\n","epoch 74, step 4691, loss: 0.000000\n","epoch 74, step 4692, loss: 0.000000\n","epoch 74, step 4693, loss: 0.000000\n","epoch 74, step 4694, loss: 0.000009\n","epoch 74, step 4695, loss: 3.079065\n","epoch 74, step 4696, loss: 9.168371\n","epoch 74, step 4697, loss: 0.000000\n","epoch 74, step 4698, loss: 0.000000\n","epoch 74, step 4699, loss: 0.004645\n","epoch 74, step 4700, loss: 0.000001\n","epoch 74, step 4701, loss: 0.000000\n","epoch 74, step 4702, loss: 1.772378\n","epoch 74, step 4703, loss: 13.589411\n","epoch 74, step 4704, loss: 8.576445\n","epoch 74, step 4705, loss: 0.614397\n","epoch 74, step 4706, loss: 0.000121\n","epoch 74, step 4707, loss: 13.932119\n","epoch 74, step 4708, loss: 0.000391\n","epoch 74, step 4709, loss: 0.001854\n","epoch 74, step 4710, loss: 0.000647\n","epoch 74, step 4711, loss: 0.000000\n","epoch 74, step 4712, loss: 0.000030\n","epoch 74, step 4713, loss: 130.932053\n","epoch 74, step 4714, loss: 17.647385\n","epoch 74, step 4715, loss: 0.000018\n","epoch 74, step 4716, loss: 0.000095\n","epoch 74, step 4717, loss: 10.944582\n","epoch 74, step 4718, loss: 0.294054\n","epoch 74, step 4719, loss: 53.680084\n","epoch 74, step 4720, loss: 32.956860\n","epoch 74, step 4721, loss: 0.000000\n","epoch 74, step 4722, loss: 0.000000\n","epoch 74, step 4723, loss: 0.000000\n","epoch 74, step 4724, loss: 75.910294\n","epoch 75, step 4725, loss: 0.000004\n","epoch 75, step 4726, loss: 0.000000\n","epoch 75, step 4727, loss: 0.000000\n","epoch 75, step 4728, loss: 131.331818\n","epoch 75, step 4729, loss: 10.102834\n","epoch 75, step 4730, loss: 0.000060\n","epoch 75, step 4731, loss: 0.000000\n","epoch 75, step 4732, loss: 0.004802\n","epoch 75, step 4733, loss: 0.000008\n","epoch 75, step 4734, loss: 49.374290\n","epoch 75, step 4735, loss: 52.763866\n","epoch 75, step 4736, loss: 33.533031\n","epoch 75, step 4737, loss: 29.417503\n","epoch 75, step 4738, loss: 0.000001\n","epoch 75, step 4739, loss: 28.426762\n","epoch 75, step 4740, loss: 0.000001\n","epoch 75, step 4741, loss: 92.048279\n","epoch 75, step 4742, loss: 26.704718\n","epoch 75, step 4743, loss: 17.872780\n","epoch 75, step 4744, loss: 105.790970\n","epoch 75, step 4745, loss: 0.000000\n","epoch 75, step 4746, loss: 0.000000\n","epoch 75, step 4747, loss: 0.000000\n","epoch 75, step 4748, loss: 0.000000\n","epoch 75, step 4749, loss: 0.933983\n","epoch 75, step 4750, loss: 79.798729\n","epoch 75, step 4751, loss: 246.273636\n","epoch 75, step 4752, loss: 14.345144\n","epoch 75, step 4753, loss: 8.068063\n","epoch 75, step 4754, loss: 0.000262\n","epoch 75, step 4755, loss: 3.393309\n","epoch 75, step 4756, loss: 0.000000\n","epoch 75, step 4757, loss: 22.583485\n","epoch 75, step 4758, loss: 0.000000\n","epoch 75, step 4759, loss: 0.000000\n","epoch 75, step 4760, loss: 0.027836\n","epoch 75, step 4761, loss: 0.000000\n","epoch 75, step 4762, loss: 0.000002\n","epoch 75, step 4763, loss: 152.256958\n","epoch 75, step 4764, loss: 0.000000\n","epoch 75, step 4765, loss: 24.677341\n","epoch 75, step 4766, loss: 72.660286\n","epoch 75, step 4767, loss: 6.509823\n","epoch 75, step 4768, loss: 33.902596\n","epoch 75, step 4769, loss: 0.000000\n","epoch 75, step 4770, loss: 3.093156\n","epoch 75, step 4771, loss: 0.592019\n","epoch 75, step 4772, loss: 55.607201\n","epoch 75, step 4773, loss: 53.094776\n","epoch 75, step 4774, loss: 13.501125\n","epoch 75, step 4775, loss: 2.693165\n","epoch 75, step 4776, loss: 0.010431\n","epoch 75, step 4777, loss: 0.039455\n","epoch 75, step 4778, loss: 0.000000\n","epoch 75, step 4779, loss: 28.807461\n","epoch 75, step 4780, loss: 5.072197\n","epoch 75, step 4781, loss: 3.109011\n","epoch 75, step 4782, loss: 119.535110\n","epoch 75, step 4783, loss: 163.977798\n","epoch 75, step 4784, loss: 28.774248\n","epoch 75, step 4785, loss: 41.108852\n","epoch 75, step 4786, loss: 5.725701\n","epoch 75, step 4787, loss: 8.130419\n","epoch 76, step 4788, loss: 123.877365\n","epoch 76, step 4789, loss: 35.481899\n","epoch 76, step 4790, loss: 7.362789\n","epoch 76, step 4791, loss: 192.897980\n","epoch 76, step 4792, loss: 76.523857\n","epoch 76, step 4793, loss: 0.000000\n","epoch 76, step 4794, loss: 7.582537\n","epoch 76, step 4795, loss: 0.000001\n","epoch 76, step 4796, loss: 0.000000\n","epoch 76, step 4797, loss: 4.660598\n","epoch 76, step 4798, loss: 133.452377\n","epoch 76, step 4799, loss: 56.165287\n","epoch 76, step 4800, loss: 31.490231\n","epoch 76, step 4801, loss: 15.301491\n","epoch 76, step 4802, loss: 79.871529\n","epoch 76, step 4803, loss: 89.337563\n","epoch 76, step 4804, loss: 50.566154\n","epoch 76, step 4805, loss: 51.704567\n","epoch 76, step 4806, loss: 97.502090\n","epoch 76, step 4807, loss: 96.218224\n","epoch 76, step 4808, loss: 49.685753\n","epoch 76, step 4809, loss: 0.000000\n","epoch 76, step 4810, loss: 0.000001\n","epoch 76, step 4811, loss: 0.000000\n","epoch 76, step 4812, loss: 0.000003\n","epoch 76, step 4813, loss: 100.841141\n","epoch 76, step 4814, loss: 10.088321\n","epoch 76, step 4815, loss: 30.314838\n","epoch 76, step 4816, loss: 26.986803\n","epoch 76, step 4817, loss: 9.242859\n","epoch 76, step 4818, loss: 0.357261\n","epoch 76, step 4819, loss: 6.817836\n","epoch 76, step 4820, loss: 22.191460\n","epoch 76, step 4821, loss: 0.000000\n","epoch 76, step 4822, loss: 0.855423\n","epoch 76, step 4823, loss: 58.935699\n","epoch 76, step 4824, loss: 55.055912\n","epoch 76, step 4825, loss: 86.636719\n","epoch 76, step 4826, loss: 9.022301\n","epoch 76, step 4827, loss: 0.000000\n","epoch 76, step 4828, loss: 0.000000\n","epoch 76, step 4829, loss: 128.773697\n","epoch 76, step 4830, loss: 0.644196\n","epoch 76, step 4831, loss: 0.000000\n","epoch 76, step 4832, loss: 128.070343\n","epoch 76, step 4833, loss: 0.000526\n","epoch 76, step 4834, loss: 0.000000\n","epoch 76, step 4835, loss: 4.493909\n","epoch 76, step 4836, loss: 147.806473\n","epoch 76, step 4837, loss: 0.000000\n","epoch 76, step 4838, loss: 27.370384\n","epoch 76, step 4839, loss: 103.903831\n","epoch 76, step 4840, loss: 3.250280\n","epoch 76, step 4841, loss: 0.000000\n","epoch 76, step 4842, loss: 18.851192\n","epoch 76, step 4843, loss: 0.002068\n","epoch 76, step 4844, loss: 0.000000\n","epoch 76, step 4845, loss: 0.000017\n","epoch 76, step 4846, loss: 145.859070\n","epoch 76, step 4847, loss: 0.000008\n","epoch 76, step 4848, loss: 0.000000\n","epoch 76, step 4849, loss: 86.117439\n","epoch 76, step 4850, loss: 138.589386\n","epoch 77, step 4851, loss: 0.000000\n","epoch 77, step 4852, loss: 67.758514\n","epoch 77, step 4853, loss: 0.000000\n","epoch 77, step 4854, loss: 274.173187\n","epoch 77, step 4855, loss: 0.000037\n","epoch 77, step 4856, loss: 43.114178\n","epoch 77, step 4857, loss: 0.388196\n","epoch 77, step 4858, loss: 61.655045\n","epoch 77, step 4859, loss: 18.935347\n","epoch 77, step 4860, loss: 93.629997\n","epoch 77, step 4861, loss: 67.339859\n","epoch 77, step 4862, loss: 9.975202\n","epoch 77, step 4863, loss: 40.233860\n","epoch 77, step 4864, loss: 0.000000\n","epoch 77, step 4865, loss: 0.978401\n","epoch 77, step 4866, loss: 81.951530\n","epoch 77, step 4867, loss: 0.000000\n","epoch 77, step 4868, loss: 37.625584\n","epoch 77, step 4869, loss: 0.000000\n","epoch 77, step 4870, loss: 106.469162\n","epoch 77, step 4871, loss: 94.344666\n","epoch 77, step 4872, loss: 21.941450\n","epoch 77, step 4873, loss: 107.654869\n","epoch 77, step 4874, loss: 0.000000\n","epoch 77, step 4875, loss: 0.003747\n","epoch 77, step 4876, loss: 53.173931\n","epoch 77, step 4877, loss: 9.505305\n","epoch 77, step 4878, loss: 0.100510\n","epoch 77, step 4879, loss: 0.000000\n","epoch 77, step 4880, loss: 3.146622\n","epoch 77, step 4881, loss: 68.529167\n","epoch 77, step 4882, loss: 0.000000\n","epoch 77, step 4883, loss: 14.394881\n","epoch 77, step 4884, loss: 0.001101\n","epoch 77, step 4885, loss: 0.000000\n","epoch 77, step 4886, loss: 0.247321\n","epoch 77, step 4887, loss: 38.140640\n","epoch 77, step 4888, loss: 170.569397\n","epoch 77, step 4889, loss: 10.496095\n","epoch 77, step 4890, loss: 0.000000\n","epoch 77, step 4891, loss: 0.004174\n","epoch 77, step 4892, loss: 61.339012\n","epoch 77, step 4893, loss: 0.000010\n","epoch 77, step 4894, loss: 0.353525\n","epoch 77, step 4895, loss: 0.000002\n","epoch 77, step 4896, loss: 5.226906\n","epoch 77, step 4897, loss: 0.000007\n","epoch 77, step 4898, loss: 21.499784\n","epoch 77, step 4899, loss: 5.122208\n","epoch 77, step 4900, loss: 109.071991\n","epoch 77, step 4901, loss: 0.000000\n","epoch 77, step 4902, loss: 13.151200\n","epoch 77, step 4903, loss: 29.545542\n","epoch 77, step 4904, loss: 0.000000\n","epoch 77, step 4905, loss: 0.000000\n","epoch 77, step 4906, loss: 0.648317\n","epoch 77, step 4907, loss: 0.000000\n","epoch 77, step 4908, loss: 120.725372\n","epoch 77, step 4909, loss: 0.000006\n","epoch 77, step 4910, loss: 54.713470\n","epoch 77, step 4911, loss: 30.355202\n","epoch 77, step 4912, loss: 3.889621\n","epoch 77, step 4913, loss: 46.231564\n","epoch 78, step 4914, loss: 0.000000\n","epoch 78, step 4915, loss: 0.000003\n","epoch 78, step 4916, loss: 2.971519\n","epoch 78, step 4917, loss: 156.723984\n","epoch 78, step 4918, loss: 5.817225\n","epoch 78, step 4919, loss: 23.618826\n","epoch 78, step 4920, loss: 0.629144\n","epoch 78, step 4921, loss: 0.000001\n","epoch 78, step 4922, loss: 0.000000\n","epoch 78, step 4923, loss: 18.015667\n","epoch 78, step 4924, loss: 97.726738\n","epoch 78, step 4925, loss: 48.744827\n","epoch 78, step 4926, loss: 135.176804\n","epoch 78, step 4927, loss: 44.562073\n","epoch 78, step 4928, loss: 0.000009\n","epoch 78, step 4929, loss: 16.169165\n","epoch 78, step 4930, loss: 4.928790\n","epoch 78, step 4931, loss: 24.655323\n","epoch 78, step 4932, loss: 0.000021\n","epoch 78, step 4933, loss: 76.207893\n","epoch 78, step 4934, loss: 0.000000\n","epoch 78, step 4935, loss: 0.000000\n","epoch 78, step 4936, loss: 0.000000\n","epoch 78, step 4937, loss: 0.000000\n","epoch 78, step 4938, loss: 0.000000\n","epoch 78, step 4939, loss: 9.067165\n","epoch 78, step 4940, loss: 0.000000\n","epoch 78, step 4941, loss: 68.075455\n","epoch 78, step 4942, loss: 0.000000\n","epoch 78, step 4943, loss: 0.000000\n","epoch 78, step 4944, loss: 33.038109\n","epoch 78, step 4945, loss: 39.914871\n","epoch 78, step 4946, loss: 0.000000\n","epoch 78, step 4947, loss: 0.000063\n","epoch 78, step 4948, loss: 75.461975\n","epoch 78, step 4949, loss: 151.365692\n","epoch 78, step 4950, loss: 0.000000\n","epoch 78, step 4951, loss: 2.387784\n","epoch 78, step 4952, loss: 0.113358\n","epoch 78, step 4953, loss: 3.417830\n","epoch 78, step 4954, loss: 20.283655\n","epoch 78, step 4955, loss: 160.186462\n","epoch 78, step 4956, loss: 0.000000\n","epoch 78, step 4957, loss: 0.000000\n","epoch 78, step 4958, loss: 10.239597\n","epoch 78, step 4959, loss: 1.565829\n","epoch 78, step 4960, loss: 0.172174\n","epoch 78, step 4961, loss: 0.000178\n","epoch 78, step 4962, loss: 12.030566\n","epoch 78, step 4963, loss: 0.000000\n","epoch 78, step 4964, loss: 0.000000\n","epoch 78, step 4965, loss: 0.003735\n","epoch 78, step 4966, loss: 0.000000\n","epoch 78, step 4967, loss: 0.000000\n","epoch 78, step 4968, loss: 22.916040\n","epoch 78, step 4969, loss: 10.459453\n","epoch 78, step 4970, loss: 12.728510\n","epoch 78, step 4971, loss: 3.585936\n","epoch 78, step 4972, loss: 81.127060\n","epoch 78, step 4973, loss: 0.000000\n","epoch 78, step 4974, loss: 0.000757\n","epoch 78, step 4975, loss: 0.000967\n","epoch 78, step 4976, loss: 68.607742\n","epoch 79, step 4977, loss: 0.000000\n","epoch 79, step 4978, loss: 0.000041\n","epoch 79, step 4979, loss: 0.000000\n","epoch 79, step 4980, loss: 66.537270\n","epoch 79, step 4981, loss: 1.733139\n","epoch 79, step 4982, loss: 0.789270\n","epoch 79, step 4983, loss: 0.000000\n","epoch 79, step 4984, loss: 0.000000\n","epoch 79, step 4985, loss: 0.000000\n","epoch 79, step 4986, loss: 18.720221\n","epoch 79, step 4987, loss: 42.705574\n","epoch 79, step 4988, loss: 13.709704\n","epoch 79, step 4989, loss: 0.001075\n","epoch 79, step 4990, loss: 0.277637\n","epoch 79, step 4991, loss: 0.000266\n","epoch 79, step 4992, loss: 14.252308\n","epoch 79, step 4993, loss: 0.000000\n","epoch 79, step 4994, loss: 0.090602\n","epoch 79, step 4995, loss: 0.000004\n","epoch 79, step 4996, loss: 42.832115\n","epoch 79, step 4997, loss: 0.000000\n","epoch 79, step 4998, loss: 0.000000\n","epoch 79, step 4999, loss: 0.000000\n","epoch 79, step 5000, loss: 39.892387\n","epoch 79, step 5001, loss: 42.464668\n","epoch 79, step 5002, loss: 42.641300\n","epoch 79, step 5003, loss: 0.000005\n","epoch 79, step 5004, loss: 0.011407\n","epoch 79, step 5005, loss: 0.000000\n","epoch 79, step 5006, loss: 0.000000\n","epoch 79, step 5007, loss: 0.000000\n","epoch 79, step 5008, loss: 0.000000\n","epoch 79, step 5009, loss: 0.000000\n","epoch 79, step 5010, loss: 0.000000\n","epoch 79, step 5011, loss: 34.962803\n","epoch 79, step 5012, loss: 0.000000\n","epoch 79, step 5013, loss: 0.000000\n","epoch 79, step 5014, loss: 0.000000\n","epoch 79, step 5015, loss: 0.000000\n","epoch 79, step 5016, loss: 0.000000\n","epoch 79, step 5017, loss: 7.396385\n","epoch 79, step 5018, loss: 39.476871\n","epoch 79, step 5019, loss: 0.000000\n","epoch 79, step 5020, loss: 0.285492\n","epoch 79, step 5021, loss: 0.000004\n","epoch 79, step 5022, loss: 28.559763\n","epoch 79, step 5023, loss: 0.000000\n","epoch 79, step 5024, loss: 13.382712\n","epoch 79, step 5025, loss: 0.000000\n","epoch 79, step 5026, loss: 0.000000\n","epoch 79, step 5027, loss: 33.506943\n","epoch 79, step 5028, loss: 0.000004\n","epoch 79, step 5029, loss: 7.567394\n","epoch 79, step 5030, loss: 0.161196\n","epoch 79, step 5031, loss: 0.000000\n","epoch 79, step 5032, loss: 0.000000\n","epoch 79, step 5033, loss: 0.000000\n","epoch 79, step 5034, loss: 19.183147\n","epoch 79, step 5035, loss: 64.215973\n","epoch 79, step 5036, loss: 0.000000\n","epoch 79, step 5037, loss: 4.795460\n","epoch 79, step 5038, loss: 0.000000\n","epoch 79, step 5039, loss: 44.722328\n","epoch 80, step 5040, loss: 80.368080\n","epoch 80, step 5041, loss: 5.076703\n","epoch 80, step 5042, loss: 0.000005\n","epoch 80, step 5043, loss: 49.952812\n","epoch 80, step 5044, loss: 7.473488\n","epoch 80, step 5045, loss: 0.000000\n","epoch 80, step 5046, loss: 0.004061\n","epoch 80, step 5047, loss: 0.000000\n","epoch 80, step 5048, loss: 0.000000\n","epoch 80, step 5049, loss: 0.000000\n","epoch 80, step 5050, loss: 24.473068\n","epoch 80, step 5051, loss: 67.210663\n","epoch 80, step 5052, loss: 0.027957\n","epoch 80, step 5053, loss: 0.000000\n","epoch 80, step 5054, loss: 0.000004\n","epoch 80, step 5055, loss: 21.172445\n","epoch 80, step 5056, loss: 0.000000\n","epoch 80, step 5057, loss: 0.000002\n","epoch 80, step 5058, loss: 0.000000\n","epoch 80, step 5059, loss: 40.979916\n","epoch 80, step 5060, loss: 10.901911\n","epoch 80, step 5061, loss: 7.967704\n","epoch 80, step 5062, loss: 0.000000\n","epoch 80, step 5063, loss: 0.000000\n","epoch 80, step 5064, loss: 0.001100\n","epoch 80, step 5065, loss: 46.329742\n","epoch 80, step 5066, loss: 0.489657\n","epoch 80, step 5067, loss: 43.894897\n","epoch 80, step 5068, loss: 0.000441\n","epoch 80, step 5069, loss: 0.000000\n","epoch 80, step 5070, loss: 0.000696\n","epoch 80, step 5071, loss: 0.000000\n","epoch 80, step 5072, loss: 0.000000\n","epoch 80, step 5073, loss: 164.379868\n","epoch 80, step 5074, loss: 0.000000\n","epoch 80, step 5075, loss: 0.000000\n","epoch 80, step 5076, loss: 0.000000\n","epoch 80, step 5077, loss: 2.055537\n","epoch 80, step 5078, loss: 0.000392\n","epoch 80, step 5079, loss: 0.000000\n","epoch 80, step 5080, loss: 0.000000\n","epoch 80, step 5081, loss: 20.591347\n","epoch 80, step 5082, loss: 0.000000\n","epoch 80, step 5083, loss: 0.000000\n","epoch 80, step 5084, loss: 0.000000\n","epoch 80, step 5085, loss: 0.000000\n","epoch 80, step 5086, loss: 0.000000\n","epoch 80, step 5087, loss: 0.000000\n","epoch 80, step 5088, loss: 0.000000\n","epoch 80, step 5089, loss: 0.000000\n","epoch 80, step 5090, loss: 0.000000\n","epoch 80, step 5091, loss: 0.000158\n","epoch 80, step 5092, loss: 0.022106\n","epoch 80, step 5093, loss: 0.000000\n","epoch 80, step 5094, loss: 0.000000\n","epoch 80, step 5095, loss: 0.000000\n","epoch 80, step 5096, loss: 0.000000\n","epoch 80, step 5097, loss: 0.000000\n","epoch 80, step 5098, loss: 97.178574\n","epoch 80, step 5099, loss: 0.000000\n","epoch 80, step 5100, loss: 0.000002\n","epoch 80, step 5101, loss: 0.000000\n","epoch 80, step 5102, loss: 83.322113\n","epoch 81, step 5103, loss: 0.000000\n","epoch 81, step 5104, loss: 0.000000\n","epoch 81, step 5105, loss: 0.000000\n","epoch 81, step 5106, loss: 212.058563\n","epoch 81, step 5107, loss: 0.001260\n","epoch 81, step 5108, loss: 0.001273\n","epoch 81, step 5109, loss: 0.000002\n","epoch 81, step 5110, loss: 0.000000\n","epoch 81, step 5111, loss: 0.000000\n","epoch 81, step 5112, loss: 2.036494\n","epoch 81, step 5113, loss: 3.979217\n","epoch 81, step 5114, loss: 25.205824\n","epoch 81, step 5115, loss: 0.000027\n","epoch 81, step 5116, loss: 7.998960\n","epoch 81, step 5117, loss: 0.005120\n","epoch 81, step 5118, loss: 8.320527\n","epoch 81, step 5119, loss: 0.000000\n","epoch 81, step 5120, loss: 0.000000\n","epoch 81, step 5121, loss: 0.000000\n","epoch 81, step 5122, loss: 47.456955\n","epoch 81, step 5123, loss: 0.000001\n","epoch 81, step 5124, loss: 0.000000\n","epoch 81, step 5125, loss: 0.000032\n","epoch 81, step 5126, loss: 0.000000\n","epoch 81, step 5127, loss: 1.159835\n","epoch 81, step 5128, loss: 58.153397\n","epoch 81, step 5129, loss: 0.000005\n","epoch 81, step 5130, loss: 0.005417\n","epoch 81, step 5131, loss: 0.001281\n","epoch 81, step 5132, loss: 0.000000\n","epoch 81, step 5133, loss: 10.985853\n","epoch 81, step 5134, loss: 25.071823\n","epoch 81, step 5135, loss: 0.000000\n","epoch 81, step 5136, loss: 0.000000\n","epoch 81, step 5137, loss: 0.000000\n","epoch 81, step 5138, loss: 0.000000\n","epoch 81, step 5139, loss: 0.000000\n","epoch 81, step 5140, loss: 0.000000\n","epoch 81, step 5141, loss: 0.000000\n","epoch 81, step 5142, loss: 0.000000\n","epoch 81, step 5143, loss: 0.000000\n","epoch 81, step 5144, loss: 15.520225\n","epoch 81, step 5145, loss: 0.000000\n","epoch 81, step 5146, loss: 0.000000\n","epoch 81, step 5147, loss: 0.000000\n","epoch 81, step 5148, loss: 0.000000\n","epoch 81, step 5149, loss: 0.001059\n","epoch 81, step 5150, loss: 0.000000\n","epoch 81, step 5151, loss: 0.000000\n","epoch 81, step 5152, loss: 0.000000\n","epoch 81, step 5153, loss: 0.000000\n","epoch 81, step 5154, loss: 0.067266\n","epoch 81, step 5155, loss: 0.000420\n","epoch 81, step 5156, loss: 0.000000\n","epoch 81, step 5157, loss: 0.000001\n","epoch 81, step 5158, loss: 0.000000\n","epoch 81, step 5159, loss: 0.000000\n","epoch 81, step 5160, loss: 7.221667\n","epoch 81, step 5161, loss: 60.085873\n","epoch 81, step 5162, loss: 0.000000\n","epoch 81, step 5163, loss: 0.000000\n","epoch 81, step 5164, loss: 0.000000\n","epoch 81, step 5165, loss: 67.580635\n","epoch 82, step 5166, loss: 0.000000\n","epoch 82, step 5167, loss: 0.000000\n","epoch 82, step 5168, loss: 0.000005\n","epoch 82, step 5169, loss: 42.721714\n","epoch 82, step 5170, loss: 19.439545\n","epoch 82, step 5171, loss: 42.826481\n","epoch 82, step 5172, loss: 0.000000\n","epoch 82, step 5173, loss: 0.000000\n","epoch 82, step 5174, loss: 0.000132\n","epoch 82, step 5175, loss: 0.000000\n","epoch 82, step 5176, loss: 20.720993\n","epoch 82, step 5177, loss: 32.476921\n","epoch 82, step 5178, loss: 22.495815\n","epoch 82, step 5179, loss: 0.000001\n","epoch 82, step 5180, loss: 0.579453\n","epoch 82, step 5181, loss: 11.583040\n","epoch 82, step 5182, loss: 15.988553\n","epoch 82, step 5183, loss: 0.001032\n","epoch 82, step 5184, loss: 0.000016\n","epoch 82, step 5185, loss: 37.169731\n","epoch 82, step 5186, loss: 5.444237\n","epoch 82, step 5187, loss: 0.000000\n","epoch 82, step 5188, loss: 0.368665\n","epoch 82, step 5189, loss: 0.114924\n","epoch 82, step 5190, loss: 0.000124\n","epoch 82, step 5191, loss: 105.671272\n","epoch 82, step 5192, loss: 0.000000\n","epoch 82, step 5193, loss: 0.000901\n","epoch 82, step 5194, loss: 0.000205\n","epoch 82, step 5195, loss: 0.000000\n","epoch 82, step 5196, loss: 0.000000\n","epoch 82, step 5197, loss: 0.000000\n","epoch 82, step 5198, loss: 0.000000\n","epoch 82, step 5199, loss: 1.658445\n","epoch 82, step 5200, loss: 0.000000\n","epoch 82, step 5201, loss: 0.000000\n","epoch 82, step 5202, loss: 0.000056\n","epoch 82, step 5203, loss: 0.000000\n","epoch 82, step 5204, loss: 0.000000\n","epoch 82, step 5205, loss: 0.000000\n","epoch 82, step 5206, loss: 0.000000\n","epoch 82, step 5207, loss: 28.198784\n","epoch 82, step 5208, loss: 0.000000\n","epoch 82, step 5209, loss: 0.000000\n","epoch 82, step 5210, loss: 0.000000\n","epoch 82, step 5211, loss: 0.000000\n","epoch 82, step 5212, loss: 0.000000\n","epoch 82, step 5213, loss: 0.000000\n","epoch 82, step 5214, loss: 0.000000\n","epoch 82, step 5215, loss: 0.000000\n","epoch 82, step 5216, loss: 0.000000\n","epoch 82, step 5217, loss: 0.000000\n","epoch 82, step 5218, loss: 0.000004\n","epoch 82, step 5219, loss: 0.000000\n","epoch 82, step 5220, loss: 0.000000\n","epoch 82, step 5221, loss: 0.000000\n","epoch 82, step 5222, loss: 0.000000\n","epoch 82, step 5223, loss: 0.000001\n","epoch 82, step 5224, loss: 33.927502\n","epoch 82, step 5225, loss: 0.000000\n","epoch 82, step 5226, loss: 3.155513\n","epoch 82, step 5227, loss: 0.000000\n","epoch 82, step 5228, loss: 68.576988\n","epoch 83, step 5229, loss: 86.659416\n","epoch 83, step 5230, loss: 0.000000\n","epoch 83, step 5231, loss: 0.000000\n","epoch 83, step 5232, loss: 51.623672\n","epoch 83, step 5233, loss: 0.001468\n","epoch 83, step 5234, loss: 0.000030\n","epoch 83, step 5235, loss: 0.000000\n","epoch 83, step 5236, loss: 0.000000\n","epoch 83, step 5237, loss: 0.006141\n","epoch 83, step 5238, loss: 11.493029\n","epoch 83, step 5239, loss: 0.001599\n","epoch 83, step 5240, loss: 35.211742\n","epoch 83, step 5241, loss: 0.000000\n","epoch 83, step 5242, loss: 0.013917\n","epoch 83, step 5243, loss: 0.000000\n","epoch 83, step 5244, loss: 0.114064\n","epoch 83, step 5245, loss: 0.000000\n","epoch 83, step 5246, loss: 0.000000\n","epoch 83, step 5247, loss: 0.000003\n","epoch 83, step 5248, loss: 45.315472\n","epoch 83, step 5249, loss: 12.658318\n","epoch 83, step 5250, loss: 0.000000\n","epoch 83, step 5251, loss: 0.000000\n","epoch 83, step 5252, loss: 0.000000\n","epoch 83, step 5253, loss: 0.000001\n","epoch 83, step 5254, loss: 100.221375\n","epoch 83, step 5255, loss: 0.000000\n","epoch 83, step 5256, loss: 1.809192\n","epoch 83, step 5257, loss: 0.417826\n","epoch 83, step 5258, loss: 0.000000\n","epoch 83, step 5259, loss: 0.000000\n","epoch 83, step 5260, loss: 0.000000\n","epoch 83, step 5261, loss: 0.000000\n","epoch 83, step 5262, loss: 0.000000\n","epoch 83, step 5263, loss: 0.000132\n","epoch 83, step 5264, loss: 9.407104\n","epoch 83, step 5265, loss: 6.341493\n","epoch 83, step 5266, loss: 0.001114\n","epoch 83, step 5267, loss: 0.000000\n","epoch 83, step 5268, loss: 0.000000\n","epoch 83, step 5269, loss: 0.000039\n","epoch 83, step 5270, loss: 26.438721\n","epoch 83, step 5271, loss: 0.000046\n","epoch 83, step 5272, loss: 0.000000\n","epoch 83, step 5273, loss: 0.000000\n","epoch 83, step 5274, loss: 0.000000\n","epoch 83, step 5275, loss: 0.000006\n","epoch 83, step 5276, loss: 0.000000\n","epoch 83, step 5277, loss: 0.000000\n","epoch 83, step 5278, loss: 0.000000\n","epoch 83, step 5279, loss: 0.000000\n","epoch 83, step 5280, loss: 0.000000\n","epoch 83, step 5281, loss: 0.000000\n","epoch 83, step 5282, loss: 0.000033\n","epoch 83, step 5283, loss: 0.000000\n","epoch 83, step 5284, loss: 0.000000\n","epoch 83, step 5285, loss: 0.000000\n","epoch 83, step 5286, loss: 0.000000\n","epoch 83, step 5287, loss: 48.352261\n","epoch 83, step 5288, loss: 0.000000\n","epoch 83, step 5289, loss: 0.000000\n","epoch 83, step 5290, loss: 0.000000\n","epoch 83, step 5291, loss: 0.000000\n","epoch 84, step 5292, loss: 0.000000\n","epoch 84, step 5293, loss: 0.000000\n","epoch 84, step 5294, loss: 0.000000\n","epoch 84, step 5295, loss: 207.004898\n","epoch 84, step 5296, loss: 0.000038\n","epoch 84, step 5297, loss: 44.507565\n","epoch 84, step 5298, loss: 0.000000\n","epoch 84, step 5299, loss: 0.000000\n","epoch 84, step 5300, loss: 0.000000\n","epoch 84, step 5301, loss: 20.308144\n","epoch 84, step 5302, loss: 41.926010\n","epoch 84, step 5303, loss: 34.372185\n","epoch 84, step 5304, loss: 0.000000\n","epoch 84, step 5305, loss: 0.000000\n","epoch 84, step 5306, loss: 0.000000\n","epoch 84, step 5307, loss: 0.000328\n","epoch 84, step 5308, loss: 0.000000\n","epoch 84, step 5309, loss: 0.000000\n","epoch 84, step 5310, loss: 0.000000\n","epoch 84, step 5311, loss: 38.814106\n","epoch 84, step 5312, loss: 0.000000\n","epoch 84, step 5313, loss: 0.000000\n","epoch 84, step 5314, loss: 0.000000\n","epoch 84, step 5315, loss: 0.000000\n","epoch 84, step 5316, loss: 0.000016\n","epoch 84, step 5317, loss: 85.354240\n","epoch 84, step 5318, loss: 0.000000\n","epoch 84, step 5319, loss: 0.000420\n","epoch 84, step 5320, loss: 0.000250\n","epoch 84, step 5321, loss: 0.000000\n","epoch 84, step 5322, loss: 0.000000\n","epoch 84, step 5323, loss: 0.000000\n","epoch 84, step 5324, loss: 0.000000\n","epoch 84, step 5325, loss: 0.000000\n","epoch 84, step 5326, loss: 0.000000\n","epoch 84, step 5327, loss: 0.000000\n","epoch 84, step 5328, loss: 0.000000\n","epoch 84, step 5329, loss: 3.839788\n","epoch 84, step 5330, loss: 0.000000\n","epoch 84, step 5331, loss: 0.000002\n","epoch 84, step 5332, loss: 12.607824\n","epoch 84, step 5333, loss: 14.614322\n","epoch 84, step 5334, loss: 0.000000\n","epoch 84, step 5335, loss: 0.000000\n","epoch 84, step 5336, loss: 0.000000\n","epoch 84, step 5337, loss: 0.000000\n","epoch 84, step 5338, loss: 0.000000\n","epoch 84, step 5339, loss: 0.000002\n","epoch 84, step 5340, loss: 0.000000\n","epoch 84, step 5341, loss: 0.000000\n","epoch 84, step 5342, loss: 0.000000\n","epoch 84, step 5343, loss: 0.010396\n","epoch 84, step 5344, loss: 0.000548\n","epoch 84, step 5345, loss: 0.000000\n","epoch 84, step 5346, loss: 0.000000\n","epoch 84, step 5347, loss: 0.000000\n","epoch 84, step 5348, loss: 0.000000\n","epoch 84, step 5349, loss: 0.000002\n","epoch 84, step 5350, loss: 53.508755\n","epoch 84, step 5351, loss: 0.000000\n","epoch 84, step 5352, loss: 0.000000\n","epoch 84, step 5353, loss: 0.000000\n","epoch 84, step 5354, loss: 69.500298\n","epoch 85, step 5355, loss: 0.000000\n","epoch 85, step 5356, loss: 0.000000\n","epoch 85, step 5357, loss: 0.000000\n","epoch 85, step 5358, loss: 78.565125\n","epoch 85, step 5359, loss: 0.016489\n","epoch 85, step 5360, loss: 0.000002\n","epoch 85, step 5361, loss: 0.000000\n","epoch 85, step 5362, loss: 0.000000\n","epoch 85, step 5363, loss: 0.000000\n","epoch 85, step 5364, loss: 27.666119\n","epoch 85, step 5365, loss: 15.884055\n","epoch 85, step 5366, loss: 36.812553\n","epoch 85, step 5367, loss: 0.000000\n","epoch 85, step 5368, loss: 0.000000\n","epoch 85, step 5369, loss: 0.000000\n","epoch 85, step 5370, loss: 10.655998\n","epoch 85, step 5371, loss: 0.000000\n","epoch 85, step 5372, loss: 0.000000\n","epoch 85, step 5373, loss: 0.000106\n","epoch 85, step 5374, loss: 33.812420\n","epoch 85, step 5375, loss: 0.000000\n","epoch 85, step 5376, loss: 0.000000\n","epoch 85, step 5377, loss: 0.000000\n","epoch 85, step 5378, loss: 0.000000\n","epoch 85, step 5379, loss: 0.000000\n","epoch 85, step 5380, loss: 91.756012\n","epoch 85, step 5381, loss: 0.000000\n","epoch 85, step 5382, loss: 0.000694\n","epoch 85, step 5383, loss: 0.000176\n","epoch 85, step 5384, loss: 0.000000\n","epoch 85, step 5385, loss: 0.000000\n","epoch 85, step 5386, loss: 0.000000\n","epoch 85, step 5387, loss: 0.000000\n","epoch 85, step 5388, loss: 0.000000\n","epoch 85, step 5389, loss: 0.000000\n","epoch 85, step 5390, loss: 0.000000\n","epoch 85, step 5391, loss: 0.000000\n","epoch 85, step 5392, loss: 0.000000\n","epoch 85, step 5393, loss: 0.000000\n","epoch 85, step 5394, loss: 0.000000\n","epoch 85, step 5395, loss: 0.000000\n","epoch 85, step 5396, loss: 17.885616\n","epoch 85, step 5397, loss: 0.000000\n","epoch 85, step 5398, loss: 0.000000\n","epoch 85, step 5399, loss: 0.000000\n","epoch 85, step 5400, loss: 0.000000\n","epoch 85, step 5401, loss: 0.000000\n","epoch 85, step 5402, loss: 17.136732\n","epoch 85, step 5403, loss: 0.000000\n","epoch 85, step 5404, loss: 0.000001\n","epoch 85, step 5405, loss: 0.000000\n","epoch 85, step 5406, loss: 0.001356\n","epoch 85, step 5407, loss: 0.000069\n","epoch 85, step 5408, loss: 0.000000\n","epoch 85, step 5409, loss: 0.000000\n","epoch 85, step 5410, loss: 0.000000\n","epoch 85, step 5411, loss: 0.000000\n","epoch 85, step 5412, loss: 0.282839\n","epoch 85, step 5413, loss: 48.767250\n","epoch 85, step 5414, loss: 0.000000\n","epoch 85, step 5415, loss: 0.000000\n","epoch 85, step 5416, loss: 0.000000\n","epoch 85, step 5417, loss: 63.689186\n","epoch 86, step 5418, loss: 70.697563\n","epoch 86, step 5419, loss: 0.000000\n","epoch 86, step 5420, loss: 0.000000\n","epoch 86, step 5421, loss: 28.781956\n","epoch 86, step 5422, loss: 0.000083\n","epoch 86, step 5423, loss: 0.025398\n","epoch 86, step 5424, loss: 0.000000\n","epoch 86, step 5425, loss: 0.000000\n","epoch 86, step 5426, loss: 52.076134\n","epoch 86, step 5427, loss: 0.000005\n","epoch 86, step 5428, loss: 0.000246\n","epoch 86, step 5429, loss: 12.542388\n","epoch 86, step 5430, loss: 0.000000\n","epoch 86, step 5431, loss: 0.000000\n","epoch 86, step 5432, loss: 0.123078\n","epoch 86, step 5433, loss: 5.959963\n","epoch 86, step 5434, loss: 0.000000\n","epoch 86, step 5435, loss: 0.000000\n","epoch 86, step 5436, loss: 0.000000\n","epoch 86, step 5437, loss: 9.440443\n","epoch 86, step 5438, loss: 0.000000\n","epoch 86, step 5439, loss: 0.000000\n","epoch 86, step 5440, loss: 0.000001\n","epoch 86, step 5441, loss: 0.000000\n","epoch 86, step 5442, loss: 0.000001\n","epoch 86, step 5443, loss: 53.076679\n","epoch 86, step 5444, loss: 0.000000\n","epoch 86, step 5445, loss: 0.003687\n","epoch 86, step 5446, loss: 0.001265\n","epoch 86, step 5447, loss: 0.000000\n","epoch 86, step 5448, loss: 0.000000\n","epoch 86, step 5449, loss: 0.000000\n","epoch 86, step 5450, loss: 0.000000\n","epoch 86, step 5451, loss: 0.000000\n","epoch 86, step 5452, loss: 0.000000\n","epoch 86, step 5453, loss: 0.000000\n","epoch 86, step 5454, loss: 0.000000\n","epoch 86, step 5455, loss: 0.000000\n","epoch 86, step 5456, loss: 0.000000\n","epoch 86, step 5457, loss: 0.000002\n","epoch 86, step 5458, loss: 0.000000\n","epoch 86, step 5459, loss: 13.898132\n","epoch 86, step 5460, loss: 2.596559\n","epoch 86, step 5461, loss: 0.000000\n","epoch 86, step 5462, loss: 0.000000\n","epoch 86, step 5463, loss: 0.000000\n","epoch 86, step 5464, loss: 0.000000\n","epoch 86, step 5465, loss: 0.000000\n","epoch 86, step 5466, loss: 0.000000\n","epoch 86, step 5467, loss: 0.000000\n","epoch 86, step 5468, loss: 0.000000\n","epoch 86, step 5469, loss: 0.124747\n","epoch 86, step 5470, loss: 0.000005\n","epoch 86, step 5471, loss: 0.000000\n","epoch 86, step 5472, loss: 0.000000\n","epoch 86, step 5473, loss: 0.000000\n","epoch 86, step 5474, loss: 0.000000\n","epoch 86, step 5475, loss: 45.242512\n","epoch 86, step 5476, loss: 70.378197\n","epoch 86, step 5477, loss: 0.000000\n","epoch 86, step 5478, loss: 0.000000\n","epoch 86, step 5479, loss: 0.000000\n","epoch 86, step 5480, loss: 0.000431\n","epoch 87, step 5481, loss: 0.000000\n","epoch 87, step 5482, loss: 0.000000\n","epoch 87, step 5483, loss: 0.000000\n","epoch 87, step 5484, loss: 241.308655\n","epoch 87, step 5485, loss: 0.014501\n","epoch 87, step 5486, loss: 0.005083\n","epoch 87, step 5487, loss: 0.000000\n","epoch 87, step 5488, loss: 0.000000\n","epoch 87, step 5489, loss: 0.000000\n","epoch 87, step 5490, loss: 0.000017\n","epoch 87, step 5491, loss: 30.817131\n","epoch 87, step 5492, loss: 39.138878\n","epoch 87, step 5493, loss: 0.000000\n","epoch 87, step 5494, loss: 0.000000\n","epoch 87, step 5495, loss: 0.000283\n","epoch 87, step 5496, loss: 5.445445\n","epoch 87, step 5497, loss: 0.000000\n","epoch 87, step 5498, loss: 0.000000\n","epoch 87, step 5499, loss: 0.000000\n","epoch 87, step 5500, loss: 22.829836\n","epoch 87, step 5501, loss: 0.000000\n","epoch 87, step 5502, loss: 0.000001\n","epoch 87, step 5503, loss: 0.000000\n","epoch 87, step 5504, loss: 0.000000\n","epoch 87, step 5505, loss: 0.000000\n","epoch 87, step 5506, loss: 79.465080\n","epoch 87, step 5507, loss: 0.000000\n","epoch 87, step 5508, loss: 3.246457\n","epoch 87, step 5509, loss: 0.000135\n","epoch 87, step 5510, loss: 0.000000\n","epoch 87, step 5511, loss: 0.000000\n","epoch 87, step 5512, loss: 0.000000\n","epoch 87, step 5513, loss: 0.000000\n","epoch 87, step 5514, loss: 0.000000\n","epoch 87, step 5515, loss: 0.000000\n","epoch 87, step 5516, loss: 0.000000\n","epoch 87, step 5517, loss: 0.000000\n","epoch 87, step 5518, loss: 0.000000\n","epoch 87, step 5519, loss: 0.000000\n","epoch 87, step 5520, loss: 0.000000\n","epoch 87, step 5521, loss: 0.000000\n","epoch 87, step 5522, loss: 20.735775\n","epoch 87, step 5523, loss: 0.000000\n","epoch 87, step 5524, loss: 0.000000\n","epoch 87, step 5525, loss: 0.000000\n","epoch 87, step 5526, loss: 0.000000\n","epoch 87, step 5527, loss: 0.000000\n","epoch 87, step 5528, loss: 0.000000\n","epoch 87, step 5529, loss: 0.000000\n","epoch 87, step 5530, loss: 0.000000\n","epoch 87, step 5531, loss: 0.000000\n","epoch 87, step 5532, loss: 0.000711\n","epoch 87, step 5533, loss: 0.000000\n","epoch 87, step 5534, loss: 0.000000\n","epoch 87, step 5535, loss: 0.000000\n","epoch 87, step 5536, loss: 0.000000\n","epoch 87, step 5537, loss: 0.000000\n","epoch 87, step 5538, loss: 16.208958\n","epoch 87, step 5539, loss: 65.449486\n","epoch 87, step 5540, loss: 0.000000\n","epoch 87, step 5541, loss: 0.000000\n","epoch 87, step 5542, loss: 0.000000\n","epoch 87, step 5543, loss: 74.574936\n","epoch 88, step 5544, loss: 0.000000\n","epoch 88, step 5545, loss: 0.000000\n","epoch 88, step 5546, loss: 0.000000\n","epoch 88, step 5547, loss: 70.907875\n","epoch 88, step 5548, loss: 0.000014\n","epoch 88, step 5549, loss: 0.000003\n","epoch 88, step 5550, loss: 0.000000\n","epoch 88, step 5551, loss: 0.000000\n","epoch 88, step 5552, loss: 0.000000\n","epoch 88, step 5553, loss: 0.000000\n","epoch 88, step 5554, loss: 10.082529\n","epoch 88, step 5555, loss: 20.620308\n","epoch 88, step 5556, loss: 0.000000\n","epoch 88, step 5557, loss: 0.000000\n","epoch 88, step 5558, loss: 0.000000\n","epoch 88, step 5559, loss: 5.256611\n","epoch 88, step 5560, loss: 0.000000\n","epoch 88, step 5561, loss: 0.000000\n","epoch 88, step 5562, loss: 0.000000\n","epoch 88, step 5563, loss: 42.976357\n","epoch 88, step 5564, loss: 0.000000\n","epoch 88, step 5565, loss: 0.000001\n","epoch 88, step 5566, loss: 0.000000\n","epoch 88, step 5567, loss: 0.000000\n","epoch 88, step 5568, loss: 0.000000\n","epoch 88, step 5569, loss: 67.009239\n","epoch 88, step 5570, loss: 0.000000\n","epoch 88, step 5571, loss: 0.000031\n","epoch 88, step 5572, loss: 0.014011\n","epoch 88, step 5573, loss: 0.000000\n","epoch 88, step 5574, loss: 0.000213\n","epoch 88, step 5575, loss: 0.000000\n","epoch 88, step 5576, loss: 0.000068\n","epoch 88, step 5577, loss: 0.000014\n","epoch 88, step 5578, loss: 0.000000\n","epoch 88, step 5579, loss: 0.000000\n","epoch 88, step 5580, loss: 0.000000\n","epoch 88, step 5581, loss: 0.000000\n","epoch 88, step 5582, loss: 0.000000\n","epoch 88, step 5583, loss: 0.000000\n","epoch 88, step 5584, loss: 0.000000\n","epoch 88, step 5585, loss: 19.970482\n","epoch 88, step 5586, loss: 0.000000\n","epoch 88, step 5587, loss: 0.000000\n","epoch 88, step 5588, loss: 0.000000\n","epoch 88, step 5589, loss: 0.000000\n","epoch 88, step 5590, loss: 0.001409\n","epoch 88, step 5591, loss: 0.000000\n","epoch 88, step 5592, loss: 0.000000\n","epoch 88, step 5593, loss: 0.000000\n","epoch 88, step 5594, loss: 0.000000\n","epoch 88, step 5595, loss: 0.003510\n","epoch 88, step 5596, loss: 0.000000\n","epoch 88, step 5597, loss: 0.000000\n","epoch 88, step 5598, loss: 0.000000\n","epoch 88, step 5599, loss: 0.000000\n","epoch 88, step 5600, loss: 10.921886\n","epoch 88, step 5601, loss: 21.373611\n","epoch 88, step 5602, loss: 50.316769\n","epoch 88, step 5603, loss: 0.000000\n","epoch 88, step 5604, loss: 0.000000\n","epoch 88, step 5605, loss: 0.000000\n","epoch 88, step 5606, loss: 69.681488\n","epoch 89, step 5607, loss: 83.218613\n","epoch 89, step 5608, loss: 0.000000\n","epoch 89, step 5609, loss: 0.000000\n","epoch 89, step 5610, loss: 35.071980\n","epoch 89, step 5611, loss: 0.303627\n","epoch 89, step 5612, loss: 0.013052\n","epoch 89, step 5613, loss: 0.000000\n","epoch 89, step 5614, loss: 0.000000\n","epoch 89, step 5615, loss: 0.000000\n","epoch 89, step 5616, loss: 0.000285\n","epoch 89, step 5617, loss: 0.066131\n","epoch 89, step 5618, loss: 47.436493\n","epoch 89, step 5619, loss: 0.000000\n","epoch 89, step 5620, loss: 0.021712\n","epoch 89, step 5621, loss: 6.510777\n","epoch 89, step 5622, loss: 0.004920\n","epoch 89, step 5623, loss: 0.000000\n","epoch 89, step 5624, loss: 0.000000\n","epoch 89, step 5625, loss: 0.000209\n","epoch 89, step 5626, loss: 51.239258\n","epoch 89, step 5627, loss: 0.000000\n","epoch 89, step 5628, loss: 0.000000\n","epoch 89, step 5629, loss: 0.000000\n","epoch 89, step 5630, loss: 0.000000\n","epoch 89, step 5631, loss: 0.003622\n","epoch 89, step 5632, loss: 18.381721\n","epoch 89, step 5633, loss: 0.000000\n","epoch 89, step 5634, loss: 1.061492\n","epoch 89, step 5635, loss: 0.223192\n","epoch 89, step 5636, loss: 0.000000\n","epoch 89, step 5637, loss: 0.000000\n","epoch 89, step 5638, loss: 0.000000\n","epoch 89, step 5639, loss: 0.000000\n","epoch 89, step 5640, loss: 0.000000\n","epoch 89, step 5641, loss: 0.000000\n","epoch 89, step 5642, loss: 0.000000\n","epoch 89, step 5643, loss: 0.000000\n","epoch 89, step 5644, loss: 0.000536\n","epoch 89, step 5645, loss: 0.000000\n","epoch 89, step 5646, loss: 0.000000\n","epoch 89, step 5647, loss: 0.000000\n","epoch 89, step 5648, loss: 18.760069\n","epoch 89, step 5649, loss: 0.000000\n","epoch 89, step 5650, loss: 0.000000\n","epoch 89, step 5651, loss: 0.000000\n","epoch 89, step 5652, loss: 0.000000\n","epoch 89, step 5653, loss: 0.000000\n","epoch 89, step 5654, loss: 0.000000\n","epoch 89, step 5655, loss: 0.000000\n","epoch 89, step 5656, loss: 0.000000\n","epoch 89, step 5657, loss: 0.000000\n","epoch 89, step 5658, loss: 0.002087\n","epoch 89, step 5659, loss: 0.000000\n","epoch 89, step 5660, loss: 0.000000\n","epoch 89, step 5661, loss: 0.000000\n","epoch 89, step 5662, loss: 0.000000\n","epoch 89, step 5663, loss: 0.000000\n","epoch 89, step 5664, loss: 0.086891\n","epoch 89, step 5665, loss: 101.328415\n","epoch 89, step 5666, loss: 0.000000\n","epoch 89, step 5667, loss: 0.000000\n","epoch 89, step 5668, loss: 0.000004\n","epoch 89, step 5669, loss: 6.705346\n","epoch 90, step 5670, loss: 0.000000\n","epoch 90, step 5671, loss: 0.000000\n","epoch 90, step 5672, loss: 0.000000\n","epoch 90, step 5673, loss: 258.164093\n","epoch 90, step 5674, loss: 0.000000\n","epoch 90, step 5675, loss: 0.000000\n","epoch 90, step 5676, loss: 0.000000\n","epoch 90, step 5677, loss: 0.000000\n","epoch 90, step 5678, loss: 0.000000\n","epoch 90, step 5679, loss: 35.302036\n","epoch 90, step 5680, loss: 83.421074\n","epoch 90, step 5681, loss: 20.051550\n","epoch 90, step 5682, loss: 0.000000\n","epoch 90, step 5683, loss: 0.000001\n","epoch 90, step 5684, loss: 0.000000\n","epoch 90, step 5685, loss: 9.305411\n","epoch 90, step 5686, loss: 0.000000\n","epoch 90, step 5687, loss: 0.000000\n","epoch 90, step 5688, loss: 0.000000\n","epoch 90, step 5689, loss: 38.101650\n","epoch 90, step 5690, loss: 0.000000\n","epoch 90, step 5691, loss: 0.000004\n","epoch 90, step 5692, loss: 0.000000\n","epoch 90, step 5693, loss: 0.000000\n","epoch 90, step 5694, loss: 0.000000\n","epoch 90, step 5695, loss: 35.847054\n","epoch 90, step 5696, loss: 0.000000\n","epoch 90, step 5697, loss: 0.000087\n","epoch 90, step 5698, loss: 0.000022\n","epoch 90, step 5699, loss: 0.000000\n","epoch 90, step 5700, loss: 0.000000\n","epoch 90, step 5701, loss: 0.000000\n","epoch 90, step 5702, loss: 0.000000\n","epoch 90, step 5703, loss: 0.000000\n","epoch 90, step 5704, loss: 0.000000\n","epoch 90, step 5705, loss: 0.000000\n","epoch 90, step 5706, loss: 0.000000\n","epoch 90, step 5707, loss: 0.000000\n","epoch 90, step 5708, loss: 0.000000\n","epoch 90, step 5709, loss: 0.000000\n","epoch 90, step 5710, loss: 0.000000\n","epoch 90, step 5711, loss: 19.928301\n","epoch 90, step 5712, loss: 0.000000\n","epoch 90, step 5713, loss: 0.000000\n","epoch 90, step 5714, loss: 0.000000\n","epoch 90, step 5715, loss: 0.000000\n","epoch 90, step 5716, loss: 0.000001\n","epoch 90, step 5717, loss: 0.000000\n","epoch 90, step 5718, loss: 0.000000\n","epoch 90, step 5719, loss: 0.000000\n","epoch 90, step 5720, loss: 0.000000\n","epoch 90, step 5721, loss: 0.004672\n","epoch 90, step 5722, loss: 0.000000\n","epoch 90, step 5723, loss: 0.000000\n","epoch 90, step 5724, loss: 0.000000\n","epoch 90, step 5725, loss: 0.000000\n","epoch 90, step 5726, loss: 0.000000\n","epoch 90, step 5727, loss: 2.203933\n","epoch 90, step 5728, loss: 82.069115\n","epoch 90, step 5729, loss: 0.000000\n","epoch 90, step 5730, loss: 0.000000\n","epoch 90, step 5731, loss: 0.000000\n","epoch 90, step 5732, loss: 0.039036\n","epoch 91, step 5733, loss: 0.000000\n","epoch 91, step 5734, loss: 0.000009\n","epoch 91, step 5735, loss: 0.000000\n","epoch 91, step 5736, loss: 94.323509\n","epoch 91, step 5737, loss: 0.000000\n","epoch 91, step 5738, loss: 0.000001\n","epoch 91, step 5739, loss: 0.000000\n","epoch 91, step 5740, loss: 0.000000\n","epoch 91, step 5741, loss: 0.000000\n","epoch 91, step 5742, loss: 0.000000\n","epoch 91, step 5743, loss: 23.028473\n","epoch 91, step 5744, loss: 44.921646\n","epoch 91, step 5745, loss: 0.000000\n","epoch 91, step 5746, loss: 0.000000\n","epoch 91, step 5747, loss: 0.000000\n","epoch 91, step 5748, loss: 6.787450\n","epoch 91, step 5749, loss: 0.000000\n","epoch 91, step 5750, loss: 0.000000\n","epoch 91, step 5751, loss: 0.000000\n","epoch 91, step 5752, loss: 50.017330\n","epoch 91, step 5753, loss: 0.000000\n","epoch 91, step 5754, loss: 0.000001\n","epoch 91, step 5755, loss: 0.000000\n","epoch 91, step 5756, loss: 0.000000\n","epoch 91, step 5757, loss: 0.000000\n","epoch 91, step 5758, loss: 51.287071\n","epoch 91, step 5759, loss: 0.000000\n","epoch 91, step 5760, loss: 0.000193\n","epoch 91, step 5761, loss: 0.000058\n","epoch 91, step 5762, loss: 0.000000\n","epoch 91, step 5763, loss: 0.000000\n","epoch 91, step 5764, loss: 0.000000\n","epoch 91, step 5765, loss: 0.000000\n","epoch 91, step 5766, loss: 0.000000\n","epoch 91, step 5767, loss: 0.000000\n","epoch 91, step 5768, loss: 0.000000\n","epoch 91, step 5769, loss: 0.000000\n","epoch 91, step 5770, loss: 0.000000\n","epoch 91, step 5771, loss: 0.000000\n","epoch 91, step 5772, loss: 0.000000\n","epoch 91, step 5773, loss: 0.000000\n","epoch 91, step 5774, loss: 17.864431\n","epoch 91, step 5775, loss: 0.000000\n","epoch 91, step 5776, loss: 0.000000\n","epoch 91, step 5777, loss: 0.000000\n","epoch 91, step 5778, loss: 0.000000\n","epoch 91, step 5779, loss: 0.299848\n","epoch 91, step 5780, loss: 0.000000\n","epoch 91, step 5781, loss: 0.000000\n","epoch 91, step 5782, loss: 0.000000\n","epoch 91, step 5783, loss: 0.000000\n","epoch 91, step 5784, loss: 0.029667\n","epoch 91, step 5785, loss: 0.000000\n","epoch 91, step 5786, loss: 0.000000\n","epoch 91, step 5787, loss: 0.000000\n","epoch 91, step 5788, loss: 0.000000\n","epoch 91, step 5789, loss: 0.000000\n","epoch 91, step 5790, loss: 21.930670\n","epoch 91, step 5791, loss: 65.652016\n","epoch 91, step 5792, loss: 0.000000\n","epoch 91, step 5793, loss: 0.000000\n","epoch 91, step 5794, loss: 0.000000\n","epoch 91, step 5795, loss: 68.797890\n","epoch 92, step 5796, loss: 74.063164\n","epoch 92, step 5797, loss: 0.000000\n","epoch 92, step 5798, loss: 0.000000\n","epoch 92, step 5799, loss: 29.260620\n","epoch 92, step 5800, loss: 0.000000\n","epoch 92, step 5801, loss: 0.000002\n","epoch 92, step 5802, loss: 0.000000\n","epoch 92, step 5803, loss: 0.000000\n","epoch 92, step 5804, loss: 0.000000\n","epoch 92, step 5805, loss: 0.000018\n","epoch 92, step 5806, loss: 0.000000\n","epoch 92, step 5807, loss: 11.514311\n","epoch 92, step 5808, loss: 0.000000\n","epoch 92, step 5809, loss: 0.000022\n","epoch 92, step 5810, loss: 0.000007\n","epoch 92, step 5811, loss: 1.516794\n","epoch 92, step 5812, loss: 0.000000\n","epoch 92, step 5813, loss: 0.000000\n","epoch 92, step 5814, loss: 0.000000\n","epoch 92, step 5815, loss: 77.997025\n","epoch 92, step 5816, loss: 0.000000\n","epoch 92, step 5817, loss: 0.000000\n","epoch 92, step 5818, loss: 0.000017\n","epoch 92, step 5819, loss: 0.000000\n","epoch 92, step 5820, loss: 2.389419\n","epoch 92, step 5821, loss: 4.994249\n","epoch 92, step 5822, loss: 0.000000\n","epoch 92, step 5823, loss: 0.000077\n","epoch 92, step 5824, loss: 0.000043\n","epoch 92, step 5825, loss: 0.000000\n","epoch 92, step 5826, loss: 0.000000\n","epoch 92, step 5827, loss: 0.000000\n","epoch 92, step 5828, loss: 0.000000\n","epoch 92, step 5829, loss: 0.000000\n","epoch 92, step 5830, loss: 0.000000\n","epoch 92, step 5831, loss: 0.000000\n","epoch 92, step 5832, loss: 0.000000\n","epoch 92, step 5833, loss: 0.000007\n","epoch 92, step 5834, loss: 0.000000\n","epoch 92, step 5835, loss: 0.000001\n","epoch 92, step 5836, loss: 0.000000\n","epoch 92, step 5837, loss: 13.985738\n","epoch 92, step 5838, loss: 0.000000\n","epoch 92, step 5839, loss: 0.000000\n","epoch 92, step 5840, loss: 0.000000\n","epoch 92, step 5841, loss: 0.000004\n","epoch 92, step 5842, loss: 0.000000\n","epoch 92, step 5843, loss: 0.000000\n","epoch 92, step 5844, loss: 0.000000\n","epoch 92, step 5845, loss: 0.000000\n","epoch 92, step 5846, loss: 0.000000\n","epoch 92, step 5847, loss: 2.487144\n","epoch 92, step 5848, loss: 0.000000\n","epoch 92, step 5849, loss: 0.000000\n","epoch 92, step 5850, loss: 0.000000\n","epoch 92, step 5851, loss: 0.000000\n","epoch 92, step 5852, loss: 0.000000\n","epoch 92, step 5853, loss: 0.042336\n","epoch 92, step 5854, loss: 93.428207\n","epoch 92, step 5855, loss: 0.000000\n","epoch 92, step 5856, loss: 0.493848\n","epoch 92, step 5857, loss: 0.000000\n","epoch 92, step 5858, loss: 4.290167\n","epoch 93, step 5859, loss: 0.000000\n","epoch 93, step 5860, loss: 0.000000\n","epoch 93, step 5861, loss: 0.000000\n","epoch 93, step 5862, loss: 258.315247\n","epoch 93, step 5863, loss: 0.000000\n","epoch 93, step 5864, loss: 0.000000\n","epoch 93, step 5865, loss: 0.000000\n","epoch 93, step 5866, loss: 0.000000\n","epoch 93, step 5867, loss: 0.000000\n","epoch 93, step 5868, loss: 35.762890\n","epoch 93, step 5869, loss: 85.435432\n","epoch 93, step 5870, loss: 38.859699\n","epoch 93, step 5871, loss: 0.000000\n","epoch 93, step 5872, loss: 0.000001\n","epoch 93, step 5873, loss: 0.000000\n","epoch 93, step 5874, loss: 0.018649\n","epoch 93, step 5875, loss: 0.000000\n","epoch 93, step 5876, loss: 0.000000\n","epoch 93, step 5877, loss: 0.000000\n","epoch 93, step 5878, loss: 28.057930\n","epoch 93, step 5879, loss: 0.000000\n","epoch 93, step 5880, loss: 0.000033\n","epoch 93, step 5881, loss: 0.000000\n","epoch 93, step 5882, loss: 0.000000\n","epoch 93, step 5883, loss: 0.000000\n","epoch 93, step 5884, loss: 58.321365\n","epoch 93, step 5885, loss: 0.000000\n","epoch 93, step 5886, loss: 0.010535\n","epoch 93, step 5887, loss: 0.005199\n","epoch 93, step 5888, loss: 0.000000\n","epoch 93, step 5889, loss: 0.000001\n","epoch 93, step 5890, loss: 0.000000\n","epoch 93, step 5891, loss: 0.000000\n","epoch 93, step 5892, loss: 0.000000\n","epoch 93, step 5893, loss: 0.000000\n","epoch 93, step 5894, loss: 0.024743\n","epoch 93, step 5895, loss: 0.000000\n","epoch 93, step 5896, loss: 0.000000\n","epoch 93, step 5897, loss: 0.000000\n","epoch 93, step 5898, loss: 0.000055\n","epoch 93, step 5899, loss: 0.000000\n","epoch 93, step 5900, loss: 10.227326\n","epoch 93, step 5901, loss: 0.000000\n","epoch 93, step 5902, loss: 0.000000\n","epoch 93, step 5903, loss: 0.000000\n","epoch 93, step 5904, loss: 17.442167\n","epoch 93, step 5905, loss: 0.003108\n","epoch 93, step 5906, loss: 0.000000\n","epoch 93, step 5907, loss: 0.000000\n","epoch 93, step 5908, loss: 0.000000\n","epoch 93, step 5909, loss: 0.000000\n","epoch 93, step 5910, loss: 1.009508\n","epoch 93, step 5911, loss: 0.000000\n","epoch 93, step 5912, loss: 0.000000\n","epoch 93, step 5913, loss: 0.000000\n","epoch 93, step 5914, loss: 0.000000\n","epoch 93, step 5915, loss: 0.000000\n","epoch 93, step 5916, loss: 5.309402\n","epoch 93, step 5917, loss: 75.983566\n","epoch 93, step 5918, loss: 0.000000\n","epoch 93, step 5919, loss: 0.000000\n","epoch 93, step 5920, loss: 0.000000\n","epoch 93, step 5921, loss: 4.687845\n","epoch 94, step 5922, loss: 0.000000\n","epoch 94, step 5923, loss: 0.000000\n","epoch 94, step 5924, loss: 0.000000\n","epoch 94, step 5925, loss: 102.675690\n","epoch 94, step 5926, loss: 0.000000\n","epoch 94, step 5927, loss: 0.000001\n","epoch 94, step 5928, loss: 0.000000\n","epoch 94, step 5929, loss: 0.000000\n","epoch 94, step 5930, loss: 0.000000\n","epoch 94, step 5931, loss: 0.000005\n","epoch 94, step 5932, loss: 80.113907\n","epoch 94, step 5933, loss: 19.541107\n","epoch 94, step 5934, loss: 0.000000\n","epoch 94, step 5935, loss: 0.000000\n","epoch 94, step 5936, loss: 0.000000\n","epoch 94, step 5937, loss: 0.001457\n","epoch 94, step 5938, loss: 0.000000\n","epoch 94, step 5939, loss: 0.000000\n","epoch 94, step 5940, loss: 0.000000\n","epoch 94, step 5941, loss: 54.731995\n","epoch 94, step 5942, loss: 0.000000\n","epoch 94, step 5943, loss: 0.000000\n","epoch 94, step 5944, loss: 0.000000\n","epoch 94, step 5945, loss: 0.000000\n","epoch 94, step 5946, loss: 0.000000\n","epoch 94, step 5947, loss: 66.355209\n","epoch 94, step 5948, loss: 0.000000\n","epoch 94, step 5949, loss: 0.006458\n","epoch 94, step 5950, loss: 0.003448\n","epoch 94, step 5951, loss: 0.000000\n","epoch 94, step 5952, loss: 0.000000\n","epoch 94, step 5953, loss: 0.000000\n","epoch 94, step 5954, loss: 0.000000\n","epoch 94, step 5955, loss: 0.000000\n","epoch 94, step 5956, loss: 0.000000\n","epoch 94, step 5957, loss: 0.000000\n","epoch 94, step 5958, loss: 0.000000\n","epoch 94, step 5959, loss: 0.000000\n","epoch 94, step 5960, loss: 0.000000\n","epoch 94, step 5961, loss: 0.000066\n","epoch 94, step 5962, loss: 0.000000\n","epoch 94, step 5963, loss: 9.966157\n","epoch 94, step 5964, loss: 0.000000\n","epoch 94, step 5965, loss: 0.000000\n","epoch 94, step 5966, loss: 0.000000\n","epoch 94, step 5967, loss: 0.000000\n","epoch 94, step 5968, loss: 0.000017\n","epoch 94, step 5969, loss: 0.000000\n","epoch 94, step 5970, loss: 0.000000\n","epoch 94, step 5971, loss: 0.000000\n","epoch 94, step 5972, loss: 0.000000\n","epoch 94, step 5973, loss: 6.557946\n","epoch 94, step 5974, loss: 0.000000\n","epoch 94, step 5975, loss: 0.000000\n","epoch 94, step 5976, loss: 0.000000\n","epoch 94, step 5977, loss: 0.000000\n","epoch 94, step 5978, loss: 0.000000\n","epoch 94, step 5979, loss: 16.240301\n","epoch 94, step 5980, loss: 53.612335\n","epoch 94, step 5981, loss: 0.000000\n","epoch 94, step 5982, loss: 0.000000\n","epoch 94, step 5983, loss: 0.000000\n","epoch 94, step 5984, loss: 0.000237\n","epoch 95, step 5985, loss: 74.037216\n","epoch 95, step 5986, loss: 0.000000\n","epoch 95, step 5987, loss: 0.000000\n","epoch 95, step 5988, loss: 22.762873\n","epoch 95, step 5989, loss: 0.000077\n","epoch 95, step 5990, loss: 0.021326\n","epoch 95, step 5991, loss: 0.012919\n","epoch 95, step 5992, loss: 0.000000\n","epoch 95, step 5993, loss: 0.000000\n","epoch 95, step 5994, loss: 22.631500\n","epoch 95, step 5995, loss: 0.000007\n","epoch 95, step 5996, loss: 57.410442\n","epoch 95, step 5997, loss: 0.000000\n","epoch 95, step 5998, loss: 24.521053\n","epoch 95, step 5999, loss: 0.000030\n","epoch 95, step 6000, loss: 0.004163\n","epoch 95, step 6001, loss: 0.000000\n","epoch 95, step 6002, loss: 0.000000\n","epoch 95, step 6003, loss: 0.000000\n","epoch 95, step 6004, loss: 58.780582\n","epoch 95, step 6005, loss: 0.000000\n","epoch 95, step 6006, loss: 0.000000\n","epoch 95, step 6007, loss: 0.000000\n","epoch 95, step 6008, loss: 0.000000\n","epoch 95, step 6009, loss: 0.000000\n","epoch 95, step 6010, loss: 51.433170\n","epoch 95, step 6011, loss: 0.000000\n","epoch 95, step 6012, loss: 6.742223\n","epoch 95, step 6013, loss: 3.864360\n","epoch 95, step 6014, loss: 0.000000\n","epoch 95, step 6015, loss: 0.000000\n","epoch 95, step 6016, loss: 0.000000\n","epoch 95, step 6017, loss: 0.000032\n","epoch 95, step 6018, loss: 0.000033\n","epoch 95, step 6019, loss: 0.000000\n","epoch 95, step 6020, loss: 0.000000\n","epoch 95, step 6021, loss: 0.000000\n","epoch 95, step 6022, loss: 0.000000\n","epoch 95, step 6023, loss: 0.000000\n","epoch 95, step 6024, loss: 0.000000\n","epoch 95, step 6025, loss: 0.000000\n","epoch 95, step 6026, loss: 35.075718\n","epoch 95, step 6027, loss: 0.000000\n","epoch 95, step 6028, loss: 0.000016\n","epoch 95, step 6029, loss: 0.000000\n","epoch 95, step 6030, loss: 0.000000\n","epoch 95, step 6031, loss: 0.000215\n","epoch 95, step 6032, loss: 0.000000\n","epoch 95, step 6033, loss: 0.000000\n","epoch 95, step 6034, loss: 0.000000\n","epoch 95, step 6035, loss: 0.000000\n","epoch 95, step 6036, loss: 0.000000\n","epoch 95, step 6037, loss: 0.000000\n","epoch 95, step 6038, loss: 0.000000\n","epoch 95, step 6039, loss: 0.000000\n","epoch 95, step 6040, loss: 0.000000\n","epoch 95, step 6041, loss: 0.000000\n","epoch 95, step 6042, loss: 10.907599\n","epoch 95, step 6043, loss: 73.023033\n","epoch 95, step 6044, loss: 0.000000\n","epoch 95, step 6045, loss: 0.000000\n","epoch 95, step 6046, loss: 0.000000\n","epoch 95, step 6047, loss: 0.350193\n","epoch 96, step 6048, loss: 0.000000\n","epoch 96, step 6049, loss: 0.000000\n","epoch 96, step 6050, loss: 0.000000\n","epoch 96, step 6051, loss: 232.813446\n","epoch 96, step 6052, loss: 0.000000\n","epoch 96, step 6053, loss: 0.000000\n","epoch 96, step 6054, loss: 0.795800\n","epoch 96, step 6055, loss: 0.000000\n","epoch 96, step 6056, loss: 8.571096\n","epoch 96, step 6057, loss: 0.000000\n","epoch 96, step 6058, loss: 38.662521\n","epoch 96, step 6059, loss: 42.657642\n","epoch 96, step 6060, loss: 0.000000\n","epoch 96, step 6061, loss: 0.000001\n","epoch 96, step 6062, loss: 0.855385\n","epoch 96, step 6063, loss: 0.000000\n","epoch 96, step 6064, loss: 0.000000\n","epoch 96, step 6065, loss: 0.000000\n","epoch 96, step 6066, loss: 0.006980\n","epoch 96, step 6067, loss: 79.267708\n","epoch 96, step 6068, loss: 0.000000\n","epoch 96, step 6069, loss: 0.000000\n","epoch 96, step 6070, loss: 0.000000\n","epoch 96, step 6071, loss: 0.000000\n","epoch 96, step 6072, loss: 0.000000\n","epoch 96, step 6073, loss: 38.110283\n","epoch 96, step 6074, loss: 0.000000\n","epoch 96, step 6075, loss: 0.000000\n","epoch 96, step 6076, loss: 0.000000\n","epoch 96, step 6077, loss: 0.000000\n","epoch 96, step 6078, loss: 0.000000\n","epoch 96, step 6079, loss: 0.000000\n","epoch 96, step 6080, loss: 0.000000\n","epoch 96, step 6081, loss: 0.000000\n","epoch 96, step 6082, loss: 0.000000\n","epoch 96, step 6083, loss: 0.000000\n","epoch 96, step 6084, loss: 0.000000\n","epoch 96, step 6085, loss: 0.000000\n","epoch 96, step 6086, loss: 0.000000\n","epoch 96, step 6087, loss: 0.000000\n","epoch 96, step 6088, loss: 0.000000\n","epoch 96, step 6089, loss: 33.968941\n","epoch 96, step 6090, loss: 0.000000\n","epoch 96, step 6091, loss: 0.000000\n","epoch 96, step 6092, loss: 0.006660\n","epoch 96, step 6093, loss: 0.000001\n","epoch 96, step 6094, loss: 0.000006\n","epoch 96, step 6095, loss: 0.000000\n","epoch 96, step 6096, loss: 0.000000\n","epoch 96, step 6097, loss: 0.000000\n","epoch 96, step 6098, loss: 0.000000\n","epoch 96, step 6099, loss: 0.000000\n","epoch 96, step 6100, loss: 6.007317\n","epoch 96, step 6101, loss: 0.000000\n","epoch 96, step 6102, loss: 0.000000\n","epoch 96, step 6103, loss: 0.000000\n","epoch 96, step 6104, loss: 0.000000\n","epoch 96, step 6105, loss: 12.904601\n","epoch 96, step 6106, loss: 79.231323\n","epoch 96, step 6107, loss: 0.000006\n","epoch 96, step 6108, loss: 0.000000\n","epoch 96, step 6109, loss: 8.523802\n","epoch 96, step 6110, loss: 61.265839\n","epoch 97, step 6111, loss: 0.000001\n","epoch 97, step 6112, loss: 0.000001\n","epoch 97, step 6113, loss: 0.000000\n","epoch 97, step 6114, loss: 48.957840\n","epoch 97, step 6115, loss: 0.027975\n","epoch 97, step 6116, loss: 0.000000\n","epoch 97, step 6117, loss: 0.000000\n","epoch 97, step 6118, loss: 0.000000\n","epoch 97, step 6119, loss: 0.002877\n","epoch 97, step 6120, loss: 0.313771\n","epoch 97, step 6121, loss: 59.774658\n","epoch 97, step 6122, loss: 14.353438\n","epoch 97, step 6123, loss: 0.000000\n","epoch 97, step 6124, loss: 0.000889\n","epoch 97, step 6125, loss: 23.461252\n","epoch 97, step 6126, loss: 6.159195\n","epoch 97, step 6127, loss: 0.000000\n","epoch 97, step 6128, loss: 24.005003\n","epoch 97, step 6129, loss: 0.000339\n","epoch 97, step 6130, loss: 36.873066\n","epoch 97, step 6131, loss: 0.000001\n","epoch 97, step 6132, loss: 0.000000\n","epoch 97, step 6133, loss: 0.000219\n","epoch 97, step 6134, loss: 0.322922\n","epoch 97, step 6135, loss: 0.000005\n","epoch 97, step 6136, loss: 76.765686\n","epoch 97, step 6137, loss: 0.000000\n","epoch 97, step 6138, loss: 0.000000\n","epoch 97, step 6139, loss: 0.600846\n","epoch 97, step 6140, loss: 0.000000\n","epoch 97, step 6141, loss: 0.015216\n","epoch 97, step 6142, loss: 0.000000\n","epoch 97, step 6143, loss: 6.927473\n","epoch 97, step 6144, loss: 0.000105\n","epoch 97, step 6145, loss: 0.000000\n","epoch 97, step 6146, loss: 0.000000\n","epoch 97, step 6147, loss: 0.000000\n","epoch 97, step 6148, loss: 0.000000\n","epoch 97, step 6149, loss: 0.000000\n","epoch 97, step 6150, loss: 0.000000\n","epoch 97, step 6151, loss: 0.000000\n","epoch 97, step 6152, loss: 34.227676\n","epoch 97, step 6153, loss: 0.000000\n","epoch 97, step 6154, loss: 0.000000\n","epoch 97, step 6155, loss: 1.546260\n","epoch 97, step 6156, loss: 0.000000\n","epoch 97, step 6157, loss: 0.000024\n","epoch 97, step 6158, loss: 0.000000\n","epoch 97, step 6159, loss: 0.000000\n","epoch 97, step 6160, loss: 0.000000\n","epoch 97, step 6161, loss: 0.000000\n","epoch 97, step 6162, loss: 0.000000\n","epoch 97, step 6163, loss: 0.007139\n","epoch 97, step 6164, loss: 0.000000\n","epoch 97, step 6165, loss: 0.000000\n","epoch 97, step 6166, loss: 0.000000\n","epoch 97, step 6167, loss: 0.000085\n","epoch 97, step 6168, loss: 0.005331\n","epoch 97, step 6169, loss: 64.092270\n","epoch 97, step 6170, loss: 0.000000\n","epoch 97, step 6171, loss: 110.339645\n","epoch 97, step 6172, loss: 0.000000\n","epoch 97, step 6173, loss: 39.849693\n","epoch 98, step 6174, loss: 110.450455\n","epoch 98, step 6175, loss: 0.000000\n","epoch 98, step 6176, loss: 0.000000\n","epoch 98, step 6177, loss: 68.635193\n","epoch 98, step 6178, loss: 0.242956\n","epoch 98, step 6179, loss: 0.000000\n","epoch 98, step 6180, loss: 0.000000\n","epoch 98, step 6181, loss: 0.000000\n","epoch 98, step 6182, loss: 0.000000\n","epoch 98, step 6183, loss: 14.814752\n","epoch 98, step 6184, loss: 21.254522\n","epoch 98, step 6185, loss: 16.025148\n","epoch 98, step 6186, loss: 0.000000\n","epoch 98, step 6187, loss: 0.000000\n","epoch 98, step 6188, loss: 135.611115\n","epoch 98, step 6189, loss: 0.000043\n","epoch 98, step 6190, loss: 0.000000\n","epoch 98, step 6191, loss: 0.000000\n","epoch 98, step 6192, loss: 0.000000\n","epoch 98, step 6193, loss: 71.327927\n","epoch 98, step 6194, loss: 0.000000\n","epoch 98, step 6195, loss: 0.000000\n","epoch 98, step 6196, loss: 0.000000\n","epoch 98, step 6197, loss: 0.000000\n","epoch 98, step 6198, loss: 0.000000\n","epoch 98, step 6199, loss: 5.770353\n","epoch 98, step 6200, loss: 0.000000\n","epoch 98, step 6201, loss: 0.000075\n","epoch 98, step 6202, loss: 0.000041\n","epoch 98, step 6203, loss: 0.000000\n","epoch 98, step 6204, loss: 0.000000\n","epoch 98, step 6205, loss: 0.000000\n","epoch 98, step 6206, loss: 0.000000\n","epoch 98, step 6207, loss: 0.000000\n","epoch 98, step 6208, loss: 0.000000\n","epoch 98, step 6209, loss: 0.000000\n","epoch 98, step 6210, loss: 0.000000\n","epoch 98, step 6211, loss: 12.170933\n","epoch 98, step 6212, loss: 0.000037\n","epoch 98, step 6213, loss: 1.515194\n","epoch 98, step 6214, loss: 0.000000\n","epoch 98, step 6215, loss: 12.729198\n","epoch 98, step 6216, loss: 0.000000\n","epoch 98, step 6217, loss: 0.000000\n","epoch 98, step 6218, loss: 0.000000\n","epoch 98, step 6219, loss: 0.000000\n","epoch 98, step 6220, loss: 0.000000\n","epoch 98, step 6221, loss: 0.000000\n","epoch 98, step 6222, loss: 0.000000\n","epoch 98, step 6223, loss: 0.000000\n","epoch 98, step 6224, loss: 0.000000\n","epoch 98, step 6225, loss: 6.239594\n","epoch 98, step 6226, loss: 0.000000\n","epoch 98, step 6227, loss: 0.000000\n","epoch 98, step 6228, loss: 0.000000\n","epoch 98, step 6229, loss: 0.000012\n","epoch 98, step 6230, loss: 0.000000\n","epoch 98, step 6231, loss: 0.000024\n","epoch 98, step 6232, loss: 85.368118\n","epoch 98, step 6233, loss: 0.000000\n","epoch 98, step 6234, loss: 0.000000\n","epoch 98, step 6235, loss: 0.000000\n","epoch 98, step 6236, loss: 48.597347\n","epoch 99, step 6237, loss: 0.000000\n","epoch 99, step 6238, loss: 0.000000\n","epoch 99, step 6239, loss: 0.000000\n","epoch 99, step 6240, loss: 204.463745\n","epoch 99, step 6241, loss: 0.000001\n","epoch 99, step 6242, loss: 0.000000\n","epoch 99, step 6243, loss: 2.893035\n","epoch 99, step 6244, loss: 0.000000\n","epoch 99, step 6245, loss: 0.000000\n","epoch 99, step 6246, loss: 28.861988\n","epoch 99, step 6247, loss: 45.247021\n","epoch 99, step 6248, loss: 24.201807\n","epoch 99, step 6249, loss: 0.000000\n","epoch 99, step 6250, loss: 0.000000\n","epoch 99, step 6251, loss: 0.000000\n","epoch 99, step 6252, loss: 0.306613\n","epoch 99, step 6253, loss: 0.000000\n","epoch 99, step 6254, loss: 0.000225\n","epoch 99, step 6255, loss: 0.000000\n","epoch 99, step 6256, loss: 32.840431\n","epoch 99, step 6257, loss: 0.000058\n","epoch 99, step 6258, loss: 0.000000\n","epoch 99, step 6259, loss: 2.995778\n","epoch 99, step 6260, loss: 0.000000\n","epoch 99, step 6261, loss: 0.000000\n","epoch 99, step 6262, loss: 65.102615\n","epoch 99, step 6263, loss: 0.000000\n","epoch 99, step 6264, loss: 0.000579\n","epoch 99, step 6265, loss: 0.000214\n","epoch 99, step 6266, loss: 0.000000\n","epoch 99, step 6267, loss: 0.000000\n","epoch 99, step 6268, loss: 0.000000\n","epoch 99, step 6269, loss: 0.000000\n","epoch 99, step 6270, loss: 0.000032\n","epoch 99, step 6271, loss: 0.000000\n","epoch 99, step 6272, loss: 0.000000\n","epoch 99, step 6273, loss: 0.000000\n","epoch 99, step 6274, loss: 0.000000\n","epoch 99, step 6275, loss: 62.842495\n","epoch 99, step 6276, loss: 0.000000\n","epoch 99, step 6277, loss: 0.000000\n","epoch 99, step 6278, loss: 16.410677\n","epoch 99, step 6279, loss: 0.000000\n","epoch 99, step 6280, loss: 0.000000\n","epoch 99, step 6281, loss: 0.000000\n","epoch 99, step 6282, loss: 0.000000\n","epoch 99, step 6283, loss: 0.000000\n","epoch 99, step 6284, loss: 0.000000\n","epoch 99, step 6285, loss: 0.000000\n","epoch 99, step 6286, loss: 0.000000\n","epoch 99, step 6287, loss: 0.000000\n","epoch 99, step 6288, loss: 0.105116\n","epoch 99, step 6289, loss: 0.003490\n","epoch 99, step 6290, loss: 0.000000\n","epoch 99, step 6291, loss: 0.000000\n","epoch 99, step 6292, loss: 0.000000\n","epoch 99, step 6293, loss: 0.011728\n","epoch 99, step 6294, loss: 0.000001\n","epoch 99, step 6295, loss: 91.177856\n","epoch 99, step 6296, loss: 0.000000\n","epoch 99, step 6297, loss: 0.000000\n","epoch 99, step 6298, loss: 0.000000\n","epoch 99, step 6299, loss: 54.037663\n"],"name":"stdout"}]},{"metadata":{"id":"P8PsQ11yhuC_","colab_type":"text"},"cell_type":"markdown","source":["In this section we use **train_y_perd**, **valid_y_perd**, **test_y_perd** which is used for the calulation of two score for each through the sklearn function **accuracy_score**, this comand take an optional argument **sample_weight** which is set normaly at 1. The first score use the same weight for all of examples (1), in the second case we set it with the given array form the tox21 dataset.\n","\n","\n","---\n","\n","\n","**Set a correct metric**\n","\n","Tox21 pass this array because we want that the model learn to recognize toxic substances, in the dataset this substances are only the 5% of the total, therefore the right way to set a metric is ballance this inequality. To do this we use mentionated array that contain a weight of 19 for toxic subtances and 1 for non-toxic substances. In this way, we have a complessive weight of toxic examples of 50% of total and a weight of 50% of total for a not-toxic examples.\n","Getting a balanced metric for our purposes\n"]},{"metadata":{"id":"WLLt1E5PgdsU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":125},"outputId":"5a936166-a746-4046-819f-e4d5ec0c07b0","executionInfo":{"status":"ok","timestamp":1552928610207,"user_tz":-60,"elapsed":5761,"user":{"displayName":"simone viozzi","photoUrl":"https://lh4.googleusercontent.com/-lHfKAhqXixo/AAAAAAAAAAI/AAAAAAAABF8/q48tw94xHaI/s64/photo.jpg","userId":"00173786927897941269"}}},"cell_type":"code","source":["#calculation of normal score for the train dataset\n","train_score = accuracy_score(train_y, train_y_pred)\n","print(\"Unweighted Classification Accuracy for the training dataset: %f\" % train_score)\n","#calculation of weighted score for the train dataset\n","weighted_train_score = accuracy_score(train_y, train_y_pred, sample_weight=train_w)\n","print(\"Weighted Classification Accuracy for the training dataset: %f\" % weighted_train_score)\n","\n","#calculation of normal score for the validation dataset\n","valid_score = accuracy_score(valid_y, valid_y_pred)\n","print(\"Unweighted Classification Accuracy for the validation dataset: %f\" % valid_score)\n","#calculation of weighted score for the validation dataset\n","weighted_valid_score = accuracy_score(valid_y, valid_y_pred, sample_weight=valid_w)\n","print(\"Weighted Classification Accuracy for the validation dataset: %f\" % weighted_valid_score)\n","\n","#calculation of normal score for the test dataset\n","test_score = accuracy_score(test_y, test_y_pred)\n","print(\"Unweighted Classification Accuracy for the test dataset: %f\" % test_score)\n","#calculation of weighted score for the test dataset\n","weighted_test_score = accuracy_score(test_y, test_y_pred, sample_weight=test_w)\n","print(\"Weighted Classification Accuracy for the test dataset: %f\" % weighted_test_score)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Unweighted Classification Accuracy for the training dataset: 0.997925\n","Weighted Classification Accuracy for the training dataset: 0.990987\n","Unweighted Classification Accuracy for the validation dataset: 0.943806\n","Weighted Classification Accuracy for the validation dataset: 0.684253\n","Unweighted Classification Accuracy for the test dataset: 0.924745\n","Weighted Classification Accuracy for the test dataset: 0.636813\n"],"name":"stdout"}]},{"metadata":{"id":"HigzclB_aXTH","colab_type":"text"},"cell_type":"markdown","source":["#Training a Random Forest to recognize if a molecule is toxic for humans"]},{"metadata":{"id":"yGefFTs4bFTd","colab_type":"text"},"cell_type":"markdown","source":["Now we importing from sklearn library a usefull method for train a random forest classifier "]},{"metadata":{"id":"ThVh1pjCaraH","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OPBHRPFxbaz7","colab_type":"text"},"cell_type":"markdown","source":["Whit this sample function is easy create a standard machine learning algorithm and train it with the sample function **fit**, noting that the two arguments of this function is the input data first and the lables, the model do all necessary adjustment."]},{"metadata":{"id":"nBr6EDrYbdQJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":143},"outputId":"61696813-6ba6-42c3-a59a-81676995e0ef","executionInfo":{"status":"ok","timestamp":1552927460831,"user_tz":-60,"elapsed":20810,"user":{"displayName":"simone viozzi","photoUrl":"https://lh4.googleusercontent.com/-lHfKAhqXixo/AAAAAAAAAAI/AAAAAAAABF8/q48tw94xHaI/s64/photo.jpg","userId":"00173786927897941269"}}},"cell_type":"code","source":["sklearnModel = RandomForestClassifier(class_weight=\"balanced\", n_estimators=500)\n","sklearnModel.fit(train_X, train_y)"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RandomForestClassifier(bootstrap=True, class_weight='balanced',\n","            criterion='gini', max_depth=None, max_features='auto',\n","            max_leaf_nodes=None, min_impurity_decrease=0.0,\n","            min_impurity_split=None, min_samples_leaf=1,\n","            min_samples_split=2, min_weight_fraction_leaf=0.0,\n","            n_estimators=500, n_jobs=None, oob_score=False,\n","            random_state=None, verbose=0, warm_start=False)"]},"metadata":{"tags":[]},"execution_count":21}]},{"metadata":{"id":"W7t2WJHBcfdu","colab_type":"text"},"cell_type":"markdown","source":["After training the model we can get the result from the entire validation and test dataset and store it in same variables, with the sample command **predict**."]},{"metadata":{"id":"c4xZ8S08ciLX","colab_type":"code","colab":{}},"cell_type":"code","source":["train_y_pred = sklearnModel.predict(train_X)\n","valid_y_pred = sklearnModel.predict(valid_X)\n","test_y_pred = sklearnModel.predict(test_X)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1ahAV-lOfIg-","colab_type":"text"},"cell_type":"markdown","source":["Now we doing the same thing done in the precedent example for ballance the metric also for the random forset classifier."]},{"metadata":{"id":"ovnqaXfSfJEf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"8fe9f40f-ec45-42ea-f8a3-30367bcee975","executionInfo":{"status":"ok","timestamp":1552927480318,"user_tz":-60,"elapsed":1330,"user":{"displayName":"simone viozzi","photoUrl":"https://lh4.googleusercontent.com/-lHfKAhqXixo/AAAAAAAAAAI/AAAAAAAABF8/q48tw94xHaI/s64/photo.jpg","userId":"00173786927897941269"}}},"cell_type":"code","source":["#calculation of weighted score for the train dataset\n","weighted_score = accuracy_score(train_y, train_y_pred, sample_weight = train_w)\n","print(\"Weighted train Classification Accurancy: %f\" %weighted_score)\n","#calculation of weighted score for the validation dataset\n","weighted_score =  accuracy_score(valid_y, valid_y_pred, sample_weight = valid_w)\n","print(\"Weighted validation Classification Accurancy: %f\" %weighted_score)\n","#calculation of weighted score for the test dataset\n","weighted_score =  accuracy_score(test_y, test_y_pred, sample_weight = test_w)\n","print(\"Weighted test Classification Accurancy: %f\" %weighted_score)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Weighted train Classification Accurancy: 0.998817\n","Weighted validation Classification Accurancy: 0.663493\n","Weighted test Classification Accurancy: 0.649528\n"],"name":"stdout"}]}]}