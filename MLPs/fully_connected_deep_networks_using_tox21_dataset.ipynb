{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fully_connected_deep_networks_using_tox21_dataset.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MassimilianoBiancucci/Tensorflow-exercises/blob/master/MLPs/fully_connected_deep_networks_using_tox21_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "407WW85KfoG4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Training an MLP network to recognize if a molecule is toxic for humans\n"
      ]
    },
    {
      "metadata": {
        "id": "M1_z56TAzYXO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We starting to setting up our VM to get all packets needed for run our code."
      ]
    },
    {
      "metadata": {
        "id": "I3n04hHnrKqe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install deepchem\n",
        "!pip install simdna\n",
        "!pip install nosetests\n",
        "!wget -c https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
        "!time bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
        "!time conda install -q -y -c conda-forge rdkit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l2ZBudqz17Kh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this section we download the packets needed for run a tensorboard remotly on colab and get a link at ngrok.com to access to the tensorboard trought the colab's firewall.\n",
        "![alt text](https://gitcdn.xyz/cdn/Tony607/blog_statics/d425c3fe4cf0d92067572e25ae6cc3198d51936b//images/ngrok/ngrok.jpg)"
      ]
    },
    {
      "metadata": {
        "id": "X73uV25F08pr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z5rXtmcd17iv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "here we launch the tensorboard in background and  set the directory for saving session log.\n"
      ]
    },
    {
      "metadata": {
        "id": "iMyAOfmG09cR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "LOG_DIR = './log'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KssEpVBS2GwZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then, we can run ngrok to tunnel TensorBoard port 6006 to the outside world. This command also runs in the background.\n"
      ]
    },
    {
      "metadata": {
        "id": "dzJYm_O12zKu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jGQdt_OC2zvi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we get the public URL where we can access the colab TensorBoard.\n",
        "It's important keep in mind that the training have to start before you can see somthing in. \n"
      ]
    },
    {
      "metadata": {
        "id": "MBPAVeUu2OH1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qM_-1V682PFP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Import libraries:**\n",
        "\n",
        "we start importing **numpy**, a python library to work efficiently with tensors, folowed by **tensorflow** and  **sklearn.metrics**, the libraries that contain the toolkit for work with data, networks and in this case tools for getting the performance of model.\n",
        "**Matplotlib** for display data and in the end **deepchem** that contain the tox21 dataset.\n",
        "\n",
        "**Tox21**, is a unique collaboration between several federal agencies to develop new ways to rapidly test whether substances adversely affect human health. This dataset consists of a set of 10,000 molecules represented vectorially  tested for interaction with the androgen receptor.\n"
      ]
    },
    {
      "metadata": {
        "id": "UyXZ1Je7TL9T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import os\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')\n",
        "import numpy as np\n",
        "np.random.seed(456)\n",
        "import  tensorflow as tf\n",
        "tf.set_random_seed(456)\n",
        "import deepchem.molnet as dc\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MHTx4slHfmtK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we load tox21 dataset and prepare it, removing usefull data."
      ]
    },
    {
      "metadata": {
        "id": "bWFBDvA3mzxl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "_, (train, valid, test), _ = dc.load_tox21()\n",
        "train_X, train_y, train_w = train.X, train.y, train.w\n",
        "valid_X, valid_y, valid_w = valid.X, valid.y, valid.w\n",
        "test_X, test_y, test_w = test.X, test.y, test.w\n",
        "\n",
        "\n",
        "# Remove extra tasks\n",
        "train_y = train_y[:, 0]\n",
        "valid_y = valid_y[:, 0]\n",
        "test_y = test_y[:, 0]\n",
        "train_w = train_w[:, 0]\n",
        "valid_w = valid_w[:, 0]\n",
        "test_w = test_w[:, 0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V4bke0dV2EiV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this section we declare the structure of the tensor graph, which represents the model."
      ]
    },
    {
      "metadata": {
        "id": "Wl1zIsI2zBVq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Generate tensorflow graph\n",
        "#general parameters\n",
        "d = 1024\n",
        "n_hidden = 100\n",
        "n_hidden2 = 80\n",
        "n_hidden3 = 30\n",
        "learning_rate = .001\n",
        "n_epochs = 100\n",
        "batch_size = 100\n",
        "\n",
        "#dataset\n",
        "with tf.name_scope(\"dataset\"):\n",
        "  x = tf.placeholder(tf.float32, (None, d))\n",
        "  y = tf.placeholder(tf.float32, (None,))\n",
        "  \n",
        "  \n",
        "with tf.name_scope(\"hidden-layer1\"):\n",
        "  W = tf.Variable(tf.random_normal((d, n_hidden)))\n",
        "  b = tf.Variable(tf.random_normal((n_hidden,)))\n",
        "  x_hidden = tf.nn.relu(tf.matmul(x, W) + b)\n",
        "  \n",
        "with tf.name_scope(\"hidden-layer2\"):\n",
        "  W = tf.Variable(tf.random_normal((n_hidden, n_hidden2)))\n",
        "  b = tf.Variable(tf.random_normal((n_hidden2,)))\n",
        "  x_hidden2 = tf.nn.relu(tf.matmul(x_hidden, W) + b)\n",
        "  \n",
        "with tf.name_scope(\"hidden-layer3\"):\n",
        "  W = tf.Variable(tf.random_normal((n_hidden2, n_hidden3)))\n",
        "  b = tf.Variable(tf.random_normal((n_hidden3,)))\n",
        "  x_hidden3 = tf.nn.relu(tf.matmul(x_hidden2, W) + b)\n",
        "  \n",
        "with tf.name_scope(\"output\"):\n",
        "  W = tf.Variable(tf.random_normal((n_hidden3, 1)))\n",
        "  b = tf.Variable(tf.random_normal((1,)))\n",
        "  y_logit = tf.matmul(x_hidden3, W) + b\n",
        "  \n",
        "  # the sigmoid gives the class probability of 1\n",
        "  y_one_prob = tf.sigmoid(y_logit)\n",
        "  # Rounding P(y=1) will give the correct prediction.\n",
        "  y_pred = tf.round(y_one_prob)\n",
        "  \n",
        " #setting of loss function\n",
        "with tf.name_scope(\"loss\"):\n",
        "  \n",
        "  # Compute the cross-entropy term for each datapoint\n",
        "  y_expand = tf.expand_dims(y, 1)\n",
        "  entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_logit, labels=y_expand)\n",
        "  \n",
        "  # Sum all contributions\n",
        "  l = tf.reduce_sum(entropy)\n",
        "\n",
        "#setting of optimization algorithm\n",
        "with tf.name_scope(\"optim\"):\n",
        "  train_op = tf.train.AdamOptimizer(learning_rate).minimize(l)\n",
        "\n",
        "#setting variables to show in tensorboard scalar section \n",
        "with tf.name_scope(\"summaries\"):\n",
        "  tf.summary.scalar(\"loss\", l)\n",
        "  merged = tf.summary.merge_all()\n",
        "\n",
        "#configure folder for tensorboard data\n",
        "train_writer = tf.summary.FileWriter(LOG_DIR, tf.get_default_graph())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rHz2XtUPf6ha",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this section the model is trained"
      ]
    },
    {
      "metadata": {
        "id": "uqHbB1vWf7H2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "N = train_X.shape[0]\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  step = 0\n",
        "  for epoch in range(n_epochs):\n",
        "    pos = 0\n",
        "    while pos < N:\n",
        "      batch_X = train_X[pos:pos+batch_size]\n",
        "      batch_y = train_y[pos:pos+batch_size]\n",
        "      feed_dict = {x: batch_X, y: batch_y}\n",
        "      _, summary, loss = sess.run([train_op, merged, l], feed_dict=feed_dict)\n",
        "      print(\"epoch %d, step %d, loss: %f\" % (epoch, step, loss))\n",
        "      train_writer.add_summary(summary, step)\n",
        "    \n",
        "      step += 1\n",
        "      pos += batch_size\n",
        "\n",
        "  # Make Predictions for model evaluetion\n",
        "  valid_y_pred = sess.run(y_pred, feed_dict={x: valid_X})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WLLt1E5PgdsU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "score = accuracy_score(valid_y, valid_y_pred)\n",
        "print(\"Unweighted Classification Accuracy: %f\" % score)\n",
        "\n",
        "weighted_score = accuracy_score(valid_y, valid_y_pred, sample_weight=valid_w)\n",
        "print(\"Weighted Classification Accuracy: %f\" % weighted_score)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}