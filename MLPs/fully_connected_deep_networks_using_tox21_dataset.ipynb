{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fully_connected_deep_networks_using_tox21_dataset.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MassimilianoBiancucci/Tensorflow-exercises/blob/master/MLPs/fully_connected_deep_networks_using_tox21_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "407WW85KfoG4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Training an MLP network to recognize if a molecule is toxic for humans\n"
      ]
    },
    {
      "metadata": {
        "id": "M1_z56TAzYXO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We starting to setting up our VM to get all packets needed for run our code."
      ]
    },
    {
      "metadata": {
        "id": "I3n04hHnrKqe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1725
        },
        "outputId": "37496238-b084-4ffd-aa67-9ebabac06b2b"
      },
      "cell_type": "code",
      "source": [
        "!pip install deepchem\n",
        "!pip install simdna\n",
        "!pip install nosetests\n",
        "!wget -c https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
        "!time bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
        "!time conda install -q -y -c conda-forge rdkit"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: deepchem in /usr/local/lib/python3.7/site-packages (2.1.1.dev353)\n",
            "Requirement already satisfied: simdna in /usr/local/lib/python3.7/site-packages (0.4.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/site-packages (from simdna) (1.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/site-packages (from simdna) (3.0.3)\n",
            "Requirement already satisfied: numpy>=1.9 in /usr/local/lib/python3.7/site-packages (from simdna) (1.16.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/site-packages (from matplotlib->simdna) (1.0.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/site-packages (from matplotlib->simdna) (2.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/site-packages (from matplotlib->simdna) (2.8.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/site-packages (from matplotlib->simdna) (0.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->simdna) (40.6.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib->simdna) (1.12.0)\n",
            "Collecting nosetests\n",
            "\u001b[31m  Could not find a version that satisfies the requirement nosetests (from versions: )\u001b[0m\n",
            "\u001b[31mNo matching distribution found for nosetests\u001b[0m\n",
            "--2019-03-17 22:22:35--  https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
            "Resolving repo.continuum.io (repo.continuum.io)... 104.18.201.79, 104.18.200.79, 2606:4700::6812:c84f, ...\n",
            "Connecting to repo.continuum.io (repo.continuum.io)|104.18.201.79|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n",
            "PREFIX=/usr/local\n",
            "reinstalling: python-3.7.1-h0371630_7 ...\n",
            "Python 3.7.1\n",
            "reinstalling: ca-certificates-2018.03.07-0 ...\n",
            "reinstalling: conda-env-2.6.0-1 ...\n",
            "reinstalling: libgcc-ng-8.2.0-hdf63c60_1 ...\n",
            "reinstalling: libstdcxx-ng-8.2.0-hdf63c60_1 ...\n",
            "reinstalling: libffi-3.2.1-hd88cf55_4 ...\n",
            "reinstalling: ncurses-6.1-he6710b0_1 ...\n",
            "reinstalling: openssl-1.1.1a-h7b6447c_0 ...\n",
            "reinstalling: xz-5.2.4-h14c3975_4 ...\n",
            "reinstalling: yaml-0.1.7-had09818_2 ...\n",
            "reinstalling: zlib-1.2.11-h7b6447c_3 ...\n",
            "reinstalling: libedit-3.1.20170329-h6b74fdf_2 ...\n",
            "reinstalling: readline-7.0-h7b6447c_5 ...\n",
            "reinstalling: tk-8.6.8-hbc83047_0 ...\n",
            "reinstalling: sqlite-3.26.0-h7b6447c_0 ...\n",
            "reinstalling: asn1crypto-0.24.0-py37_0 ...\n",
            "reinstalling: certifi-2018.11.29-py37_0 ...\n",
            "reinstalling: chardet-3.0.4-py37_1 ...\n",
            "reinstalling: idna-2.8-py37_0 ...\n",
            "reinstalling: pycosat-0.6.3-py37h14c3975_0 ...\n",
            "reinstalling: pycparser-2.19-py37_0 ...\n",
            "reinstalling: pysocks-1.6.8-py37_0 ...\n",
            "reinstalling: ruamel_yaml-0.15.46-py37h14c3975_0 ...\n",
            "reinstalling: six-1.12.0-py37_0 ...\n",
            "reinstalling: cffi-1.11.5-py37he75722e_1 ...\n",
            "reinstalling: setuptools-40.6.3-py37_0 ...\n",
            "reinstalling: cryptography-2.4.2-py37h1ba5d50_0 ...\n",
            "reinstalling: wheel-0.32.3-py37_0 ...\n",
            "reinstalling: pip-18.1-py37_0 ...\n",
            "reinstalling: pyopenssl-18.0.0-py37_0 ...\n",
            "reinstalling: urllib3-1.24.1-py37_0 ...\n",
            "reinstalling: requests-2.21.0-py37_0 ...\n",
            "reinstalling: conda-4.5.12-py37_0 ...\n",
            "unlinking: ca-certificates-2019.3.9-hecc5488_0\n",
            "unlinking: certifi-2019.3.9-py37_0\n",
            "unlinking: conda-4.6.8-py37_0\n",
            "unlinking: openssl-1.1.1b-h14c3975_1\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n",
            "\n",
            "real\t0m17.405s\n",
            "user\t0m15.261s\n",
            "sys\t0m3.695s\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs: \n",
            "    - rdkit\n",
            "\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "    ca-certificates: 2018.03.07-0      --> 2019.3.9-hecc5488_0 conda-forge\n",
            "    certifi:         2018.11.29-py37_0 --> 2019.3.9-py37_0     conda-forge\n",
            "    conda:           4.5.12-py37_0     --> 4.6.8-py37_0        conda-forge\n",
            "    openssl:         1.1.1a-h7b6447c_0 --> 1.1.1b-h14c3975_1   conda-forge\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "\n",
            "real\t0m23.521s\n",
            "user\t0m22.228s\n",
            "sys\t0m1.090s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l2ZBudqz17Kh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this section we download the packets needed for run a tensorboard remotly on colab and get a link at ngrok.com to access to the tensorboard trought the colab's firewall.\n",
        "![alt text](https://gitcdn.xyz/cdn/Tony607/blog_statics/d425c3fe4cf0d92067572e25ae6cc3198d51936b//images/ngrok/ngrok.jpg)"
      ]
    },
    {
      "metadata": {
        "id": "X73uV25F08pr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "30a9bbc8-f44c-43fc-e1c9-6955b4dbb983"
      },
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-17 22:23:21--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.86.186.182, 52.200.123.104, 52.201.75.180, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.86.186.182|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14910739 (14M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  14.22M  18.1MB/s    in 0.8s    \n",
            "\n",
            "2019-03-17 22:23:22 (18.1 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [14910739/14910739]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z5rXtmcd17iv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "here we launch the tensorboard in background and  set the directory for saving session log.\n"
      ]
    },
    {
      "metadata": {
        "id": "iMyAOfmG09cR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "LOG_DIR = './log'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KssEpVBS2GwZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then, we can run ngrok to tunnel TensorBoard port 6006 to the outside world. This command also runs in the background.\n"
      ]
    },
    {
      "metadata": {
        "id": "dzJYm_O12zKu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jGQdt_OC2zvi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we get the public URL where we can access the colab TensorBoard.\n",
        "It's important keep in mind that the training have to start before you can see somthing in. \n"
      ]
    },
    {
      "metadata": {
        "id": "MBPAVeUu2OH1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c7eb8911-51b8-4092-f88b-372880803408"
      },
      "cell_type": "code",
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://fdd0e037.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qM_-1V682PFP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Import libraries:**\n",
        "\n",
        "we start importing **numpy**, a python library to work efficiently with tensors, folowed by **tensorflow** and  **sklearn.metrics**, the libraries that contain the toolkit for work with data, networks and in this case tools for getting the performance of model.\n",
        "**Matplotlib** for display data and in the end **deepchem** that contain the tox21 dataset.\n",
        "\n",
        "**Tox21**, is a unique collaboration between several federal agencies to develop new ways to rapidly test whether substances adversely affect human health. This dataset consists of a set of 10,000 molecules represented vectorially  tested for interaction with the androgen receptor.\n"
      ]
    },
    {
      "metadata": {
        "id": "UyXZ1Je7TL9T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import os\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')\n",
        "import numpy as np\n",
        "np.random.seed(456)\n",
        "import  tensorflow as tf\n",
        "tf.set_random_seed(456)\n",
        "import deepchem.molnet as dc\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MHTx4slHfmtK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we load tox21 dataset and prepare it, removing usefull data."
      ]
    },
    {
      "metadata": {
        "id": "bWFBDvA3mzxl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "f49f7e95-b945-448f-faf4-efc0bdfbd23f"
      },
      "cell_type": "code",
      "source": [
        "_, (train, valid, test), _ = dc.load_tox21()\n",
        "train_X, train_y, train_w = train.X, train.y, train.w\n",
        "valid_X, valid_y, valid_w = valid.X, valid.y, valid.w\n",
        "test_X, test_y, test_w = test.X, test.y, test.w\n",
        "\n",
        "\n",
        "# Remove extra tasks\n",
        "train_y = train_y[:, 0]\n",
        "valid_y = valid_y[:, 0]\n",
        "test_y = test_y[:, 0]\n",
        "train_w = train_w[:, 0]\n",
        "valid_w = valid_w[:, 0]\n",
        "test_w = test_w[:, 0]\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading dataset from disk.\n",
            "Loading dataset from disk.\n",
            "Loading dataset from disk.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V4bke0dV2EiV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this section we declare the structure of the tensor graph, which represents the model."
      ]
    },
    {
      "metadata": {
        "id": "Wl1zIsI2zBVq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "e9264290-dc6c-4e6e-c720-232658fe71da"
      },
      "cell_type": "code",
      "source": [
        "# Generate tensorflow graph\n",
        "#general parameters\n",
        "d = 1024\n",
        "n_hidden = 100\n",
        "n_hidden2 = 80\n",
        "n_hidden3 = 30\n",
        "learning_rate = .001\n",
        "n_epochs = 100\n",
        "batch_size = 100\n",
        "\n",
        "#dataset\n",
        "with tf.name_scope(\"dataset\"):\n",
        "  x = tf.placeholder(tf.float32, (None, d))\n",
        "  y = tf.placeholder(tf.float32, (None,))\n",
        "  \n",
        "  \n",
        "with tf.name_scope(\"hidden-layer1\"):\n",
        "  W = tf.Variable(tf.random_normal((d, n_hidden)))\n",
        "  b = tf.Variable(tf.random_normal((n_hidden,)))\n",
        "  x_hidden = tf.nn.relu(tf.matmul(x, W) + b)\n",
        "  \n",
        "with tf.name_scope(\"hidden-layer2\"):\n",
        "  W = tf.Variable(tf.random_normal((n_hidden, n_hidden2)))\n",
        "  b = tf.Variable(tf.random_normal((n_hidden2,)))\n",
        "  x_hidden2 = tf.nn.relu(tf.matmul(x_hidden, W) + b)\n",
        "  \n",
        "with tf.name_scope(\"hidden-layer3\"):\n",
        "  W = tf.Variable(tf.random_normal((n_hidden2, n_hidden3)))\n",
        "  b = tf.Variable(tf.random_normal((n_hidden3,)))\n",
        "  x_hidden3 = tf.nn.relu(tf.matmul(x_hidden2, W) + b)\n",
        "  \n",
        "with tf.name_scope(\"output\"):\n",
        "  W = tf.Variable(tf.random_normal((n_hidden3, 1)))\n",
        "  b = tf.Variable(tf.random_normal((1,)))\n",
        "  y_logit = tf.matmul(x_hidden3, W) + b\n",
        "  \n",
        "  # the sigmoid gives the class probability of 1\n",
        "  y_one_prob = tf.sigmoid(y_logit)\n",
        "  # Rounding P(y=1) will give the correct prediction.\n",
        "  y_pred = tf.round(y_one_prob)\n",
        "  \n",
        " #setting of loss function\n",
        "with tf.name_scope(\"loss\"):\n",
        "  \n",
        "  # Compute the cross-entropy term for each datapoint\n",
        "  y_expand = tf.expand_dims(y, 1)\n",
        "  entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_logit, labels=y_expand)\n",
        "  \n",
        "  # Sum all contributions\n",
        "  l = tf.reduce_sum(entropy)\n",
        "\n",
        "#setting of optimization algorithm\n",
        "with tf.name_scope(\"optim\"):\n",
        "  train_op = tf.train.AdamOptimizer(learning_rate).minimize(l)\n",
        "\n",
        "#setting variables to show in tensorboard scalar section \n",
        "with tf.name_scope(\"summaries\"):\n",
        "  tf.summary.scalar(\"loss\", l)\n",
        "  merged = tf.summary.merge_all()\n",
        "\n",
        "#configure folder for tensorboard data\n",
        "train_writer = tf.summary.FileWriter(LOG_DIR, tf.get_default_graph())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rHz2XtUPf6ha",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this section the model is trained"
      ]
    },
    {
      "metadata": {
        "id": "uqHbB1vWf7H2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74985
        },
        "outputId": "660a99af-9d4c-4f73-c182-89131861c126"
      },
      "cell_type": "code",
      "source": [
        "N = train_X.shape[0]\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  step = 0\n",
        "  for epoch in range(n_epochs):\n",
        "    pos = 0\n",
        "    while pos < N:\n",
        "      batch_X = train_X[pos:pos+batch_size]\n",
        "      batch_y = train_y[pos:pos+batch_size]\n",
        "      feed_dict = {x: batch_X, y: batch_y}\n",
        "      _, summary, loss = sess.run([train_op, merged, l], feed_dict=feed_dict)\n",
        "      print(\"epoch %d, step %d, loss: %f\" % (epoch, step, loss))\n",
        "      train_writer.add_summary(summary, step)\n",
        "    \n",
        "      step += 1\n",
        "      pos += batch_size\n",
        "\n",
        "  # Make Predictions for model evaluetion\n",
        "  valid_y_pred = sess.run(y_pred, feed_dict={x: valid_X})"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 33, step 2083, loss: 0.118257\n",
            "epoch 33, step 2084, loss: 0.000001\n",
            "epoch 33, step 2085, loss: 0.000000\n",
            "epoch 33, step 2086, loss: 0.000000\n",
            "epoch 33, step 2087, loss: 0.000000\n",
            "epoch 33, step 2088, loss: 0.000000\n",
            "epoch 33, step 2089, loss: 0.000000\n",
            "epoch 33, step 2090, loss: 3.811352\n",
            "epoch 33, step 2091, loss: 0.000000\n",
            "epoch 33, step 2092, loss: 0.000000\n",
            "epoch 33, step 2093, loss: 0.005260\n",
            "epoch 33, step 2094, loss: 0.000000\n",
            "epoch 33, step 2095, loss: 21.406046\n",
            "epoch 33, step 2096, loss: 0.000000\n",
            "epoch 33, step 2097, loss: 0.913829\n",
            "epoch 33, step 2098, loss: 126.103233\n",
            "epoch 33, step 2099, loss: 0.000000\n",
            "epoch 33, step 2100, loss: 0.000000\n",
            "epoch 33, step 2101, loss: 0.000000\n",
            "epoch 33, step 2102, loss: 0.000000\n",
            "epoch 33, step 2103, loss: 0.000001\n",
            "epoch 33, step 2104, loss: 0.000595\n",
            "epoch 33, step 2105, loss: 0.000000\n",
            "epoch 33, step 2106, loss: 0.000000\n",
            "epoch 33, step 2107, loss: 0.000000\n",
            "epoch 33, step 2108, loss: 0.000000\n",
            "epoch 33, step 2109, loss: 0.000000\n",
            "epoch 33, step 2110, loss: 0.000001\n",
            "epoch 33, step 2111, loss: 0.000000\n",
            "epoch 33, step 2112, loss: 0.000005\n",
            "epoch 33, step 2113, loss: 0.000052\n",
            "epoch 33, step 2114, loss: 0.214641\n",
            "epoch 33, step 2115, loss: 0.000000\n",
            "epoch 33, step 2116, loss: 0.000000\n",
            "epoch 33, step 2117, loss: 0.000000\n",
            "epoch 33, step 2118, loss: 23.419691\n",
            "epoch 33, step 2119, loss: 0.000000\n",
            "epoch 33, step 2120, loss: 58.289455\n",
            "epoch 33, step 2121, loss: 0.000000\n",
            "epoch 33, step 2122, loss: 0.071144\n",
            "epoch 33, step 2123, loss: 0.001295\n",
            "epoch 33, step 2124, loss: 0.000000\n",
            "epoch 33, step 2125, loss: 0.000000\n",
            "epoch 33, step 2126, loss: 0.000009\n",
            "epoch 33, step 2127, loss: 0.000000\n",
            "epoch 33, step 2128, loss: 24.078114\n",
            "epoch 33, step 2129, loss: 0.000020\n",
            "epoch 33, step 2130, loss: 0.000000\n",
            "epoch 33, step 2131, loss: 0.001429\n",
            "epoch 33, step 2132, loss: 0.000000\n",
            "epoch 33, step 2133, loss: 0.000000\n",
            "epoch 33, step 2134, loss: 0.081803\n",
            "epoch 33, step 2135, loss: 0.000000\n",
            "epoch 33, step 2136, loss: 132.024536\n",
            "epoch 33, step 2137, loss: 0.186649\n",
            "epoch 33, step 2138, loss: 0.000059\n",
            "epoch 33, step 2139, loss: 0.000000\n",
            "epoch 33, step 2140, loss: 0.000065\n",
            "epoch 33, step 2141, loss: 2.065889\n",
            "epoch 34, step 2142, loss: 0.000001\n",
            "epoch 34, step 2143, loss: 0.000086\n",
            "epoch 34, step 2144, loss: 0.001287\n",
            "epoch 34, step 2145, loss: 176.294052\n",
            "epoch 34, step 2146, loss: 0.000000\n",
            "epoch 34, step 2147, loss: 0.043932\n",
            "epoch 34, step 2148, loss: 0.000069\n",
            "epoch 34, step 2149, loss: 0.000000\n",
            "epoch 34, step 2150, loss: 0.000810\n",
            "epoch 34, step 2151, loss: 24.035452\n",
            "epoch 34, step 2152, loss: 61.701263\n",
            "epoch 34, step 2153, loss: 69.672623\n",
            "epoch 34, step 2154, loss: 0.565779\n",
            "epoch 34, step 2155, loss: 0.000001\n",
            "epoch 34, step 2156, loss: 0.000005\n",
            "epoch 34, step 2157, loss: 0.000000\n",
            "epoch 34, step 2158, loss: 0.000024\n",
            "epoch 34, step 2159, loss: 0.162576\n",
            "epoch 34, step 2160, loss: 0.466759\n",
            "epoch 34, step 2161, loss: 173.603256\n",
            "epoch 34, step 2162, loss: 0.000000\n",
            "epoch 34, step 2163, loss: 0.000000\n",
            "epoch 34, step 2164, loss: 0.000000\n",
            "epoch 34, step 2165, loss: 0.000276\n",
            "epoch 34, step 2166, loss: 1.326131\n",
            "epoch 34, step 2167, loss: 11.485253\n",
            "epoch 34, step 2168, loss: 0.000000\n",
            "epoch 34, step 2169, loss: 0.000000\n",
            "epoch 34, step 2170, loss: 0.000000\n",
            "epoch 34, step 2171, loss: 0.000000\n",
            "epoch 34, step 2172, loss: 0.000000\n",
            "epoch 34, step 2173, loss: 0.000031\n",
            "epoch 34, step 2174, loss: 0.000000\n",
            "epoch 34, step 2175, loss: 0.000000\n",
            "epoch 34, step 2176, loss: 0.000000\n",
            "epoch 34, step 2177, loss: 0.008017\n",
            "epoch 34, step 2178, loss: 0.000000\n",
            "epoch 34, step 2179, loss: 0.000004\n",
            "epoch 34, step 2180, loss: 0.000000\n",
            "epoch 34, step 2181, loss: 0.000000\n",
            "epoch 34, step 2182, loss: 0.000000\n",
            "epoch 34, step 2183, loss: 40.226429\n",
            "epoch 34, step 2184, loss: 0.000000\n",
            "epoch 34, step 2185, loss: 0.000001\n",
            "epoch 34, step 2186, loss: 0.000087\n",
            "epoch 34, step 2187, loss: 0.000143\n",
            "epoch 34, step 2188, loss: 0.000000\n",
            "epoch 34, step 2189, loss: 9.312323\n",
            "epoch 34, step 2190, loss: 0.000000\n",
            "epoch 34, step 2191, loss: 0.000000\n",
            "epoch 34, step 2192, loss: 0.000000\n",
            "epoch 34, step 2193, loss: 0.000000\n",
            "epoch 34, step 2194, loss: 0.528550\n",
            "epoch 34, step 2195, loss: 0.000000\n",
            "epoch 34, step 2196, loss: 0.050213\n",
            "epoch 34, step 2197, loss: 0.000007\n",
            "epoch 34, step 2198, loss: 0.000000\n",
            "epoch 34, step 2199, loss: 0.311447\n",
            "epoch 34, step 2200, loss: 198.919586\n",
            "epoch 34, step 2201, loss: 0.065881\n",
            "epoch 34, step 2202, loss: 0.000000\n",
            "epoch 34, step 2203, loss: 0.000000\n",
            "epoch 34, step 2204, loss: 70.706161\n",
            "epoch 35, step 2205, loss: 0.000000\n",
            "epoch 35, step 2206, loss: 0.000000\n",
            "epoch 35, step 2207, loss: 0.000000\n",
            "epoch 35, step 2208, loss: 36.576828\n",
            "epoch 35, step 2209, loss: 0.000000\n",
            "epoch 35, step 2210, loss: 0.000003\n",
            "epoch 35, step 2211, loss: 0.000000\n",
            "epoch 35, step 2212, loss: 0.000000\n",
            "epoch 35, step 2213, loss: 0.000000\n",
            "epoch 35, step 2214, loss: 14.946895\n",
            "epoch 35, step 2215, loss: 7.155414\n",
            "epoch 35, step 2216, loss: 13.550120\n",
            "epoch 35, step 2217, loss: 0.000000\n",
            "epoch 35, step 2218, loss: 0.000000\n",
            "epoch 35, step 2219, loss: 0.000000\n",
            "epoch 35, step 2220, loss: 0.000000\n",
            "epoch 35, step 2221, loss: 0.000000\n",
            "epoch 35, step 2222, loss: 0.000000\n",
            "epoch 35, step 2223, loss: 0.000000\n",
            "epoch 35, step 2224, loss: 150.545654\n",
            "epoch 35, step 2225, loss: 0.000000\n",
            "epoch 35, step 2226, loss: 0.000000\n",
            "epoch 35, step 2227, loss: 0.000000\n",
            "epoch 35, step 2228, loss: 0.000001\n",
            "epoch 35, step 2229, loss: 0.000000\n",
            "epoch 35, step 2230, loss: 11.440530\n",
            "epoch 35, step 2231, loss: 0.000000\n",
            "epoch 35, step 2232, loss: 0.000019\n",
            "epoch 35, step 2233, loss: 0.000000\n",
            "epoch 35, step 2234, loss: 0.000000\n",
            "epoch 35, step 2235, loss: 0.000000\n",
            "epoch 35, step 2236, loss: 1.010879\n",
            "epoch 35, step 2237, loss: 0.000000\n",
            "epoch 35, step 2238, loss: 0.000000\n",
            "epoch 35, step 2239, loss: 0.000000\n",
            "epoch 35, step 2240, loss: 0.000000\n",
            "epoch 35, step 2241, loss: 0.000000\n",
            "epoch 35, step 2242, loss: 0.000000\n",
            "epoch 35, step 2243, loss: 0.000000\n",
            "epoch 35, step 2244, loss: 0.000000\n",
            "epoch 35, step 2245, loss: 0.000000\n",
            "epoch 35, step 2246, loss: 25.857109\n",
            "epoch 35, step 2247, loss: 0.000000\n",
            "epoch 35, step 2248, loss: 0.000000\n",
            "epoch 35, step 2249, loss: 0.006332\n",
            "epoch 35, step 2250, loss: 0.000243\n",
            "epoch 35, step 2251, loss: 0.000000\n",
            "epoch 35, step 2252, loss: 0.594874\n",
            "epoch 35, step 2253, loss: 0.000000\n",
            "epoch 35, step 2254, loss: 0.000000\n",
            "epoch 35, step 2255, loss: 0.000000\n",
            "epoch 35, step 2256, loss: 0.000000\n",
            "epoch 35, step 2257, loss: 0.000004\n",
            "epoch 35, step 2258, loss: 0.000000\n",
            "epoch 35, step 2259, loss: 0.000000\n",
            "epoch 35, step 2260, loss: 0.613287\n",
            "epoch 35, step 2261, loss: 0.000000\n",
            "epoch 35, step 2262, loss: 0.000002\n",
            "epoch 35, step 2263, loss: 178.373917\n",
            "epoch 35, step 2264, loss: 0.000008\n",
            "epoch 35, step 2265, loss: 0.000000\n",
            "epoch 35, step 2266, loss: 0.000000\n",
            "epoch 35, step 2267, loss: 56.483402\n",
            "epoch 36, step 2268, loss: 142.532303\n",
            "epoch 36, step 2269, loss: 0.000000\n",
            "epoch 36, step 2270, loss: 0.000000\n",
            "epoch 36, step 2271, loss: 96.556030\n",
            "epoch 36, step 2272, loss: 0.000086\n",
            "epoch 36, step 2273, loss: 0.000000\n",
            "epoch 36, step 2274, loss: 0.000000\n",
            "epoch 36, step 2275, loss: 0.000000\n",
            "epoch 36, step 2276, loss: 0.000000\n",
            "epoch 36, step 2277, loss: 2.490915\n",
            "epoch 36, step 2278, loss: 0.000007\n",
            "epoch 36, step 2279, loss: 38.461586\n",
            "epoch 36, step 2280, loss: 0.000000\n",
            "epoch 36, step 2281, loss: 0.000000\n",
            "epoch 36, step 2282, loss: 0.000000\n",
            "epoch 36, step 2283, loss: 0.000002\n",
            "epoch 36, step 2284, loss: 0.000000\n",
            "epoch 36, step 2285, loss: 0.000000\n",
            "epoch 36, step 2286, loss: 0.000000\n",
            "epoch 36, step 2287, loss: 144.175034\n",
            "epoch 36, step 2288, loss: 0.000000\n",
            "epoch 36, step 2289, loss: 0.000000\n",
            "epoch 36, step 2290, loss: 0.000000\n",
            "epoch 36, step 2291, loss: 0.000000\n",
            "epoch 36, step 2292, loss: 0.000000\n",
            "epoch 36, step 2293, loss: 2.176256\n",
            "epoch 36, step 2294, loss: 0.000000\n",
            "epoch 36, step 2295, loss: 0.000007\n",
            "epoch 36, step 2296, loss: 0.000004\n",
            "epoch 36, step 2297, loss: 0.000000\n",
            "epoch 36, step 2298, loss: 0.000000\n",
            "epoch 36, step 2299, loss: 0.000000\n",
            "epoch 36, step 2300, loss: 0.000000\n",
            "epoch 36, step 2301, loss: 0.000000\n",
            "epoch 36, step 2302, loss: 0.000000\n",
            "epoch 36, step 2303, loss: 0.000000\n",
            "epoch 36, step 2304, loss: 0.000000\n",
            "epoch 36, step 2305, loss: 0.000005\n",
            "epoch 36, step 2306, loss: 0.000000\n",
            "epoch 36, step 2307, loss: 0.000000\n",
            "epoch 36, step 2308, loss: 0.000000\n",
            "epoch 36, step 2309, loss: 14.866830\n",
            "epoch 36, step 2310, loss: 0.000000\n",
            "epoch 36, step 2311, loss: 0.000001\n",
            "epoch 36, step 2312, loss: 1.581154\n",
            "epoch 36, step 2313, loss: 0.000000\n",
            "epoch 36, step 2314, loss: 0.000000\n",
            "epoch 36, step 2315, loss: 0.001251\n",
            "epoch 36, step 2316, loss: 0.000000\n",
            "epoch 36, step 2317, loss: 0.000000\n",
            "epoch 36, step 2318, loss: 0.000000\n",
            "epoch 36, step 2319, loss: 0.100012\n",
            "epoch 36, step 2320, loss: 0.000518\n",
            "epoch 36, step 2321, loss: 0.000000\n",
            "epoch 36, step 2322, loss: 0.000000\n",
            "epoch 36, step 2323, loss: 0.000000\n",
            "epoch 36, step 2324, loss: 0.000000\n",
            "epoch 36, step 2325, loss: 16.872993\n",
            "epoch 36, step 2326, loss: 156.256241\n",
            "epoch 36, step 2327, loss: 0.000032\n",
            "epoch 36, step 2328, loss: 0.000000\n",
            "epoch 36, step 2329, loss: 0.000000\n",
            "epoch 36, step 2330, loss: 0.021513\n",
            "epoch 37, step 2331, loss: 0.000000\n",
            "epoch 37, step 2332, loss: 0.000000\n",
            "epoch 37, step 2333, loss: 0.000000\n",
            "epoch 37, step 2334, loss: 226.343292\n",
            "epoch 37, step 2335, loss: 0.000001\n",
            "epoch 37, step 2336, loss: 0.000000\n",
            "epoch 37, step 2337, loss: 0.000000\n",
            "epoch 37, step 2338, loss: 0.000000\n",
            "epoch 37, step 2339, loss: 0.000094\n",
            "epoch 37, step 2340, loss: 0.000000\n",
            "epoch 37, step 2341, loss: 20.262711\n",
            "epoch 37, step 2342, loss: 20.863720\n",
            "epoch 37, step 2343, loss: 0.000000\n",
            "epoch 37, step 2344, loss: 0.000000\n",
            "epoch 37, step 2345, loss: 0.000000\n",
            "epoch 37, step 2346, loss: 0.000007\n",
            "epoch 37, step 2347, loss: 0.000000\n",
            "epoch 37, step 2348, loss: 0.000000\n",
            "epoch 37, step 2349, loss: 0.000000\n",
            "epoch 37, step 2350, loss: 214.564133\n",
            "epoch 37, step 2351, loss: 0.000065\n",
            "epoch 37, step 2352, loss: 0.000000\n",
            "epoch 37, step 2353, loss: 0.000000\n",
            "epoch 37, step 2354, loss: 0.000150\n",
            "epoch 37, step 2355, loss: 0.000000\n",
            "epoch 37, step 2356, loss: 38.663116\n",
            "epoch 37, step 2357, loss: 0.000000\n",
            "epoch 37, step 2358, loss: 10.632489\n",
            "epoch 37, step 2359, loss: 8.146448\n",
            "epoch 37, step 2360, loss: 0.000000\n",
            "epoch 37, step 2361, loss: 0.000000\n",
            "epoch 37, step 2362, loss: 0.000000\n",
            "epoch 37, step 2363, loss: 0.000000\n",
            "epoch 37, step 2364, loss: 0.000000\n",
            "epoch 37, step 2365, loss: 0.000000\n",
            "epoch 37, step 2366, loss: 0.000000\n",
            "epoch 37, step 2367, loss: 0.000000\n",
            "epoch 37, step 2368, loss: 0.000000\n",
            "epoch 37, step 2369, loss: 0.000000\n",
            "epoch 37, step 2370, loss: 0.000000\n",
            "epoch 37, step 2371, loss: 0.000000\n",
            "epoch 37, step 2372, loss: 24.422621\n",
            "epoch 37, step 2373, loss: 0.000000\n",
            "epoch 37, step 2374, loss: 0.000000\n",
            "epoch 37, step 2375, loss: 0.000000\n",
            "epoch 37, step 2376, loss: 0.000000\n",
            "epoch 37, step 2377, loss: 0.000000\n",
            "epoch 37, step 2378, loss: 0.003146\n",
            "epoch 37, step 2379, loss: 0.000000\n",
            "epoch 37, step 2380, loss: 0.000000\n",
            "epoch 37, step 2381, loss: 0.000000\n",
            "epoch 37, step 2382, loss: 0.000000\n",
            "epoch 37, step 2383, loss: 0.000067\n",
            "epoch 37, step 2384, loss: 0.000000\n",
            "epoch 37, step 2385, loss: 0.000000\n",
            "epoch 37, step 2386, loss: 0.000000\n",
            "epoch 37, step 2387, loss: 0.000000\n",
            "epoch 37, step 2388, loss: 0.000017\n",
            "epoch 37, step 2389, loss: 163.327774\n",
            "epoch 37, step 2390, loss: 0.000278\n",
            "epoch 37, step 2391, loss: 0.000000\n",
            "epoch 37, step 2392, loss: 0.000000\n",
            "epoch 37, step 2393, loss: 71.048248\n",
            "epoch 38, step 2394, loss: 0.000000\n",
            "epoch 38, step 2395, loss: 0.000000\n",
            "epoch 38, step 2396, loss: 0.000000\n",
            "epoch 38, step 2397, loss: 39.084587\n",
            "epoch 38, step 2398, loss: 0.000011\n",
            "epoch 38, step 2399, loss: 0.000001\n",
            "epoch 38, step 2400, loss: 0.000000\n",
            "epoch 38, step 2401, loss: 0.000000\n",
            "epoch 38, step 2402, loss: 0.000018\n",
            "epoch 38, step 2403, loss: 26.240868\n",
            "epoch 38, step 2404, loss: 16.862226\n",
            "epoch 38, step 2405, loss: 21.551065\n",
            "epoch 38, step 2406, loss: 0.000000\n",
            "epoch 38, step 2407, loss: 0.000386\n",
            "epoch 38, step 2408, loss: 0.000109\n",
            "epoch 38, step 2409, loss: 0.345547\n",
            "epoch 38, step 2410, loss: 0.000000\n",
            "epoch 38, step 2411, loss: 0.000003\n",
            "epoch 38, step 2412, loss: 0.060771\n",
            "epoch 38, step 2413, loss: 124.355446\n",
            "epoch 38, step 2414, loss: 0.183568\n",
            "epoch 38, step 2415, loss: 0.000000\n",
            "epoch 38, step 2416, loss: 0.000000\n",
            "epoch 38, step 2417, loss: 1.506137\n",
            "epoch 38, step 2418, loss: 0.000000\n",
            "epoch 38, step 2419, loss: 61.435570\n",
            "epoch 38, step 2420, loss: 0.000000\n",
            "epoch 38, step 2421, loss: 0.001021\n",
            "epoch 38, step 2422, loss: 0.000263\n",
            "epoch 38, step 2423, loss: 0.000000\n",
            "epoch 38, step 2424, loss: 0.000000\n",
            "epoch 38, step 2425, loss: 0.000005\n",
            "epoch 38, step 2426, loss: 0.000000\n",
            "epoch 38, step 2427, loss: 0.000035\n",
            "epoch 38, step 2428, loss: 0.000000\n",
            "epoch 38, step 2429, loss: 0.000000\n",
            "epoch 38, step 2430, loss: 0.000000\n",
            "epoch 38, step 2431, loss: 0.000000\n",
            "epoch 38, step 2432, loss: 0.000000\n",
            "epoch 38, step 2433, loss: 0.000000\n",
            "epoch 38, step 2434, loss: 0.000000\n",
            "epoch 38, step 2435, loss: 17.379721\n",
            "epoch 38, step 2436, loss: 0.000000\n",
            "epoch 38, step 2437, loss: 0.000000\n",
            "epoch 38, step 2438, loss: 0.000000\n",
            "epoch 38, step 2439, loss: 0.000000\n",
            "epoch 38, step 2440, loss: 0.000000\n",
            "epoch 38, step 2441, loss: 0.000000\n",
            "epoch 38, step 2442, loss: 0.000000\n",
            "epoch 38, step 2443, loss: 0.000000\n",
            "epoch 38, step 2444, loss: 0.000000\n",
            "epoch 38, step 2445, loss: 0.004730\n",
            "epoch 38, step 2446, loss: 0.000000\n",
            "epoch 38, step 2447, loss: 0.000000\n",
            "epoch 38, step 2448, loss: 0.000000\n",
            "epoch 38, step 2449, loss: 0.000000\n",
            "epoch 38, step 2450, loss: 0.000000\n",
            "epoch 38, step 2451, loss: 3.119839\n",
            "epoch 38, step 2452, loss: 136.843781\n",
            "epoch 38, step 2453, loss: 0.000000\n",
            "epoch 38, step 2454, loss: 0.000000\n",
            "epoch 38, step 2455, loss: 0.000421\n",
            "epoch 38, step 2456, loss: 58.523853\n",
            "epoch 39, step 2457, loss: 157.836823\n",
            "epoch 39, step 2458, loss: 0.000001\n",
            "epoch 39, step 2459, loss: 0.000000\n",
            "epoch 39, step 2460, loss: 107.175232\n",
            "epoch 39, step 2461, loss: 0.000000\n",
            "epoch 39, step 2462, loss: 0.000000\n",
            "epoch 39, step 2463, loss: 0.000000\n",
            "epoch 39, step 2464, loss: 0.000000\n",
            "epoch 39, step 2465, loss: 0.000000\n",
            "epoch 39, step 2466, loss: 0.000000\n",
            "epoch 39, step 2467, loss: 0.000000\n",
            "epoch 39, step 2468, loss: 14.857150\n",
            "epoch 39, step 2469, loss: 0.000000\n",
            "epoch 39, step 2470, loss: 0.000000\n",
            "epoch 39, step 2471, loss: 0.000000\n",
            "epoch 39, step 2472, loss: 0.000000\n",
            "epoch 39, step 2473, loss: 0.000000\n",
            "epoch 39, step 2474, loss: 0.000000\n",
            "epoch 39, step 2475, loss: 0.000000\n",
            "epoch 39, step 2476, loss: 221.842438\n",
            "epoch 39, step 2477, loss: 0.000000\n",
            "epoch 39, step 2478, loss: 0.000000\n",
            "epoch 39, step 2479, loss: 0.000000\n",
            "epoch 39, step 2480, loss: 0.000000\n",
            "epoch 39, step 2481, loss: 0.000000\n",
            "epoch 39, step 2482, loss: 15.084338\n",
            "epoch 39, step 2483, loss: 0.000000\n",
            "epoch 39, step 2484, loss: 0.000079\n",
            "epoch 39, step 2485, loss: 0.000051\n",
            "epoch 39, step 2486, loss: 0.000000\n",
            "epoch 39, step 2487, loss: 0.000000\n",
            "epoch 39, step 2488, loss: 0.000000\n",
            "epoch 39, step 2489, loss: 0.000000\n",
            "epoch 39, step 2490, loss: 0.000000\n",
            "epoch 39, step 2491, loss: 0.000000\n",
            "epoch 39, step 2492, loss: 0.004064\n",
            "epoch 39, step 2493, loss: 0.000000\n",
            "epoch 39, step 2494, loss: 0.776906\n",
            "epoch 39, step 2495, loss: 0.000000\n",
            "epoch 39, step 2496, loss: 0.000005\n",
            "epoch 39, step 2497, loss: 0.000000\n",
            "epoch 39, step 2498, loss: 11.963106\n",
            "epoch 39, step 2499, loss: 0.000000\n",
            "epoch 39, step 2500, loss: 0.009481\n",
            "epoch 39, step 2501, loss: 0.000000\n",
            "epoch 39, step 2502, loss: 0.000000\n",
            "epoch 39, step 2503, loss: 0.000000\n",
            "epoch 39, step 2504, loss: 0.044540\n",
            "epoch 39, step 2505, loss: 0.000000\n",
            "epoch 39, step 2506, loss: 0.000000\n",
            "epoch 39, step 2507, loss: 0.000000\n",
            "epoch 39, step 2508, loss: 0.160433\n",
            "epoch 39, step 2509, loss: 0.011893\n",
            "epoch 39, step 2510, loss: 0.000000\n",
            "epoch 39, step 2511, loss: 0.000000\n",
            "epoch 39, step 2512, loss: 0.000000\n",
            "epoch 39, step 2513, loss: 0.000000\n",
            "epoch 39, step 2514, loss: 12.133282\n",
            "epoch 39, step 2515, loss: 162.667969\n",
            "epoch 39, step 2516, loss: 0.011200\n",
            "epoch 39, step 2517, loss: 0.000000\n",
            "epoch 39, step 2518, loss: 0.000000\n",
            "epoch 39, step 2519, loss: 3.825718\n",
            "epoch 40, step 2520, loss: 0.000000\n",
            "epoch 40, step 2521, loss: 0.000000\n",
            "epoch 40, step 2522, loss: 0.000000\n",
            "epoch 40, step 2523, loss: 230.551514\n",
            "epoch 40, step 2524, loss: 0.000000\n",
            "epoch 40, step 2525, loss: 0.000000\n",
            "epoch 40, step 2526, loss: 0.000000\n",
            "epoch 40, step 2527, loss: 0.000035\n",
            "epoch 40, step 2528, loss: 0.005733\n",
            "epoch 40, step 2529, loss: 0.000000\n",
            "epoch 40, step 2530, loss: 80.236206\n",
            "epoch 40, step 2531, loss: 25.167599\n",
            "epoch 40, step 2532, loss: 0.000000\n",
            "epoch 40, step 2533, loss: 0.011464\n",
            "epoch 40, step 2534, loss: 0.019161\n",
            "epoch 40, step 2535, loss: 7.774095\n",
            "epoch 40, step 2536, loss: 0.014135\n",
            "epoch 40, step 2537, loss: 0.000000\n",
            "epoch 40, step 2538, loss: 0.000005\n",
            "epoch 40, step 2539, loss: 184.137695\n",
            "epoch 40, step 2540, loss: 0.000003\n",
            "epoch 40, step 2541, loss: 0.000000\n",
            "epoch 40, step 2542, loss: 0.000000\n",
            "epoch 40, step 2543, loss: 0.006564\n",
            "epoch 40, step 2544, loss: 0.021377\n",
            "epoch 40, step 2545, loss: 67.737968\n",
            "epoch 40, step 2546, loss: 0.000000\n",
            "epoch 40, step 2547, loss: 0.000000\n",
            "epoch 40, step 2548, loss: 0.000002\n",
            "epoch 40, step 2549, loss: 0.000000\n",
            "epoch 40, step 2550, loss: 0.001371\n",
            "epoch 40, step 2551, loss: 0.000689\n",
            "epoch 40, step 2552, loss: 0.000000\n",
            "epoch 40, step 2553, loss: 0.000007\n",
            "epoch 40, step 2554, loss: 0.064460\n",
            "epoch 40, step 2555, loss: 0.000000\n",
            "epoch 40, step 2556, loss: 0.000000\n",
            "epoch 40, step 2557, loss: 0.000048\n",
            "epoch 40, step 2558, loss: 0.000000\n",
            "epoch 40, step 2559, loss: 0.000000\n",
            "epoch 40, step 2560, loss: 0.000000\n",
            "epoch 40, step 2561, loss: 39.107674\n",
            "epoch 40, step 2562, loss: 0.000000\n",
            "epoch 40, step 2563, loss: 0.000000\n",
            "epoch 40, step 2564, loss: 0.000000\n",
            "epoch 40, step 2565, loss: 0.000000\n",
            "epoch 40, step 2566, loss: 0.000000\n",
            "epoch 40, step 2567, loss: 0.000000\n",
            "epoch 40, step 2568, loss: 0.000000\n",
            "epoch 40, step 2569, loss: 0.000000\n",
            "epoch 40, step 2570, loss: 0.000031\n",
            "epoch 40, step 2571, loss: 6.634418\n",
            "epoch 40, step 2572, loss: 0.000000\n",
            "epoch 40, step 2573, loss: 0.000000\n",
            "epoch 40, step 2574, loss: 0.000222\n",
            "epoch 40, step 2575, loss: 0.000000\n",
            "epoch 40, step 2576, loss: 0.003234\n",
            "epoch 40, step 2577, loss: 20.813061\n",
            "epoch 40, step 2578, loss: 134.690414\n",
            "epoch 40, step 2579, loss: 0.000000\n",
            "epoch 40, step 2580, loss: 0.000000\n",
            "epoch 40, step 2581, loss: 0.004854\n",
            "epoch 40, step 2582, loss: 0.008585\n",
            "epoch 41, step 2583, loss: 0.000000\n",
            "epoch 41, step 2584, loss: 0.000251\n",
            "epoch 41, step 2585, loss: 0.000004\n",
            "epoch 41, step 2586, loss: 18.289555\n",
            "epoch 41, step 2587, loss: 0.001218\n",
            "epoch 41, step 2588, loss: 0.000018\n",
            "epoch 41, step 2589, loss: 0.000000\n",
            "epoch 41, step 2590, loss: 0.240789\n",
            "epoch 41, step 2591, loss: 0.000005\n",
            "epoch 41, step 2592, loss: 0.000000\n",
            "epoch 41, step 2593, loss: 20.852280\n",
            "epoch 41, step 2594, loss: 35.353802\n",
            "epoch 41, step 2595, loss: 0.000000\n",
            "epoch 41, step 2596, loss: 0.000583\n",
            "epoch 41, step 2597, loss: 0.000052\n",
            "epoch 41, step 2598, loss: 0.000026\n",
            "epoch 41, step 2599, loss: 0.000000\n",
            "epoch 41, step 2600, loss: 0.000000\n",
            "epoch 41, step 2601, loss: 0.000000\n",
            "epoch 41, step 2602, loss: 169.304230\n",
            "epoch 41, step 2603, loss: 0.000000\n",
            "epoch 41, step 2604, loss: 0.000000\n",
            "epoch 41, step 2605, loss: 0.000000\n",
            "epoch 41, step 2606, loss: 0.000105\n",
            "epoch 41, step 2607, loss: 0.000000\n",
            "epoch 41, step 2608, loss: 50.254879\n",
            "epoch 41, step 2609, loss: 0.000000\n",
            "epoch 41, step 2610, loss: 0.000014\n",
            "epoch 41, step 2611, loss: 0.000003\n",
            "epoch 41, step 2612, loss: 0.000000\n",
            "epoch 41, step 2613, loss: 0.000000\n",
            "epoch 41, step 2614, loss: 0.000013\n",
            "epoch 41, step 2615, loss: 0.000000\n",
            "epoch 41, step 2616, loss: 0.000001\n",
            "epoch 41, step 2617, loss: 0.000000\n",
            "epoch 41, step 2618, loss: 0.000000\n",
            "epoch 41, step 2619, loss: 0.000000\n",
            "epoch 41, step 2620, loss: 0.000609\n",
            "epoch 41, step 2621, loss: 0.000000\n",
            "epoch 41, step 2622, loss: 0.000000\n",
            "epoch 41, step 2623, loss: 0.000000\n",
            "epoch 41, step 2624, loss: 23.158819\n",
            "epoch 41, step 2625, loss: 0.000000\n",
            "epoch 41, step 2626, loss: 0.000000\n",
            "epoch 41, step 2627, loss: 0.000000\n",
            "epoch 41, step 2628, loss: 0.000000\n",
            "epoch 41, step 2629, loss: 0.000000\n",
            "epoch 41, step 2630, loss: 0.000000\n",
            "epoch 41, step 2631, loss: 0.000000\n",
            "epoch 41, step 2632, loss: 0.000000\n",
            "epoch 41, step 2633, loss: 0.000000\n",
            "epoch 41, step 2634, loss: 0.019237\n",
            "epoch 41, step 2635, loss: 0.000000\n",
            "epoch 41, step 2636, loss: 0.000000\n",
            "epoch 41, step 2637, loss: 0.000001\n",
            "epoch 41, step 2638, loss: 0.000000\n",
            "epoch 41, step 2639, loss: 0.000001\n",
            "epoch 41, step 2640, loss: 20.081753\n",
            "epoch 41, step 2641, loss: 135.840195\n",
            "epoch 41, step 2642, loss: 0.000001\n",
            "epoch 41, step 2643, loss: 0.000000\n",
            "epoch 41, step 2644, loss: 0.004508\n",
            "epoch 41, step 2645, loss: 89.915413\n",
            "epoch 42, step 2646, loss: 164.661835\n",
            "epoch 42, step 2647, loss: 0.000001\n",
            "epoch 42, step 2648, loss: 0.000000\n",
            "epoch 42, step 2649, loss: 113.506248\n",
            "epoch 42, step 2650, loss: 0.000137\n",
            "epoch 42, step 2651, loss: 0.000000\n",
            "epoch 42, step 2652, loss: 0.000000\n",
            "epoch 42, step 2653, loss: 0.000000\n",
            "epoch 42, step 2654, loss: 0.000000\n",
            "epoch 42, step 2655, loss: 0.000000\n",
            "epoch 42, step 2656, loss: 0.000000\n",
            "epoch 42, step 2657, loss: 35.236782\n",
            "epoch 42, step 2658, loss: 0.000000\n",
            "epoch 42, step 2659, loss: 0.000000\n",
            "epoch 42, step 2660, loss: 0.000000\n",
            "epoch 42, step 2661, loss: 0.249028\n",
            "epoch 42, step 2662, loss: 0.000000\n",
            "epoch 42, step 2663, loss: 0.000000\n",
            "epoch 42, step 2664, loss: 0.000000\n",
            "epoch 42, step 2665, loss: 185.780365\n",
            "epoch 42, step 2666, loss: 0.000000\n",
            "epoch 42, step 2667, loss: 0.000000\n",
            "epoch 42, step 2668, loss: 0.000000\n",
            "epoch 42, step 2669, loss: 0.000000\n",
            "epoch 42, step 2670, loss: 0.000000\n",
            "epoch 42, step 2671, loss: 37.831066\n",
            "epoch 42, step 2672, loss: 0.000000\n",
            "epoch 42, step 2673, loss: 8.208570\n",
            "epoch 42, step 2674, loss: 5.420877\n",
            "epoch 42, step 2675, loss: 0.000000\n",
            "epoch 42, step 2676, loss: 0.000000\n",
            "epoch 42, step 2677, loss: 0.000000\n",
            "epoch 42, step 2678, loss: 0.000000\n",
            "epoch 42, step 2679, loss: 0.000000\n",
            "epoch 42, step 2680, loss: 0.000000\n",
            "epoch 42, step 2681, loss: 0.000000\n",
            "epoch 42, step 2682, loss: 0.000000\n",
            "epoch 42, step 2683, loss: 0.000000\n",
            "epoch 42, step 2684, loss: 0.000000\n",
            "epoch 42, step 2685, loss: 0.000000\n",
            "epoch 42, step 2686, loss: 0.000000\n",
            "epoch 42, step 2687, loss: 30.369928\n",
            "epoch 42, step 2688, loss: 0.000000\n",
            "epoch 42, step 2689, loss: 0.000188\n",
            "epoch 42, step 2690, loss: 0.000000\n",
            "epoch 42, step 2691, loss: 0.000000\n",
            "epoch 42, step 2692, loss: 0.000000\n",
            "epoch 42, step 2693, loss: 0.002320\n",
            "epoch 42, step 2694, loss: 0.000000\n",
            "epoch 42, step 2695, loss: 0.000000\n",
            "epoch 42, step 2696, loss: 0.000000\n",
            "epoch 42, step 2697, loss: 0.000000\n",
            "epoch 42, step 2698, loss: 0.004940\n",
            "epoch 42, step 2699, loss: 0.000000\n",
            "epoch 42, step 2700, loss: 0.000000\n",
            "epoch 42, step 2701, loss: 0.000000\n",
            "epoch 42, step 2702, loss: 0.000000\n",
            "epoch 42, step 2703, loss: 0.000000\n",
            "epoch 42, step 2704, loss: 173.573730\n",
            "epoch 42, step 2705, loss: 0.001675\n",
            "epoch 42, step 2706, loss: 0.000000\n",
            "epoch 42, step 2707, loss: 0.000000\n",
            "epoch 42, step 2708, loss: 23.192476\n",
            "epoch 43, step 2709, loss: 0.000000\n",
            "epoch 43, step 2710, loss: 0.000000\n",
            "epoch 43, step 2711, loss: 0.000000\n",
            "epoch 43, step 2712, loss: 229.698318\n",
            "epoch 43, step 2713, loss: 0.000000\n",
            "epoch 43, step 2714, loss: 0.000001\n",
            "epoch 43, step 2715, loss: 0.000000\n",
            "epoch 43, step 2716, loss: 0.000002\n",
            "epoch 43, step 2717, loss: 0.000014\n",
            "epoch 43, step 2718, loss: 20.367750\n",
            "epoch 43, step 2719, loss: 86.953949\n",
            "epoch 43, step 2720, loss: 29.677135\n",
            "epoch 43, step 2721, loss: 0.000000\n",
            "epoch 43, step 2722, loss: 0.003871\n",
            "epoch 43, step 2723, loss: 0.000247\n",
            "epoch 43, step 2724, loss: 0.000771\n",
            "epoch 43, step 2725, loss: 0.000000\n",
            "epoch 43, step 2726, loss: 0.000000\n",
            "epoch 43, step 2727, loss: 0.000000\n",
            "epoch 43, step 2728, loss: 130.366653\n",
            "epoch 43, step 2729, loss: 0.000000\n",
            "epoch 43, step 2730, loss: 0.000000\n",
            "epoch 43, step 2731, loss: 0.000000\n",
            "epoch 43, step 2732, loss: 0.000000\n",
            "epoch 43, step 2733, loss: 0.000000\n",
            "epoch 43, step 2734, loss: 51.600403\n",
            "epoch 43, step 2735, loss: 0.000000\n",
            "epoch 43, step 2736, loss: 0.003308\n",
            "epoch 43, step 2737, loss: 0.001899\n",
            "epoch 43, step 2738, loss: 0.000000\n",
            "epoch 43, step 2739, loss: 0.000000\n",
            "epoch 43, step 2740, loss: 0.000000\n",
            "epoch 43, step 2741, loss: 0.000000\n",
            "epoch 43, step 2742, loss: 0.000000\n",
            "epoch 43, step 2743, loss: 0.000000\n",
            "epoch 43, step 2744, loss: 0.000000\n",
            "epoch 43, step 2745, loss: 0.000000\n",
            "epoch 43, step 2746, loss: 0.000000\n",
            "epoch 43, step 2747, loss: 0.000000\n",
            "epoch 43, step 2748, loss: 0.000059\n",
            "epoch 43, step 2749, loss: 0.000000\n",
            "epoch 43, step 2750, loss: 10.564533\n",
            "epoch 43, step 2751, loss: 0.000000\n",
            "epoch 43, step 2752, loss: 0.000000\n",
            "epoch 43, step 2753, loss: 0.000000\n",
            "epoch 43, step 2754, loss: 0.000000\n",
            "epoch 43, step 2755, loss: 0.000000\n",
            "epoch 43, step 2756, loss: 0.000000\n",
            "epoch 43, step 2757, loss: 0.000000\n",
            "epoch 43, step 2758, loss: 0.000000\n",
            "epoch 43, step 2759, loss: 0.000000\n",
            "epoch 43, step 2760, loss: 0.742049\n",
            "epoch 43, step 2761, loss: 0.000001\n",
            "epoch 43, step 2762, loss: 0.000000\n",
            "epoch 43, step 2763, loss: 0.000000\n",
            "epoch 43, step 2764, loss: 0.000000\n",
            "epoch 43, step 2765, loss: 0.000000\n",
            "epoch 43, step 2766, loss: 0.006245\n",
            "epoch 43, step 2767, loss: 137.945435\n",
            "epoch 43, step 2768, loss: 0.000000\n",
            "epoch 43, step 2769, loss: 0.000000\n",
            "epoch 43, step 2770, loss: 0.000003\n",
            "epoch 43, step 2771, loss: 0.000964\n",
            "epoch 44, step 2772, loss: 0.000000\n",
            "epoch 44, step 2773, loss: 0.000005\n",
            "epoch 44, step 2774, loss: 0.000000\n",
            "epoch 44, step 2775, loss: 20.689106\n",
            "epoch 44, step 2776, loss: 0.000141\n",
            "epoch 44, step 2777, loss: 0.000003\n",
            "epoch 44, step 2778, loss: 0.000000\n",
            "epoch 44, step 2779, loss: 0.000001\n",
            "epoch 44, step 2780, loss: 0.000076\n",
            "epoch 44, step 2781, loss: 13.104592\n",
            "epoch 44, step 2782, loss: 31.170835\n",
            "epoch 44, step 2783, loss: 21.070171\n",
            "epoch 44, step 2784, loss: 0.000000\n",
            "epoch 44, step 2785, loss: 3.118158\n",
            "epoch 44, step 2786, loss: 0.001454\n",
            "epoch 44, step 2787, loss: 0.000007\n",
            "epoch 44, step 2788, loss: 0.000000\n",
            "epoch 44, step 2789, loss: 0.000000\n",
            "epoch 44, step 2790, loss: 0.000000\n",
            "epoch 44, step 2791, loss: 141.899261\n",
            "epoch 44, step 2792, loss: 0.000000\n",
            "epoch 44, step 2793, loss: 0.000000\n",
            "epoch 44, step 2794, loss: 0.000000\n",
            "epoch 44, step 2795, loss: 0.000000\n",
            "epoch 44, step 2796, loss: 0.000000\n",
            "epoch 44, step 2797, loss: 54.367279\n",
            "epoch 44, step 2798, loss: 0.000000\n",
            "epoch 44, step 2799, loss: 0.039268\n",
            "epoch 44, step 2800, loss: 0.077347\n",
            "epoch 44, step 2801, loss: 0.000000\n",
            "epoch 44, step 2802, loss: 0.000000\n",
            "epoch 44, step 2803, loss: 0.000000\n",
            "epoch 44, step 2804, loss: 0.000000\n",
            "epoch 44, step 2805, loss: 0.000001\n",
            "epoch 44, step 2806, loss: 0.000000\n",
            "epoch 44, step 2807, loss: 0.000000\n",
            "epoch 44, step 2808, loss: 0.000000\n",
            "epoch 44, step 2809, loss: 0.000002\n",
            "epoch 44, step 2810, loss: 0.000000\n",
            "epoch 44, step 2811, loss: 0.026866\n",
            "epoch 44, step 2812, loss: 0.000000\n",
            "epoch 44, step 2813, loss: 4.246189\n",
            "epoch 44, step 2814, loss: 0.000000\n",
            "epoch 44, step 2815, loss: 0.000000\n",
            "epoch 44, step 2816, loss: 0.000000\n",
            "epoch 44, step 2817, loss: 0.000000\n",
            "epoch 44, step 2818, loss: 0.000000\n",
            "epoch 44, step 2819, loss: 0.000000\n",
            "epoch 44, step 2820, loss: 0.000000\n",
            "epoch 44, step 2821, loss: 0.000000\n",
            "epoch 44, step 2822, loss: 0.000000\n",
            "epoch 44, step 2823, loss: 7.810800\n",
            "epoch 44, step 2824, loss: 0.000000\n",
            "epoch 44, step 2825, loss: 0.000015\n",
            "epoch 44, step 2826, loss: 0.000000\n",
            "epoch 44, step 2827, loss: 0.000000\n",
            "epoch 44, step 2828, loss: 0.000000\n",
            "epoch 44, step 2829, loss: 17.755409\n",
            "epoch 44, step 2830, loss: 163.130768\n",
            "epoch 44, step 2831, loss: 0.001792\n",
            "epoch 44, step 2832, loss: 0.000000\n",
            "epoch 44, step 2833, loss: 0.000004\n",
            "epoch 44, step 2834, loss: 97.768692\n",
            "epoch 45, step 2835, loss: 163.505890\n",
            "epoch 45, step 2836, loss: 0.000000\n",
            "epoch 45, step 2837, loss: 0.000000\n",
            "epoch 45, step 2838, loss: 109.586220\n",
            "epoch 45, step 2839, loss: 0.000017\n",
            "epoch 45, step 2840, loss: 0.084491\n",
            "epoch 45, step 2841, loss: 0.000000\n",
            "epoch 45, step 2842, loss: 0.000000\n",
            "epoch 45, step 2843, loss: 0.000000\n",
            "epoch 45, step 2844, loss: 0.000005\n",
            "epoch 45, step 2845, loss: 0.000000\n",
            "epoch 45, step 2846, loss: 13.943440\n",
            "epoch 45, step 2847, loss: 0.000000\n",
            "epoch 45, step 2848, loss: 0.000000\n",
            "epoch 45, step 2849, loss: 2.026863\n",
            "epoch 45, step 2850, loss: 0.000000\n",
            "epoch 45, step 2851, loss: 0.000000\n",
            "epoch 45, step 2852, loss: 0.000000\n",
            "epoch 45, step 2853, loss: 0.000000\n",
            "epoch 45, step 2854, loss: 196.213120\n",
            "epoch 45, step 2855, loss: 0.000000\n",
            "epoch 45, step 2856, loss: 0.000000\n",
            "epoch 45, step 2857, loss: 0.000000\n",
            "epoch 45, step 2858, loss: 0.000000\n",
            "epoch 45, step 2859, loss: 0.000000\n",
            "epoch 45, step 2860, loss: 23.816246\n",
            "epoch 45, step 2861, loss: 0.000000\n",
            "epoch 45, step 2862, loss: 0.000008\n",
            "epoch 45, step 2863, loss: 0.000000\n",
            "epoch 45, step 2864, loss: 0.000000\n",
            "epoch 45, step 2865, loss: 0.000000\n",
            "epoch 45, step 2866, loss: 0.000000\n",
            "epoch 45, step 2867, loss: 0.000000\n",
            "epoch 45, step 2868, loss: 0.000000\n",
            "epoch 45, step 2869, loss: 0.000000\n",
            "epoch 45, step 2870, loss: 0.000000\n",
            "epoch 45, step 2871, loss: 0.000000\n",
            "epoch 45, step 2872, loss: 0.000000\n",
            "epoch 45, step 2873, loss: 0.000000\n",
            "epoch 45, step 2874, loss: 0.000000\n",
            "epoch 45, step 2875, loss: 0.000002\n",
            "epoch 45, step 2876, loss: 19.991339\n",
            "epoch 45, step 2877, loss: 0.000000\n",
            "epoch 45, step 2878, loss: 0.000000\n",
            "epoch 45, step 2879, loss: 0.000000\n",
            "epoch 45, step 2880, loss: 0.000000\n",
            "epoch 45, step 2881, loss: 0.000000\n",
            "epoch 45, step 2882, loss: 0.000011\n",
            "epoch 45, step 2883, loss: 0.000000\n",
            "epoch 45, step 2884, loss: 0.000000\n",
            "epoch 45, step 2885, loss: 0.000000\n",
            "epoch 45, step 2886, loss: 0.011504\n",
            "epoch 45, step 2887, loss: 0.003273\n",
            "epoch 45, step 2888, loss: 0.000000\n",
            "epoch 45, step 2889, loss: 0.000003\n",
            "epoch 45, step 2890, loss: 0.000000\n",
            "epoch 45, step 2891, loss: 0.000007\n",
            "epoch 45, step 2892, loss: 13.598154\n",
            "epoch 45, step 2893, loss: 155.174118\n",
            "epoch 45, step 2894, loss: 0.003950\n",
            "epoch 45, step 2895, loss: 0.000000\n",
            "epoch 45, step 2896, loss: 5.725675\n",
            "epoch 45, step 2897, loss: 22.241938\n",
            "epoch 46, step 2898, loss: 0.000000\n",
            "epoch 46, step 2899, loss: 0.000000\n",
            "epoch 46, step 2900, loss: 0.000016\n",
            "epoch 46, step 2901, loss: 262.402618\n",
            "epoch 46, step 2902, loss: 0.000010\n",
            "epoch 46, step 2903, loss: 0.000000\n",
            "epoch 46, step 2904, loss: 0.000000\n",
            "epoch 46, step 2905, loss: 0.000001\n",
            "epoch 46, step 2906, loss: 0.000000\n",
            "epoch 46, step 2907, loss: 0.000000\n",
            "epoch 46, step 2908, loss: 70.100143\n",
            "epoch 46, step 2909, loss: 37.559654\n",
            "epoch 46, step 2910, loss: 0.000000\n",
            "epoch 46, step 2911, loss: 0.000000\n",
            "epoch 46, step 2912, loss: 0.000006\n",
            "epoch 46, step 2913, loss: 1.584802\n",
            "epoch 46, step 2914, loss: 0.000000\n",
            "epoch 46, step 2915, loss: 0.000000\n",
            "epoch 46, step 2916, loss: 0.000001\n",
            "epoch 46, step 2917, loss: 69.661583\n",
            "epoch 46, step 2918, loss: 0.000003\n",
            "epoch 46, step 2919, loss: 0.000099\n",
            "epoch 46, step 2920, loss: 0.000000\n",
            "epoch 46, step 2921, loss: 0.000000\n",
            "epoch 46, step 2922, loss: 0.000000\n",
            "epoch 46, step 2923, loss: 43.840466\n",
            "epoch 46, step 2924, loss: 0.000000\n",
            "epoch 46, step 2925, loss: 0.005319\n",
            "epoch 46, step 2926, loss: 0.002315\n",
            "epoch 46, step 2927, loss: 0.000000\n",
            "epoch 46, step 2928, loss: 0.000000\n",
            "epoch 46, step 2929, loss: 0.000003\n",
            "epoch 46, step 2930, loss: 0.000000\n",
            "epoch 46, step 2931, loss: 0.000000\n",
            "epoch 46, step 2932, loss: 0.000000\n",
            "epoch 46, step 2933, loss: 0.000000\n",
            "epoch 46, step 2934, loss: 0.000000\n",
            "epoch 46, step 2935, loss: 0.000000\n",
            "epoch 46, step 2936, loss: 0.000000\n",
            "epoch 46, step 2937, loss: 0.000012\n",
            "epoch 46, step 2938, loss: 0.000000\n",
            "epoch 46, step 2939, loss: 11.800286\n",
            "epoch 46, step 2940, loss: 0.000000\n",
            "epoch 46, step 2941, loss: 0.000000\n",
            "epoch 46, step 2942, loss: 0.000000\n",
            "epoch 46, step 2943, loss: 0.000000\n",
            "epoch 46, step 2944, loss: 0.000000\n",
            "epoch 46, step 2945, loss: 0.000000\n",
            "epoch 46, step 2946, loss: 0.000000\n",
            "epoch 46, step 2947, loss: 0.000000\n",
            "epoch 46, step 2948, loss: 0.000000\n",
            "epoch 46, step 2949, loss: 2.925308\n",
            "epoch 46, step 2950, loss: 0.000000\n",
            "epoch 46, step 2951, loss: 0.000000\n",
            "epoch 46, step 2952, loss: 0.000004\n",
            "epoch 46, step 2953, loss: 0.000000\n",
            "epoch 46, step 2954, loss: 0.071458\n",
            "epoch 46, step 2955, loss: 9.598190\n",
            "epoch 46, step 2956, loss: 112.305275\n",
            "epoch 46, step 2957, loss: 0.009366\n",
            "epoch 46, step 2958, loss: 0.000001\n",
            "epoch 46, step 2959, loss: 0.000003\n",
            "epoch 46, step 2960, loss: 6.585939\n",
            "epoch 47, step 2961, loss: 0.000000\n",
            "epoch 47, step 2962, loss: 0.000224\n",
            "epoch 47, step 2963, loss: 0.000021\n",
            "epoch 47, step 2964, loss: 41.992687\n",
            "epoch 47, step 2965, loss: 0.000000\n",
            "epoch 47, step 2966, loss: 0.000001\n",
            "epoch 47, step 2967, loss: 0.000002\n",
            "epoch 47, step 2968, loss: 1.550225\n",
            "epoch 47, step 2969, loss: 0.000000\n",
            "epoch 47, step 2970, loss: 0.000000\n",
            "epoch 47, step 2971, loss: 94.670830\n",
            "epoch 47, step 2972, loss: 35.745651\n",
            "epoch 47, step 2973, loss: 0.000000\n",
            "epoch 47, step 2974, loss: 0.000000\n",
            "epoch 47, step 2975, loss: 0.000014\n",
            "epoch 47, step 2976, loss: 0.000232\n",
            "epoch 47, step 2977, loss: 0.000000\n",
            "epoch 47, step 2978, loss: 0.000000\n",
            "epoch 47, step 2979, loss: 0.000000\n",
            "epoch 47, step 2980, loss: 57.933334\n",
            "epoch 47, step 2981, loss: 0.000000\n",
            "epoch 47, step 2982, loss: 12.295077\n",
            "epoch 47, step 2983, loss: 0.000000\n",
            "epoch 47, step 2984, loss: 0.000000\n",
            "epoch 47, step 2985, loss: 0.000000\n",
            "epoch 47, step 2986, loss: 55.711349\n",
            "epoch 47, step 2987, loss: 0.000000\n",
            "epoch 47, step 2988, loss: 0.247720\n",
            "epoch 47, step 2989, loss: 0.148282\n",
            "epoch 47, step 2990, loss: 0.000000\n",
            "epoch 47, step 2991, loss: 7.716769\n",
            "epoch 47, step 2992, loss: 0.000002\n",
            "epoch 47, step 2993, loss: 0.000000\n",
            "epoch 47, step 2994, loss: 0.000000\n",
            "epoch 47, step 2995, loss: 0.000000\n",
            "epoch 47, step 2996, loss: 0.000000\n",
            "epoch 47, step 2997, loss: 0.000000\n",
            "epoch 47, step 2998, loss: 0.000208\n",
            "epoch 47, step 2999, loss: 0.000000\n",
            "epoch 47, step 3000, loss: 0.000000\n",
            "epoch 47, step 3001, loss: 0.000229\n",
            "epoch 47, step 3002, loss: 16.324545\n",
            "epoch 47, step 3003, loss: 5.685147\n",
            "epoch 47, step 3004, loss: 0.000000\n",
            "epoch 47, step 3005, loss: 0.000000\n",
            "epoch 47, step 3006, loss: 0.000000\n",
            "epoch 47, step 3007, loss: 0.000000\n",
            "epoch 47, step 3008, loss: 0.000000\n",
            "epoch 47, step 3009, loss: 0.000000\n",
            "epoch 47, step 3010, loss: 0.000000\n",
            "epoch 47, step 3011, loss: 0.000000\n",
            "epoch 47, step 3012, loss: 0.016788\n",
            "epoch 47, step 3013, loss: 0.000000\n",
            "epoch 47, step 3014, loss: 0.000000\n",
            "epoch 47, step 3015, loss: 1.436466\n",
            "epoch 47, step 3016, loss: 0.000058\n",
            "epoch 47, step 3017, loss: 0.000046\n",
            "epoch 47, step 3018, loss: 35.550064\n",
            "epoch 47, step 3019, loss: 89.272491\n",
            "epoch 47, step 3020, loss: 0.000339\n",
            "epoch 47, step 3021, loss: 0.000000\n",
            "epoch 47, step 3022, loss: 0.000074\n",
            "epoch 47, step 3023, loss: 51.099758\n",
            "epoch 48, step 3024, loss: 166.210434\n",
            "epoch 48, step 3025, loss: 0.000182\n",
            "epoch 48, step 3026, loss: 0.000000\n",
            "epoch 48, step 3027, loss: 112.549896\n",
            "epoch 48, step 3028, loss: 0.000000\n",
            "epoch 48, step 3029, loss: 0.000000\n",
            "epoch 48, step 3030, loss: 0.000000\n",
            "epoch 48, step 3031, loss: 0.000000\n",
            "epoch 48, step 3032, loss: 0.000000\n",
            "epoch 48, step 3033, loss: 0.000000\n",
            "epoch 48, step 3034, loss: 60.447941\n",
            "epoch 48, step 3035, loss: 40.653172\n",
            "epoch 48, step 3036, loss: 0.000000\n",
            "epoch 48, step 3037, loss: 0.000000\n",
            "epoch 48, step 3038, loss: 28.069038\n",
            "epoch 48, step 3039, loss: 0.000014\n",
            "epoch 48, step 3040, loss: 0.000000\n",
            "epoch 48, step 3041, loss: 0.000000\n",
            "epoch 48, step 3042, loss: 0.000002\n",
            "epoch 48, step 3043, loss: 76.337349\n",
            "epoch 48, step 3044, loss: 0.000023\n",
            "epoch 48, step 3045, loss: 0.000000\n",
            "epoch 48, step 3046, loss: 0.000000\n",
            "epoch 48, step 3047, loss: 0.000009\n",
            "epoch 48, step 3048, loss: 0.000000\n",
            "epoch 48, step 3049, loss: 26.707550\n",
            "epoch 48, step 3050, loss: 0.000000\n",
            "epoch 48, step 3051, loss: 0.092476\n",
            "epoch 48, step 3052, loss: 0.059396\n",
            "epoch 48, step 3053, loss: 0.000000\n",
            "epoch 48, step 3054, loss: 0.000000\n",
            "epoch 48, step 3055, loss: 0.000000\n",
            "epoch 48, step 3056, loss: 0.000000\n",
            "epoch 48, step 3057, loss: 0.000000\n",
            "epoch 48, step 3058, loss: 0.000000\n",
            "epoch 48, step 3059, loss: 0.010370\n",
            "epoch 48, step 3060, loss: 0.000000\n",
            "epoch 48, step 3061, loss: 0.000000\n",
            "epoch 48, step 3062, loss: 0.000000\n",
            "epoch 48, step 3063, loss: 0.000023\n",
            "epoch 48, step 3064, loss: 0.000105\n",
            "epoch 48, step 3065, loss: 11.586578\n",
            "epoch 48, step 3066, loss: 0.000000\n",
            "epoch 48, step 3067, loss: 0.000000\n",
            "epoch 48, step 3068, loss: 0.000000\n",
            "epoch 48, step 3069, loss: 0.000000\n",
            "epoch 48, step 3070, loss: 0.000000\n",
            "epoch 48, step 3071, loss: 0.000000\n",
            "epoch 48, step 3072, loss: 0.000000\n",
            "epoch 48, step 3073, loss: 0.000000\n",
            "epoch 48, step 3074, loss: 0.000000\n",
            "epoch 48, step 3075, loss: 2.021927\n",
            "epoch 48, step 3076, loss: 0.000000\n",
            "epoch 48, step 3077, loss: 0.000000\n",
            "epoch 48, step 3078, loss: 0.000000\n",
            "epoch 48, step 3079, loss: 0.056473\n",
            "epoch 48, step 3080, loss: 0.469054\n",
            "epoch 48, step 3081, loss: 9.506647\n",
            "epoch 48, step 3082, loss: 125.279762\n",
            "epoch 48, step 3083, loss: 0.000000\n",
            "epoch 48, step 3084, loss: 0.000000\n",
            "epoch 48, step 3085, loss: 0.000000\n",
            "epoch 48, step 3086, loss: 73.320076\n",
            "epoch 49, step 3087, loss: 0.000000\n",
            "epoch 49, step 3088, loss: 0.000001\n",
            "epoch 49, step 3089, loss: 0.000000\n",
            "epoch 49, step 3090, loss: 238.050812\n",
            "epoch 49, step 3091, loss: 0.040800\n",
            "epoch 49, step 3092, loss: 0.000502\n",
            "epoch 49, step 3093, loss: 0.000023\n",
            "epoch 49, step 3094, loss: 0.000000\n",
            "epoch 49, step 3095, loss: 0.000001\n",
            "epoch 49, step 3096, loss: 0.000000\n",
            "epoch 49, step 3097, loss: 79.913864\n",
            "epoch 49, step 3098, loss: 36.799213\n",
            "epoch 49, step 3099, loss: 0.000000\n",
            "epoch 49, step 3100, loss: 0.000000\n",
            "epoch 49, step 3101, loss: 0.001075\n",
            "epoch 49, step 3102, loss: 0.000305\n",
            "epoch 49, step 3103, loss: 0.163545\n",
            "epoch 49, step 3104, loss: 0.011995\n",
            "epoch 49, step 3105, loss: 5.925443\n",
            "epoch 49, step 3106, loss: 62.434879\n",
            "epoch 49, step 3107, loss: 0.000160\n",
            "epoch 49, step 3108, loss: 0.000000\n",
            "epoch 49, step 3109, loss: 0.000000\n",
            "epoch 49, step 3110, loss: 0.002001\n",
            "epoch 49, step 3111, loss: 0.000000\n",
            "epoch 49, step 3112, loss: 34.869286\n",
            "epoch 49, step 3113, loss: 0.000000\n",
            "epoch 49, step 3114, loss: 0.001354\n",
            "epoch 49, step 3115, loss: 0.000242\n",
            "epoch 49, step 3116, loss: 0.000000\n",
            "epoch 49, step 3117, loss: 0.000000\n",
            "epoch 49, step 3118, loss: 0.000000\n",
            "epoch 49, step 3119, loss: 0.000000\n",
            "epoch 49, step 3120, loss: 0.000000\n",
            "epoch 49, step 3121, loss: 0.000000\n",
            "epoch 49, step 3122, loss: 0.000000\n",
            "epoch 49, step 3123, loss: 0.000000\n",
            "epoch 49, step 3124, loss: 0.000000\n",
            "epoch 49, step 3125, loss: 0.000000\n",
            "epoch 49, step 3126, loss: 0.000001\n",
            "epoch 49, step 3127, loss: 0.000317\n",
            "epoch 49, step 3128, loss: 14.199601\n",
            "epoch 49, step 3129, loss: 0.000000\n",
            "epoch 49, step 3130, loss: 0.000000\n",
            "epoch 49, step 3131, loss: 0.000000\n",
            "epoch 49, step 3132, loss: 2.444333\n",
            "epoch 49, step 3133, loss: 0.000000\n",
            "epoch 49, step 3134, loss: 0.000000\n",
            "epoch 49, step 3135, loss: 0.000000\n",
            "epoch 49, step 3136, loss: 0.000000\n",
            "epoch 49, step 3137, loss: 0.000000\n",
            "epoch 49, step 3138, loss: 4.087278\n",
            "epoch 49, step 3139, loss: 0.000000\n",
            "epoch 49, step 3140, loss: 0.000000\n",
            "epoch 49, step 3141, loss: 0.000022\n",
            "epoch 49, step 3142, loss: 0.000000\n",
            "epoch 49, step 3143, loss: 0.000000\n",
            "epoch 49, step 3144, loss: 28.597429\n",
            "epoch 49, step 3145, loss: 123.358437\n",
            "epoch 49, step 3146, loss: 0.000423\n",
            "epoch 49, step 3147, loss: 0.000000\n",
            "epoch 49, step 3148, loss: 0.000000\n",
            "epoch 49, step 3149, loss: 60.111404\n",
            "epoch 50, step 3150, loss: 0.000000\n",
            "epoch 50, step 3151, loss: 0.000186\n",
            "epoch 50, step 3152, loss: 0.000000\n",
            "epoch 50, step 3153, loss: 24.003283\n",
            "epoch 50, step 3154, loss: 0.000000\n",
            "epoch 50, step 3155, loss: 0.000917\n",
            "epoch 50, step 3156, loss: 0.000000\n",
            "epoch 50, step 3157, loss: 0.000000\n",
            "epoch 50, step 3158, loss: 0.003675\n",
            "epoch 50, step 3159, loss: 0.091024\n",
            "epoch 50, step 3160, loss: 95.030251\n",
            "epoch 50, step 3161, loss: 15.324594\n",
            "epoch 50, step 3162, loss: 0.000000\n",
            "epoch 50, step 3163, loss: 0.007212\n",
            "epoch 50, step 3164, loss: 3.107203\n",
            "epoch 50, step 3165, loss: 0.901583\n",
            "epoch 50, step 3166, loss: 0.913526\n",
            "epoch 50, step 3167, loss: 0.149891\n",
            "epoch 50, step 3168, loss: 0.000000\n",
            "epoch 50, step 3169, loss: 45.133083\n",
            "epoch 50, step 3170, loss: 0.000000\n",
            "epoch 50, step 3171, loss: 0.008109\n",
            "epoch 50, step 3172, loss: 0.000000\n",
            "epoch 50, step 3173, loss: 0.002228\n",
            "epoch 50, step 3174, loss: 0.186410\n",
            "epoch 50, step 3175, loss: 59.042881\n",
            "epoch 50, step 3176, loss: 0.000000\n",
            "epoch 50, step 3177, loss: 0.000009\n",
            "epoch 50, step 3178, loss: 0.000002\n",
            "epoch 50, step 3179, loss: 0.000000\n",
            "epoch 50, step 3180, loss: 0.000000\n",
            "epoch 50, step 3181, loss: 0.000000\n",
            "epoch 50, step 3182, loss: 0.000000\n",
            "epoch 50, step 3183, loss: 0.000412\n",
            "epoch 50, step 3184, loss: 0.000000\n",
            "epoch 50, step 3185, loss: 0.000000\n",
            "epoch 50, step 3186, loss: 0.000000\n",
            "epoch 50, step 3187, loss: 0.000002\n",
            "epoch 50, step 3188, loss: 0.000000\n",
            "epoch 50, step 3189, loss: 0.000000\n",
            "epoch 50, step 3190, loss: 0.000000\n",
            "epoch 50, step 3191, loss: 22.972672\n",
            "epoch 50, step 3192, loss: 0.000000\n",
            "epoch 50, step 3193, loss: 0.000000\n",
            "epoch 50, step 3194, loss: 0.000000\n",
            "epoch 50, step 3195, loss: 0.000000\n",
            "epoch 50, step 3196, loss: 0.000000\n",
            "epoch 50, step 3197, loss: 0.000002\n",
            "epoch 50, step 3198, loss: 0.000000\n",
            "epoch 50, step 3199, loss: 0.000000\n",
            "epoch 50, step 3200, loss: 0.000045\n",
            "epoch 50, step 3201, loss: 0.000505\n",
            "epoch 50, step 3202, loss: 0.000000\n",
            "epoch 50, step 3203, loss: 0.134505\n",
            "epoch 50, step 3204, loss: 0.000148\n",
            "epoch 50, step 3205, loss: 0.000000\n",
            "epoch 50, step 3206, loss: 0.000000\n",
            "epoch 50, step 3207, loss: 32.615894\n",
            "epoch 50, step 3208, loss: 107.996353\n",
            "epoch 50, step 3209, loss: 9.234495\n",
            "epoch 50, step 3210, loss: 0.000000\n",
            "epoch 50, step 3211, loss: 0.000011\n",
            "epoch 50, step 3212, loss: 45.386124\n",
            "epoch 51, step 3213, loss: 191.668182\n",
            "epoch 51, step 3214, loss: 0.000225\n",
            "epoch 51, step 3215, loss: 0.000000\n",
            "epoch 51, step 3216, loss: 125.922966\n",
            "epoch 51, step 3217, loss: 0.002055\n",
            "epoch 51, step 3218, loss: 0.000000\n",
            "epoch 51, step 3219, loss: 0.000000\n",
            "epoch 51, step 3220, loss: 0.000000\n",
            "epoch 51, step 3221, loss: 0.000000\n",
            "epoch 51, step 3222, loss: 0.000000\n",
            "epoch 51, step 3223, loss: 69.319557\n",
            "epoch 51, step 3224, loss: 12.498540\n",
            "epoch 51, step 3225, loss: 0.000000\n",
            "epoch 51, step 3226, loss: 0.788201\n",
            "epoch 51, step 3227, loss: 0.000000\n",
            "epoch 51, step 3228, loss: 0.000012\n",
            "epoch 51, step 3229, loss: 0.000000\n",
            "epoch 51, step 3230, loss: 0.000008\n",
            "epoch 51, step 3231, loss: 0.000001\n",
            "epoch 51, step 3232, loss: 98.211021\n",
            "epoch 51, step 3233, loss: 0.000000\n",
            "epoch 51, step 3234, loss: 0.000000\n",
            "epoch 51, step 3235, loss: 0.000000\n",
            "epoch 51, step 3236, loss: 0.164221\n",
            "epoch 51, step 3237, loss: 0.000000\n",
            "epoch 51, step 3238, loss: 31.041903\n",
            "epoch 51, step 3239, loss: 0.000000\n",
            "epoch 51, step 3240, loss: 0.087964\n",
            "epoch 51, step 3241, loss: 0.000000\n",
            "epoch 51, step 3242, loss: 0.000000\n",
            "epoch 51, step 3243, loss: 0.000000\n",
            "epoch 51, step 3244, loss: 0.000000\n",
            "epoch 51, step 3245, loss: 0.000000\n",
            "epoch 51, step 3246, loss: 0.000000\n",
            "epoch 51, step 3247, loss: 0.127445\n",
            "epoch 51, step 3248, loss: 9.454124\n",
            "epoch 51, step 3249, loss: 0.000000\n",
            "epoch 51, step 3250, loss: 0.000000\n",
            "epoch 51, step 3251, loss: 0.000000\n",
            "epoch 51, step 3252, loss: 0.000018\n",
            "epoch 51, step 3253, loss: 0.000002\n",
            "epoch 51, step 3254, loss: 8.006868\n",
            "epoch 51, step 3255, loss: 0.000000\n",
            "epoch 51, step 3256, loss: 0.000000\n",
            "epoch 51, step 3257, loss: 0.064421\n",
            "epoch 51, step 3258, loss: 0.000000\n",
            "epoch 51, step 3259, loss: 0.000032\n",
            "epoch 51, step 3260, loss: 13.583668\n",
            "epoch 51, step 3261, loss: 0.062401\n",
            "epoch 51, step 3262, loss: 0.000000\n",
            "epoch 51, step 3263, loss: 4.355700\n",
            "epoch 51, step 3264, loss: 13.242002\n",
            "epoch 51, step 3265, loss: 5.108477\n",
            "epoch 51, step 3266, loss: 0.000001\n",
            "epoch 51, step 3267, loss: 22.063101\n",
            "epoch 51, step 3268, loss: 0.002430\n",
            "epoch 51, step 3269, loss: 0.000003\n",
            "epoch 51, step 3270, loss: 2.984058\n",
            "epoch 51, step 3271, loss: 119.580139\n",
            "epoch 51, step 3272, loss: 0.003243\n",
            "epoch 51, step 3273, loss: 0.000000\n",
            "epoch 51, step 3274, loss: 0.000000\n",
            "epoch 51, step 3275, loss: 73.134483\n",
            "epoch 52, step 3276, loss: 0.000000\n",
            "epoch 52, step 3277, loss: 0.000000\n",
            "epoch 52, step 3278, loss: 0.000024\n",
            "epoch 52, step 3279, loss: 267.032959\n",
            "epoch 52, step 3280, loss: 0.002158\n",
            "epoch 52, step 3281, loss: 0.004016\n",
            "epoch 52, step 3282, loss: 0.000000\n",
            "epoch 52, step 3283, loss: 0.000000\n",
            "epoch 52, step 3284, loss: 0.000000\n",
            "epoch 52, step 3285, loss: 0.000000\n",
            "epoch 52, step 3286, loss: 78.870415\n",
            "epoch 52, step 3287, loss: 53.101906\n",
            "epoch 52, step 3288, loss: 0.000000\n",
            "epoch 52, step 3289, loss: 0.000000\n",
            "epoch 52, step 3290, loss: 13.170150\n",
            "epoch 52, step 3291, loss: 0.001222\n",
            "epoch 52, step 3292, loss: 0.000001\n",
            "epoch 52, step 3293, loss: 0.000000\n",
            "epoch 52, step 3294, loss: 0.000000\n",
            "epoch 52, step 3295, loss: 61.145702\n",
            "epoch 52, step 3296, loss: 0.000000\n",
            "epoch 52, step 3297, loss: 0.000000\n",
            "epoch 52, step 3298, loss: 0.012039\n",
            "epoch 52, step 3299, loss: 0.000004\n",
            "epoch 52, step 3300, loss: 162.005981\n",
            "epoch 52, step 3301, loss: 85.088303\n",
            "epoch 52, step 3302, loss: 0.000000\n",
            "epoch 52, step 3303, loss: 0.011296\n",
            "epoch 52, step 3304, loss: 0.002597\n",
            "epoch 52, step 3305, loss: 0.000000\n",
            "epoch 52, step 3306, loss: 0.000000\n",
            "epoch 52, step 3307, loss: 0.000000\n",
            "epoch 52, step 3308, loss: 0.000000\n",
            "epoch 52, step 3309, loss: 0.000000\n",
            "epoch 52, step 3310, loss: 0.000000\n",
            "epoch 52, step 3311, loss: 0.000000\n",
            "epoch 52, step 3312, loss: 0.000000\n",
            "epoch 52, step 3313, loss: 0.000000\n",
            "epoch 52, step 3314, loss: 0.000000\n",
            "epoch 52, step 3315, loss: 0.000000\n",
            "epoch 52, step 3316, loss: 15.763300\n",
            "epoch 52, step 3317, loss: 18.085953\n",
            "epoch 52, step 3318, loss: 0.000000\n",
            "epoch 52, step 3319, loss: 0.000443\n",
            "epoch 52, step 3320, loss: 0.000258\n",
            "epoch 52, step 3321, loss: 0.000000\n",
            "epoch 52, step 3322, loss: 0.000000\n",
            "epoch 52, step 3323, loss: 0.000000\n",
            "epoch 52, step 3324, loss: 0.000000\n",
            "epoch 52, step 3325, loss: 0.000000\n",
            "epoch 52, step 3326, loss: 0.000000\n",
            "epoch 52, step 3327, loss: 14.182988\n",
            "epoch 52, step 3328, loss: 0.000000\n",
            "epoch 52, step 3329, loss: 0.000000\n",
            "epoch 52, step 3330, loss: 0.000000\n",
            "epoch 52, step 3331, loss: 0.000000\n",
            "epoch 52, step 3332, loss: 2.602510\n",
            "epoch 52, step 3333, loss: 35.974110\n",
            "epoch 52, step 3334, loss: 82.599037\n",
            "epoch 52, step 3335, loss: 0.139974\n",
            "epoch 52, step 3336, loss: 0.000000\n",
            "epoch 52, step 3337, loss: 28.383938\n",
            "epoch 52, step 3338, loss: 29.122019\n",
            "epoch 53, step 3339, loss: 0.001708\n",
            "epoch 53, step 3340, loss: 0.009240\n",
            "epoch 53, step 3341, loss: 0.000000\n",
            "epoch 53, step 3342, loss: 47.916889\n",
            "epoch 53, step 3343, loss: 0.000012\n",
            "epoch 53, step 3344, loss: 32.339474\n",
            "epoch 53, step 3345, loss: 0.000013\n",
            "epoch 53, step 3346, loss: 0.000000\n",
            "epoch 53, step 3347, loss: 0.000049\n",
            "epoch 53, step 3348, loss: 0.000000\n",
            "epoch 53, step 3349, loss: 100.956139\n",
            "epoch 53, step 3350, loss: 14.997856\n",
            "epoch 53, step 3351, loss: 0.000000\n",
            "epoch 53, step 3352, loss: 0.000000\n",
            "epoch 53, step 3353, loss: 0.000000\n",
            "epoch 53, step 3354, loss: 0.000001\n",
            "epoch 53, step 3355, loss: 0.000000\n",
            "epoch 53, step 3356, loss: 0.000000\n",
            "epoch 53, step 3357, loss: 0.000000\n",
            "epoch 53, step 3358, loss: 74.827744\n",
            "epoch 53, step 3359, loss: 0.000000\n",
            "epoch 53, step 3360, loss: 0.000000\n",
            "epoch 53, step 3361, loss: 0.000000\n",
            "epoch 53, step 3362, loss: 0.000000\n",
            "epoch 53, step 3363, loss: 0.000000\n",
            "epoch 53, step 3364, loss: 64.840454\n",
            "epoch 53, step 3365, loss: 0.000000\n",
            "epoch 53, step 3366, loss: 0.000000\n",
            "epoch 53, step 3367, loss: 0.000000\n",
            "epoch 53, step 3368, loss: 0.000000\n",
            "epoch 53, step 3369, loss: 0.000000\n",
            "epoch 53, step 3370, loss: 6.416850\n",
            "epoch 53, step 3371, loss: 50.959347\n",
            "epoch 53, step 3372, loss: 0.000000\n",
            "epoch 53, step 3373, loss: 0.000000\n",
            "epoch 53, step 3374, loss: 0.000000\n",
            "epoch 53, step 3375, loss: 0.000000\n",
            "epoch 53, step 3376, loss: 0.000000\n",
            "epoch 53, step 3377, loss: 0.001519\n",
            "epoch 53, step 3378, loss: 0.000000\n",
            "epoch 53, step 3379, loss: 0.000000\n",
            "epoch 53, step 3380, loss: 42.615185\n",
            "epoch 53, step 3381, loss: 0.000000\n",
            "epoch 53, step 3382, loss: 49.834423\n",
            "epoch 53, step 3383, loss: 114.626076\n",
            "epoch 53, step 3384, loss: 0.000000\n",
            "epoch 53, step 3385, loss: 0.000000\n",
            "epoch 53, step 3386, loss: 0.076386\n",
            "epoch 53, step 3387, loss: 0.000011\n",
            "epoch 53, step 3388, loss: 0.000000\n",
            "epoch 53, step 3389, loss: 0.000000\n",
            "epoch 53, step 3390, loss: 0.149587\n",
            "epoch 53, step 3391, loss: 0.021386\n",
            "epoch 53, step 3392, loss: 0.000000\n",
            "epoch 53, step 3393, loss: 0.000000\n",
            "epoch 53, step 3394, loss: 0.000000\n",
            "epoch 53, step 3395, loss: 0.012422\n",
            "epoch 53, step 3396, loss: 0.459650\n",
            "epoch 53, step 3397, loss: 90.691635\n",
            "epoch 53, step 3398, loss: 0.000000\n",
            "epoch 53, step 3399, loss: 0.010641\n",
            "epoch 53, step 3400, loss: 5.078154\n",
            "epoch 53, step 3401, loss: 59.326820\n",
            "epoch 54, step 3402, loss: 162.004852\n",
            "epoch 54, step 3403, loss: 7.775486\n",
            "epoch 54, step 3404, loss: 3.769220\n",
            "epoch 54, step 3405, loss: 75.056549\n",
            "epoch 54, step 3406, loss: 0.023660\n",
            "epoch 54, step 3407, loss: 8.250303\n",
            "epoch 54, step 3408, loss: 0.000000\n",
            "epoch 54, step 3409, loss: 0.000000\n",
            "epoch 54, step 3410, loss: 2.660246\n",
            "epoch 54, step 3411, loss: 1.345240\n",
            "epoch 54, step 3412, loss: 38.536648\n",
            "epoch 54, step 3413, loss: 60.490376\n",
            "epoch 54, step 3414, loss: 0.000000\n",
            "epoch 54, step 3415, loss: 0.000000\n",
            "epoch 54, step 3416, loss: 0.137149\n",
            "epoch 54, step 3417, loss: 9.599277\n",
            "epoch 54, step 3418, loss: 0.000000\n",
            "epoch 54, step 3419, loss: 0.000000\n",
            "epoch 54, step 3420, loss: 0.000000\n",
            "epoch 54, step 3421, loss: 76.423500\n",
            "epoch 54, step 3422, loss: 0.000000\n",
            "epoch 54, step 3423, loss: 0.000000\n",
            "epoch 54, step 3424, loss: 0.000000\n",
            "epoch 54, step 3425, loss: 35.013294\n",
            "epoch 54, step 3426, loss: 0.000000\n",
            "epoch 54, step 3427, loss: 48.507874\n",
            "epoch 54, step 3428, loss: 0.000002\n",
            "epoch 54, step 3429, loss: 0.000349\n",
            "epoch 54, step 3430, loss: 0.000032\n",
            "epoch 54, step 3431, loss: 0.000000\n",
            "epoch 54, step 3432, loss: 17.030727\n",
            "epoch 54, step 3433, loss: 0.000000\n",
            "epoch 54, step 3434, loss: 0.000000\n",
            "epoch 54, step 3435, loss: 0.000000\n",
            "epoch 54, step 3436, loss: 0.000000\n",
            "epoch 54, step 3437, loss: 0.000000\n",
            "epoch 54, step 3438, loss: 0.000000\n",
            "epoch 54, step 3439, loss: 0.000000\n",
            "epoch 54, step 3440, loss: 0.000000\n",
            "epoch 54, step 3441, loss: 0.000001\n",
            "epoch 54, step 3442, loss: 0.000000\n",
            "epoch 54, step 3443, loss: 13.838607\n",
            "epoch 54, step 3444, loss: 0.000000\n",
            "epoch 54, step 3445, loss: 0.000003\n",
            "epoch 54, step 3446, loss: 0.000000\n",
            "epoch 54, step 3447, loss: 0.000000\n",
            "epoch 54, step 3448, loss: 0.000000\n",
            "epoch 54, step 3449, loss: 0.000000\n",
            "epoch 54, step 3450, loss: 0.000000\n",
            "epoch 54, step 3451, loss: 0.000000\n",
            "epoch 54, step 3452, loss: 0.000000\n",
            "epoch 54, step 3453, loss: 2.810407\n",
            "epoch 54, step 3454, loss: 36.403473\n",
            "epoch 54, step 3455, loss: 0.000000\n",
            "epoch 54, step 3456, loss: 10.258511\n",
            "epoch 54, step 3457, loss: 0.000002\n",
            "epoch 54, step 3458, loss: 0.000068\n",
            "epoch 54, step 3459, loss: 34.998276\n",
            "epoch 54, step 3460, loss: 103.607803\n",
            "epoch 54, step 3461, loss: 0.331311\n",
            "epoch 54, step 3462, loss: 0.000042\n",
            "epoch 54, step 3463, loss: 0.000000\n",
            "epoch 54, step 3464, loss: 24.685190\n",
            "epoch 55, step 3465, loss: 8.463485\n",
            "epoch 55, step 3466, loss: 0.013986\n",
            "epoch 55, step 3467, loss: 0.000000\n",
            "epoch 55, step 3468, loss: 252.668076\n",
            "epoch 55, step 3469, loss: 0.001000\n",
            "epoch 55, step 3470, loss: 0.000083\n",
            "epoch 55, step 3471, loss: 14.979650\n",
            "epoch 55, step 3472, loss: 0.000000\n",
            "epoch 55, step 3473, loss: 7.848074\n",
            "epoch 55, step 3474, loss: 12.981659\n",
            "epoch 55, step 3475, loss: 91.982353\n",
            "epoch 55, step 3476, loss: 16.811586\n",
            "epoch 55, step 3477, loss: 0.000000\n",
            "epoch 55, step 3478, loss: 0.000000\n",
            "epoch 55, step 3479, loss: 0.000002\n",
            "epoch 55, step 3480, loss: 0.000000\n",
            "epoch 55, step 3481, loss: 0.000000\n",
            "epoch 55, step 3482, loss: 8.259527\n",
            "epoch 55, step 3483, loss: 0.000000\n",
            "epoch 55, step 3484, loss: 60.718601\n",
            "epoch 55, step 3485, loss: 0.000172\n",
            "epoch 55, step 3486, loss: 0.000000\n",
            "epoch 55, step 3487, loss: 1.405899\n",
            "epoch 55, step 3488, loss: 0.000000\n",
            "epoch 55, step 3489, loss: 5.985975\n",
            "epoch 55, step 3490, loss: 0.000000\n",
            "epoch 55, step 3491, loss: 0.001842\n",
            "epoch 55, step 3492, loss: 15.719772\n",
            "epoch 55, step 3493, loss: 0.000106\n",
            "epoch 55, step 3494, loss: 0.000000\n",
            "epoch 55, step 3495, loss: 0.000000\n",
            "epoch 55, step 3496, loss: 0.000000\n",
            "epoch 55, step 3497, loss: 0.000000\n",
            "epoch 55, step 3498, loss: 0.000000\n",
            "epoch 55, step 3499, loss: 3.780468\n",
            "epoch 55, step 3500, loss: 0.000000\n",
            "epoch 55, step 3501, loss: 0.000000\n",
            "epoch 55, step 3502, loss: 0.006300\n",
            "epoch 55, step 3503, loss: 1.758163\n",
            "epoch 55, step 3504, loss: 2.605292\n",
            "epoch 55, step 3505, loss: 0.000000\n",
            "epoch 55, step 3506, loss: 0.831659\n",
            "epoch 55, step 3507, loss: 0.000416\n",
            "epoch 55, step 3508, loss: 0.000069\n",
            "epoch 55, step 3509, loss: 0.968344\n",
            "epoch 55, step 3510, loss: 0.000000\n",
            "epoch 55, step 3511, loss: 0.000097\n",
            "epoch 55, step 3512, loss: 0.000008\n",
            "epoch 55, step 3513, loss: 1.586143\n",
            "epoch 55, step 3514, loss: 0.000000\n",
            "epoch 55, step 3515, loss: 23.421797\n",
            "epoch 55, step 3516, loss: 13.320276\n",
            "epoch 55, step 3517, loss: 0.000000\n",
            "epoch 55, step 3518, loss: 0.069577\n",
            "epoch 55, step 3519, loss: 0.000002\n",
            "epoch 55, step 3520, loss: 0.000000\n",
            "epoch 55, step 3521, loss: 0.000050\n",
            "epoch 55, step 3522, loss: 26.522772\n",
            "epoch 55, step 3523, loss: 0.231227\n",
            "epoch 55, step 3524, loss: 0.000277\n",
            "epoch 55, step 3525, loss: 0.000000\n",
            "epoch 55, step 3526, loss: 223.748444\n",
            "epoch 55, step 3527, loss: 83.233452\n",
            "epoch 56, step 3528, loss: 0.000000\n",
            "epoch 56, step 3529, loss: 0.000000\n",
            "epoch 56, step 3530, loss: 0.000000\n",
            "epoch 56, step 3531, loss: 125.026718\n",
            "epoch 56, step 3532, loss: 40.412697\n",
            "epoch 56, step 3533, loss: 0.010639\n",
            "epoch 56, step 3534, loss: 0.000000\n",
            "epoch 56, step 3535, loss: 0.000000\n",
            "epoch 56, step 3536, loss: 0.000000\n",
            "epoch 56, step 3537, loss: 0.000000\n",
            "epoch 56, step 3538, loss: 82.548431\n",
            "epoch 56, step 3539, loss: 49.369995\n",
            "epoch 56, step 3540, loss: 0.000000\n",
            "epoch 56, step 3541, loss: 0.000000\n",
            "epoch 56, step 3542, loss: 0.345039\n",
            "epoch 56, step 3543, loss: 0.000535\n",
            "epoch 56, step 3544, loss: 6.424242\n",
            "epoch 56, step 3545, loss: 0.000078\n",
            "epoch 56, step 3546, loss: 0.000630\n",
            "epoch 56, step 3547, loss: 48.443249\n",
            "epoch 56, step 3548, loss: 0.001029\n",
            "epoch 56, step 3549, loss: 0.000873\n",
            "epoch 56, step 3550, loss: 82.379768\n",
            "epoch 56, step 3551, loss: 0.000000\n",
            "epoch 56, step 3552, loss: 15.741420\n",
            "epoch 56, step 3553, loss: 37.984005\n",
            "epoch 56, step 3554, loss: 0.000000\n",
            "epoch 56, step 3555, loss: 0.000006\n",
            "epoch 56, step 3556, loss: 0.000002\n",
            "epoch 56, step 3557, loss: 0.000000\n",
            "epoch 56, step 3558, loss: 0.000000\n",
            "epoch 56, step 3559, loss: 0.000000\n",
            "epoch 56, step 3560, loss: 0.000000\n",
            "epoch 56, step 3561, loss: 0.000000\n",
            "epoch 56, step 3562, loss: 0.000000\n",
            "epoch 56, step 3563, loss: 0.000000\n",
            "epoch 56, step 3564, loss: 0.000000\n",
            "epoch 56, step 3565, loss: 82.495110\n",
            "epoch 56, step 3566, loss: 0.012169\n",
            "epoch 56, step 3567, loss: 0.000000\n",
            "epoch 56, step 3568, loss: 0.000438\n",
            "epoch 56, step 3569, loss: 31.675871\n",
            "epoch 56, step 3570, loss: 0.000000\n",
            "epoch 56, step 3571, loss: 0.000000\n",
            "epoch 56, step 3572, loss: 0.000000\n",
            "epoch 56, step 3573, loss: 12.514758\n",
            "epoch 56, step 3574, loss: 0.000000\n",
            "epoch 56, step 3575, loss: 5.933681\n",
            "epoch 56, step 3576, loss: 0.000002\n",
            "epoch 56, step 3577, loss: 0.000000\n",
            "epoch 56, step 3578, loss: 0.000000\n",
            "epoch 56, step 3579, loss: 0.000000\n",
            "epoch 56, step 3580, loss: 0.000000\n",
            "epoch 56, step 3581, loss: 0.000000\n",
            "epoch 56, step 3582, loss: 0.000000\n",
            "epoch 56, step 3583, loss: 0.000000\n",
            "epoch 56, step 3584, loss: 1.302451\n",
            "epoch 56, step 3585, loss: 20.090904\n",
            "epoch 56, step 3586, loss: 142.474472\n",
            "epoch 56, step 3587, loss: 8.874941\n",
            "epoch 56, step 3588, loss: 0.000000\n",
            "epoch 56, step 3589, loss: 0.000000\n",
            "epoch 56, step 3590, loss: 75.856041\n",
            "epoch 57, step 3591, loss: 114.208199\n",
            "epoch 57, step 3592, loss: 0.000000\n",
            "epoch 57, step 3593, loss: 0.000000\n",
            "epoch 57, step 3594, loss: 65.647972\n",
            "epoch 57, step 3595, loss: 0.000000\n",
            "epoch 57, step 3596, loss: 0.000041\n",
            "epoch 57, step 3597, loss: 0.000000\n",
            "epoch 57, step 3598, loss: 0.000000\n",
            "epoch 57, step 3599, loss: 0.000000\n",
            "epoch 57, step 3600, loss: 0.015974\n",
            "epoch 57, step 3601, loss: 42.428825\n",
            "epoch 57, step 3602, loss: 24.687967\n",
            "epoch 57, step 3603, loss: 0.000592\n",
            "epoch 57, step 3604, loss: 0.000000\n",
            "epoch 57, step 3605, loss: 0.000000\n",
            "epoch 57, step 3606, loss: 0.000000\n",
            "epoch 57, step 3607, loss: 0.000000\n",
            "epoch 57, step 3608, loss: 0.000000\n",
            "epoch 57, step 3609, loss: 0.000000\n",
            "epoch 57, step 3610, loss: 89.077194\n",
            "epoch 57, step 3611, loss: 0.000000\n",
            "epoch 57, step 3612, loss: 0.000000\n",
            "epoch 57, step 3613, loss: 0.000000\n",
            "epoch 57, step 3614, loss: 0.000000\n",
            "epoch 57, step 3615, loss: 0.000000\n",
            "epoch 57, step 3616, loss: 17.623230\n",
            "epoch 57, step 3617, loss: 7.495006\n",
            "epoch 57, step 3618, loss: 0.000125\n",
            "epoch 57, step 3619, loss: 0.000000\n",
            "epoch 57, step 3620, loss: 0.000000\n",
            "epoch 57, step 3621, loss: 0.000000\n",
            "epoch 57, step 3622, loss: 0.000000\n",
            "epoch 57, step 3623, loss: 0.000000\n",
            "epoch 57, step 3624, loss: 0.000000\n",
            "epoch 57, step 3625, loss: 0.000000\n",
            "epoch 57, step 3626, loss: 0.000000\n",
            "epoch 57, step 3627, loss: 0.089494\n",
            "epoch 57, step 3628, loss: 0.000000\n",
            "epoch 57, step 3629, loss: 0.000000\n",
            "epoch 57, step 3630, loss: 0.000000\n",
            "epoch 57, step 3631, loss: 0.000000\n",
            "epoch 57, step 3632, loss: 18.701862\n",
            "epoch 57, step 3633, loss: 0.000000\n",
            "epoch 57, step 3634, loss: 0.000000\n",
            "epoch 57, step 3635, loss: 0.000000\n",
            "epoch 57, step 3636, loss: 0.000000\n",
            "epoch 57, step 3637, loss: 0.000000\n",
            "epoch 57, step 3638, loss: 0.000000\n",
            "epoch 57, step 3639, loss: 0.000000\n",
            "epoch 57, step 3640, loss: 0.000000\n",
            "epoch 57, step 3641, loss: 0.000000\n",
            "epoch 57, step 3642, loss: 0.837607\n",
            "epoch 57, step 3643, loss: 0.000000\n",
            "epoch 57, step 3644, loss: 0.000000\n",
            "epoch 57, step 3645, loss: 0.000000\n",
            "epoch 57, step 3646, loss: 0.000000\n",
            "epoch 57, step 3647, loss: 0.000000\n",
            "epoch 57, step 3648, loss: 9.360421\n",
            "epoch 57, step 3649, loss: 134.831512\n",
            "epoch 57, step 3650, loss: 0.000000\n",
            "epoch 57, step 3651, loss: 0.000000\n",
            "epoch 57, step 3652, loss: 0.000000\n",
            "epoch 57, step 3653, loss: 89.750557\n",
            "epoch 58, step 3654, loss: 0.000000\n",
            "epoch 58, step 3655, loss: 0.000000\n",
            "epoch 58, step 3656, loss: 0.000000\n",
            "epoch 58, step 3657, loss: 297.281616\n",
            "epoch 58, step 3658, loss: 0.000000\n",
            "epoch 58, step 3659, loss: 0.000078\n",
            "epoch 58, step 3660, loss: 0.000000\n",
            "epoch 58, step 3661, loss: 0.000000\n",
            "epoch 58, step 3662, loss: 0.000000\n",
            "epoch 58, step 3663, loss: 0.000012\n",
            "epoch 58, step 3664, loss: 56.619900\n",
            "epoch 58, step 3665, loss: 40.014557\n",
            "epoch 58, step 3666, loss: 5.960739\n",
            "epoch 58, step 3667, loss: 0.000000\n",
            "epoch 58, step 3668, loss: 0.000000\n",
            "epoch 58, step 3669, loss: 4.207515\n",
            "epoch 58, step 3670, loss: 0.000000\n",
            "epoch 58, step 3671, loss: 0.000000\n",
            "epoch 58, step 3672, loss: 0.000166\n",
            "epoch 58, step 3673, loss: 60.429016\n",
            "epoch 58, step 3674, loss: 0.000000\n",
            "epoch 58, step 3675, loss: 0.000001\n",
            "epoch 58, step 3676, loss: 0.000000\n",
            "epoch 58, step 3677, loss: 0.000004\n",
            "epoch 58, step 3678, loss: 0.000000\n",
            "epoch 58, step 3679, loss: 57.489719\n",
            "epoch 58, step 3680, loss: 0.000000\n",
            "epoch 58, step 3681, loss: 0.002391\n",
            "epoch 58, step 3682, loss: 0.000881\n",
            "epoch 58, step 3683, loss: 0.000000\n",
            "epoch 58, step 3684, loss: 0.000006\n",
            "epoch 58, step 3685, loss: 7.310906\n",
            "epoch 58, step 3686, loss: 2.216565\n",
            "epoch 58, step 3687, loss: 0.000000\n",
            "epoch 58, step 3688, loss: 0.000000\n",
            "epoch 58, step 3689, loss: 0.000225\n",
            "epoch 58, step 3690, loss: 0.000000\n",
            "epoch 58, step 3691, loss: 0.000000\n",
            "epoch 58, step 3692, loss: 0.000034\n",
            "epoch 58, step 3693, loss: 0.000000\n",
            "epoch 58, step 3694, loss: 9.307419\n",
            "epoch 58, step 3695, loss: 24.458811\n",
            "epoch 58, step 3696, loss: 0.000000\n",
            "epoch 58, step 3697, loss: 0.000000\n",
            "epoch 58, step 3698, loss: 0.000000\n",
            "epoch 58, step 3699, loss: 0.000000\n",
            "epoch 58, step 3700, loss: 0.000000\n",
            "epoch 58, step 3701, loss: 3.414824\n",
            "epoch 58, step 3702, loss: 0.000000\n",
            "epoch 58, step 3703, loss: 0.000000\n",
            "epoch 58, step 3704, loss: 0.000000\n",
            "epoch 58, step 3705, loss: 0.000000\n",
            "epoch 58, step 3706, loss: 0.000003\n",
            "epoch 58, step 3707, loss: 0.000000\n",
            "epoch 58, step 3708, loss: 0.000000\n",
            "epoch 58, step 3709, loss: 0.000000\n",
            "epoch 58, step 3710, loss: 0.000000\n",
            "epoch 58, step 3711, loss: 8.743917\n",
            "epoch 58, step 3712, loss: 158.492508\n",
            "epoch 58, step 3713, loss: 0.000000\n",
            "epoch 58, step 3714, loss: 0.000000\n",
            "epoch 58, step 3715, loss: 27.103279\n",
            "epoch 58, step 3716, loss: 100.754295\n",
            "epoch 59, step 3717, loss: 0.000000\n",
            "epoch 59, step 3718, loss: 0.000000\n",
            "epoch 59, step 3719, loss: 0.000000\n",
            "epoch 59, step 3720, loss: 113.428749\n",
            "epoch 59, step 3721, loss: 0.000000\n",
            "epoch 59, step 3722, loss: 0.001931\n",
            "epoch 59, step 3723, loss: 0.000000\n",
            "epoch 59, step 3724, loss: 0.000000\n",
            "epoch 59, step 3725, loss: 0.111921\n",
            "epoch 59, step 3726, loss: 49.337379\n",
            "epoch 59, step 3727, loss: 77.534851\n",
            "epoch 59, step 3728, loss: 49.752365\n",
            "epoch 59, step 3729, loss: 0.000136\n",
            "epoch 59, step 3730, loss: 0.000000\n",
            "epoch 59, step 3731, loss: 0.000000\n",
            "epoch 59, step 3732, loss: 18.094139\n",
            "epoch 59, step 3733, loss: 0.021237\n",
            "epoch 59, step 3734, loss: 0.000000\n",
            "epoch 59, step 3735, loss: 0.000005\n",
            "epoch 59, step 3736, loss: 23.074299\n",
            "epoch 59, step 3737, loss: 0.132434\n",
            "epoch 59, step 3738, loss: 0.000001\n",
            "epoch 59, step 3739, loss: 0.000000\n",
            "epoch 59, step 3740, loss: 13.784615\n",
            "epoch 59, step 3741, loss: 0.000000\n",
            "epoch 59, step 3742, loss: 66.917984\n",
            "epoch 59, step 3743, loss: 0.000000\n",
            "epoch 59, step 3744, loss: 0.398019\n",
            "epoch 59, step 3745, loss: 0.029115\n",
            "epoch 59, step 3746, loss: 0.000000\n",
            "epoch 59, step 3747, loss: 0.000000\n",
            "epoch 59, step 3748, loss: 6.432829\n",
            "epoch 59, step 3749, loss: 0.000000\n",
            "epoch 59, step 3750, loss: 0.000000\n",
            "epoch 59, step 3751, loss: 0.000001\n",
            "epoch 59, step 3752, loss: 0.000000\n",
            "epoch 59, step 3753, loss: 0.000000\n",
            "epoch 59, step 3754, loss: 0.000000\n",
            "epoch 59, step 3755, loss: 0.000004\n",
            "epoch 59, step 3756, loss: 0.000000\n",
            "epoch 59, step 3757, loss: 0.000000\n",
            "epoch 59, step 3758, loss: 25.974543\n",
            "epoch 59, step 3759, loss: 0.000000\n",
            "epoch 59, step 3760, loss: 0.000000\n",
            "epoch 59, step 3761, loss: 0.000000\n",
            "epoch 59, step 3762, loss: 0.000000\n",
            "epoch 59, step 3763, loss: 0.000000\n",
            "epoch 59, step 3764, loss: 0.000000\n",
            "epoch 59, step 3765, loss: 0.000000\n",
            "epoch 59, step 3766, loss: 0.000000\n",
            "epoch 59, step 3767, loss: 0.000000\n",
            "epoch 59, step 3768, loss: 0.000001\n",
            "epoch 59, step 3769, loss: 0.000000\n",
            "epoch 59, step 3770, loss: 0.000000\n",
            "epoch 59, step 3771, loss: 0.000000\n",
            "epoch 59, step 3772, loss: 0.000000\n",
            "epoch 59, step 3773, loss: 0.000000\n",
            "epoch 59, step 3774, loss: 11.651392\n",
            "epoch 59, step 3775, loss: 163.123459\n",
            "epoch 59, step 3776, loss: 0.000000\n",
            "epoch 59, step 3777, loss: 0.000000\n",
            "epoch 59, step 3778, loss: 6.523149\n",
            "epoch 59, step 3779, loss: 113.557770\n",
            "epoch 60, step 3780, loss: 44.494396\n",
            "epoch 60, step 3781, loss: 0.000000\n",
            "epoch 60, step 3782, loss: 0.000000\n",
            "epoch 60, step 3783, loss: 3.223568\n",
            "epoch 60, step 3784, loss: 0.000009\n",
            "epoch 60, step 3785, loss: 6.760342\n",
            "epoch 60, step 3786, loss: 0.000000\n",
            "epoch 60, step 3787, loss: 0.000000\n",
            "epoch 60, step 3788, loss: 0.000000\n",
            "epoch 60, step 3789, loss: 0.000005\n",
            "epoch 60, step 3790, loss: 38.692669\n",
            "epoch 60, step 3791, loss: 27.276829\n",
            "epoch 60, step 3792, loss: 0.000000\n",
            "epoch 60, step 3793, loss: 0.000000\n",
            "epoch 60, step 3794, loss: 0.000000\n",
            "epoch 60, step 3795, loss: 3.345043\n",
            "epoch 60, step 3796, loss: 0.000000\n",
            "epoch 60, step 3797, loss: 0.000093\n",
            "epoch 60, step 3798, loss: 0.000000\n",
            "epoch 60, step 3799, loss: 51.161095\n",
            "epoch 60, step 3800, loss: 0.000000\n",
            "epoch 60, step 3801, loss: 0.001320\n",
            "epoch 60, step 3802, loss: 0.000000\n",
            "epoch 60, step 3803, loss: 0.001439\n",
            "epoch 60, step 3804, loss: 0.000000\n",
            "epoch 60, step 3805, loss: 76.201286\n",
            "epoch 60, step 3806, loss: 0.000000\n",
            "epoch 60, step 3807, loss: 0.019162\n",
            "epoch 60, step 3808, loss: 2.003163\n",
            "epoch 60, step 3809, loss: 0.000000\n",
            "epoch 60, step 3810, loss: 0.000801\n",
            "epoch 60, step 3811, loss: 0.000023\n",
            "epoch 60, step 3812, loss: 6.442261\n",
            "epoch 60, step 3813, loss: 0.000000\n",
            "epoch 60, step 3814, loss: 13.554757\n",
            "epoch 60, step 3815, loss: 0.003025\n",
            "epoch 60, step 3816, loss: 0.000000\n",
            "epoch 60, step 3817, loss: 0.000000\n",
            "epoch 60, step 3818, loss: 0.000000\n",
            "epoch 60, step 3819, loss: 0.000000\n",
            "epoch 60, step 3820, loss: 0.000000\n",
            "epoch 60, step 3821, loss: 29.642963\n",
            "epoch 60, step 3822, loss: 0.000000\n",
            "epoch 60, step 3823, loss: 0.000000\n",
            "epoch 60, step 3824, loss: 0.000000\n",
            "epoch 60, step 3825, loss: 0.000000\n",
            "epoch 60, step 3826, loss: 0.000000\n",
            "epoch 60, step 3827, loss: 0.000000\n",
            "epoch 60, step 3828, loss: 0.000000\n",
            "epoch 60, step 3829, loss: 0.000000\n",
            "epoch 60, step 3830, loss: 0.000000\n",
            "epoch 60, step 3831, loss: 0.000000\n",
            "epoch 60, step 3832, loss: 0.000000\n",
            "epoch 60, step 3833, loss: 0.000000\n",
            "epoch 60, step 3834, loss: 0.000000\n",
            "epoch 60, step 3835, loss: 0.000000\n",
            "epoch 60, step 3836, loss: 0.000000\n",
            "epoch 60, step 3837, loss: 0.000046\n",
            "epoch 60, step 3838, loss: 166.269440\n",
            "epoch 60, step 3839, loss: 0.000000\n",
            "epoch 60, step 3840, loss: 0.000000\n",
            "epoch 60, step 3841, loss: 0.000000\n",
            "epoch 60, step 3842, loss: 102.460381\n",
            "epoch 61, step 3843, loss: 0.011252\n",
            "epoch 61, step 3844, loss: 0.000000\n",
            "epoch 61, step 3845, loss: 0.000000\n",
            "epoch 61, step 3846, loss: 6.722862\n",
            "epoch 61, step 3847, loss: 0.439583\n",
            "epoch 61, step 3848, loss: 0.000000\n",
            "epoch 61, step 3849, loss: 0.000000\n",
            "epoch 61, step 3850, loss: 0.000000\n",
            "epoch 61, step 3851, loss: 0.000000\n",
            "epoch 61, step 3852, loss: 2.992248\n",
            "epoch 61, step 3853, loss: 26.260185\n",
            "epoch 61, step 3854, loss: 19.867859\n",
            "epoch 61, step 3855, loss: 0.000000\n",
            "epoch 61, step 3856, loss: 0.000000\n",
            "epoch 61, step 3857, loss: 0.000188\n",
            "epoch 61, step 3858, loss: 0.000000\n",
            "epoch 61, step 3859, loss: 0.000000\n",
            "epoch 61, step 3860, loss: 0.000000\n",
            "epoch 61, step 3861, loss: 9.076488\n",
            "epoch 61, step 3862, loss: 85.003380\n",
            "epoch 61, step 3863, loss: 0.000000\n",
            "epoch 61, step 3864, loss: 0.000000\n",
            "epoch 61, step 3865, loss: 0.000000\n",
            "epoch 61, step 3866, loss: 0.002259\n",
            "epoch 61, step 3867, loss: 0.000000\n",
            "epoch 61, step 3868, loss: 12.465062\n",
            "epoch 61, step 3869, loss: 0.000000\n",
            "epoch 61, step 3870, loss: 5.070881\n",
            "epoch 61, step 3871, loss: 0.000000\n",
            "epoch 61, step 3872, loss: 0.000000\n",
            "epoch 61, step 3873, loss: 0.263782\n",
            "epoch 61, step 3874, loss: 0.024826\n",
            "epoch 61, step 3875, loss: 0.000000\n",
            "epoch 61, step 3876, loss: 0.000000\n",
            "epoch 61, step 3877, loss: 0.000000\n",
            "epoch 61, step 3878, loss: 0.000000\n",
            "epoch 61, step 3879, loss: 0.000000\n",
            "epoch 61, step 3880, loss: 0.000000\n",
            "epoch 61, step 3881, loss: 0.000584\n",
            "epoch 61, step 3882, loss: 0.000003\n",
            "epoch 61, step 3883, loss: 0.000000\n",
            "epoch 61, step 3884, loss: 12.253583\n",
            "epoch 61, step 3885, loss: 0.000000\n",
            "epoch 61, step 3886, loss: 0.000000\n",
            "epoch 61, step 3887, loss: 0.000000\n",
            "epoch 61, step 3888, loss: 0.000000\n",
            "epoch 61, step 3889, loss: 0.000000\n",
            "epoch 61, step 3890, loss: 0.000000\n",
            "epoch 61, step 3891, loss: 0.000000\n",
            "epoch 61, step 3892, loss: 0.000000\n",
            "epoch 61, step 3893, loss: 0.000000\n",
            "epoch 61, step 3894, loss: 5.451215\n",
            "epoch 61, step 3895, loss: 0.003796\n",
            "epoch 61, step 3896, loss: 0.000000\n",
            "epoch 61, step 3897, loss: 0.000000\n",
            "epoch 61, step 3898, loss: 6.158764\n",
            "epoch 61, step 3899, loss: 0.000008\n",
            "epoch 61, step 3900, loss: 6.194554\n",
            "epoch 61, step 3901, loss: 116.080597\n",
            "epoch 61, step 3902, loss: 0.000000\n",
            "epoch 61, step 3903, loss: 0.000000\n",
            "epoch 61, step 3904, loss: 0.000000\n",
            "epoch 61, step 3905, loss: 106.680702\n",
            "epoch 62, step 3906, loss: 0.000000\n",
            "epoch 62, step 3907, loss: 0.000000\n",
            "epoch 62, step 3908, loss: 0.000000\n",
            "epoch 62, step 3909, loss: 170.216629\n",
            "epoch 62, step 3910, loss: 0.000086\n",
            "epoch 62, step 3911, loss: 0.000000\n",
            "epoch 62, step 3912, loss: 0.000000\n",
            "epoch 62, step 3913, loss: 0.000000\n",
            "epoch 62, step 3914, loss: 0.000000\n",
            "epoch 62, step 3915, loss: 0.000000\n",
            "epoch 62, step 3916, loss: 36.810932\n",
            "epoch 62, step 3917, loss: 23.190456\n",
            "epoch 62, step 3918, loss: 0.000000\n",
            "epoch 62, step 3919, loss: 0.000024\n",
            "epoch 62, step 3920, loss: 0.000000\n",
            "epoch 62, step 3921, loss: 0.007081\n",
            "epoch 62, step 3922, loss: 0.000000\n",
            "epoch 62, step 3923, loss: 0.071869\n",
            "epoch 62, step 3924, loss: 0.000061\n",
            "epoch 62, step 3925, loss: 61.312019\n",
            "epoch 62, step 3926, loss: 7.319625\n",
            "epoch 62, step 3927, loss: 0.255241\n",
            "epoch 62, step 3928, loss: 0.000000\n",
            "epoch 62, step 3929, loss: 0.144757\n",
            "epoch 62, step 3930, loss: 0.000000\n",
            "epoch 62, step 3931, loss: 79.673126\n",
            "epoch 62, step 3932, loss: 0.000000\n",
            "epoch 62, step 3933, loss: 0.022944\n",
            "epoch 62, step 3934, loss: 0.007972\n",
            "epoch 62, step 3935, loss: 0.000000\n",
            "epoch 62, step 3936, loss: 0.000000\n",
            "epoch 62, step 3937, loss: 0.000000\n",
            "epoch 62, step 3938, loss: 0.000000\n",
            "epoch 62, step 3939, loss: 0.000000\n",
            "epoch 62, step 3940, loss: 0.000000\n",
            "epoch 62, step 3941, loss: 0.000059\n",
            "epoch 62, step 3942, loss: 0.000000\n",
            "epoch 62, step 3943, loss: 0.000000\n",
            "epoch 62, step 3944, loss: 0.000000\n",
            "epoch 62, step 3945, loss: 0.000010\n",
            "epoch 62, step 3946, loss: 0.000000\n",
            "epoch 62, step 3947, loss: 12.139969\n",
            "epoch 62, step 3948, loss: 0.000000\n",
            "epoch 62, step 3949, loss: 0.000000\n",
            "epoch 62, step 3950, loss: 0.000000\n",
            "epoch 62, step 3951, loss: 0.000000\n",
            "epoch 62, step 3952, loss: 0.000000\n",
            "epoch 62, step 3953, loss: 0.000000\n",
            "epoch 62, step 3954, loss: 0.000000\n",
            "epoch 62, step 3955, loss: 0.000000\n",
            "epoch 62, step 3956, loss: 0.000000\n",
            "epoch 62, step 3957, loss: 3.937878\n",
            "epoch 62, step 3958, loss: 0.000000\n",
            "epoch 62, step 3959, loss: 0.000000\n",
            "epoch 62, step 3960, loss: 0.000000\n",
            "epoch 62, step 3961, loss: 0.000000\n",
            "epoch 62, step 3962, loss: 2.195414\n",
            "epoch 62, step 3963, loss: 16.724104\n",
            "epoch 62, step 3964, loss: 113.466202\n",
            "epoch 62, step 3965, loss: 0.000000\n",
            "epoch 62, step 3966, loss: 0.000000\n",
            "epoch 62, step 3967, loss: 0.000000\n",
            "epoch 62, step 3968, loss: 102.497322\n",
            "epoch 63, step 3969, loss: 0.354743\n",
            "epoch 63, step 3970, loss: 0.000000\n",
            "epoch 63, step 3971, loss: 0.000000\n",
            "epoch 63, step 3972, loss: 14.432017\n",
            "epoch 63, step 3973, loss: 0.000000\n",
            "epoch 63, step 3974, loss: 11.271276\n",
            "epoch 63, step 3975, loss: 0.000000\n",
            "epoch 63, step 3976, loss: 0.000000\n",
            "epoch 63, step 3977, loss: 0.000000\n",
            "epoch 63, step 3978, loss: 0.000000\n",
            "epoch 63, step 3979, loss: 38.735214\n",
            "epoch 63, step 3980, loss: 50.243156\n",
            "epoch 63, step 3981, loss: 0.000000\n",
            "epoch 63, step 3982, loss: 0.000000\n",
            "epoch 63, step 3983, loss: 0.000000\n",
            "epoch 63, step 3984, loss: 0.000000\n",
            "epoch 63, step 3985, loss: 0.000000\n",
            "epoch 63, step 3986, loss: 0.000000\n",
            "epoch 63, step 3987, loss: 0.000003\n",
            "epoch 63, step 3988, loss: 76.000618\n",
            "epoch 63, step 3989, loss: 0.000000\n",
            "epoch 63, step 3990, loss: 0.000000\n",
            "epoch 63, step 3991, loss: 0.000000\n",
            "epoch 63, step 3992, loss: 0.000437\n",
            "epoch 63, step 3993, loss: 0.000000\n",
            "epoch 63, step 3994, loss: 67.026161\n",
            "epoch 63, step 3995, loss: 0.000000\n",
            "epoch 63, step 3996, loss: 0.000034\n",
            "epoch 63, step 3997, loss: 0.000022\n",
            "epoch 63, step 3998, loss: 0.000000\n",
            "epoch 63, step 3999, loss: 0.000000\n",
            "epoch 63, step 4000, loss: 0.000000\n",
            "epoch 63, step 4001, loss: 0.000000\n",
            "epoch 63, step 4002, loss: 0.000000\n",
            "epoch 63, step 4003, loss: 0.000000\n",
            "epoch 63, step 4004, loss: 0.000488\n",
            "epoch 63, step 4005, loss: 0.000000\n",
            "epoch 63, step 4006, loss: 0.000000\n",
            "epoch 63, step 4007, loss: 0.000000\n",
            "epoch 63, step 4008, loss: 0.000002\n",
            "epoch 63, step 4009, loss: 0.000000\n",
            "epoch 63, step 4010, loss: 13.629566\n",
            "epoch 63, step 4011, loss: 0.000000\n",
            "epoch 63, step 4012, loss: 0.000000\n",
            "epoch 63, step 4013, loss: 0.000001\n",
            "epoch 63, step 4014, loss: 0.000000\n",
            "epoch 63, step 4015, loss: 0.000000\n",
            "epoch 63, step 4016, loss: 0.000000\n",
            "epoch 63, step 4017, loss: 0.000000\n",
            "epoch 63, step 4018, loss: 0.000000\n",
            "epoch 63, step 4019, loss: 0.000000\n",
            "epoch 63, step 4020, loss: 4.517725\n",
            "epoch 63, step 4021, loss: 0.000001\n",
            "epoch 63, step 4022, loss: 0.000002\n",
            "epoch 63, step 4023, loss: 0.000000\n",
            "epoch 63, step 4024, loss: 0.000000\n",
            "epoch 63, step 4025, loss: 0.000000\n",
            "epoch 63, step 4026, loss: 19.580498\n",
            "epoch 63, step 4027, loss: 103.738930\n",
            "epoch 63, step 4028, loss: 0.000000\n",
            "epoch 63, step 4029, loss: 0.000000\n",
            "epoch 63, step 4030, loss: 0.000000\n",
            "epoch 63, step 4031, loss: 75.403137\n",
            "epoch 64, step 4032, loss: 142.512115\n",
            "epoch 64, step 4033, loss: 0.000000\n",
            "epoch 64, step 4034, loss: 0.000029\n",
            "epoch 64, step 4035, loss: 77.608543\n",
            "epoch 64, step 4036, loss: 0.000000\n",
            "epoch 64, step 4037, loss: 0.000000\n",
            "epoch 64, step 4038, loss: 0.000000\n",
            "epoch 64, step 4039, loss: 0.000000\n",
            "epoch 64, step 4040, loss: 0.000000\n",
            "epoch 64, step 4041, loss: 3.303151\n",
            "epoch 64, step 4042, loss: 17.098370\n",
            "epoch 64, step 4043, loss: 52.528744\n",
            "epoch 64, step 4044, loss: 0.000000\n",
            "epoch 64, step 4045, loss: 0.000000\n",
            "epoch 64, step 4046, loss: 0.000012\n",
            "epoch 64, step 4047, loss: 0.000000\n",
            "epoch 64, step 4048, loss: 0.000000\n",
            "epoch 64, step 4049, loss: 0.000001\n",
            "epoch 64, step 4050, loss: 0.348986\n",
            "epoch 64, step 4051, loss: 96.800507\n",
            "epoch 64, step 4052, loss: 0.000000\n",
            "epoch 64, step 4053, loss: 0.000000\n",
            "epoch 64, step 4054, loss: 0.000000\n",
            "epoch 64, step 4055, loss: 0.000000\n",
            "epoch 64, step 4056, loss: 0.000000\n",
            "epoch 64, step 4057, loss: 17.519491\n",
            "epoch 64, step 4058, loss: 0.000000\n",
            "epoch 64, step 4059, loss: 0.000000\n",
            "epoch 64, step 4060, loss: 0.000000\n",
            "epoch 64, step 4061, loss: 0.000000\n",
            "epoch 64, step 4062, loss: 0.000000\n",
            "epoch 64, step 4063, loss: 0.000000\n",
            "epoch 64, step 4064, loss: 0.064672\n",
            "epoch 64, step 4065, loss: 0.000000\n",
            "epoch 64, step 4066, loss: 0.000000\n",
            "epoch 64, step 4067, loss: 0.000000\n",
            "epoch 64, step 4068, loss: 0.000000\n",
            "epoch 64, step 4069, loss: 0.000000\n",
            "epoch 64, step 4070, loss: 20.652365\n",
            "epoch 64, step 4071, loss: 0.000000\n",
            "epoch 64, step 4072, loss: 0.000000\n",
            "epoch 64, step 4073, loss: 23.474983\n",
            "epoch 64, step 4074, loss: 0.000000\n",
            "epoch 64, step 4075, loss: 0.000000\n",
            "epoch 64, step 4076, loss: 0.000000\n",
            "epoch 64, step 4077, loss: 0.000000\n",
            "epoch 64, step 4078, loss: 0.000000\n",
            "epoch 64, step 4079, loss: 0.000000\n",
            "epoch 64, step 4080, loss: 0.000000\n",
            "epoch 64, step 4081, loss: 0.000000\n",
            "epoch 64, step 4082, loss: 0.000000\n",
            "epoch 64, step 4083, loss: 1.403722\n",
            "epoch 64, step 4084, loss: 24.137142\n",
            "epoch 64, step 4085, loss: 0.000000\n",
            "epoch 64, step 4086, loss: 0.000000\n",
            "epoch 64, step 4087, loss: 0.000000\n",
            "epoch 64, step 4088, loss: 0.000000\n",
            "epoch 64, step 4089, loss: 25.643805\n",
            "epoch 64, step 4090, loss: 64.078796\n",
            "epoch 64, step 4091, loss: 0.000000\n",
            "epoch 64, step 4092, loss: 0.000000\n",
            "epoch 64, step 4093, loss: 0.000000\n",
            "epoch 64, step 4094, loss: 87.153740\n",
            "epoch 65, step 4095, loss: 0.000000\n",
            "epoch 65, step 4096, loss: 0.317302\n",
            "epoch 65, step 4097, loss: 0.000000\n",
            "epoch 65, step 4098, loss: 223.708038\n",
            "epoch 65, step 4099, loss: 0.000000\n",
            "epoch 65, step 4100, loss: 0.000000\n",
            "epoch 65, step 4101, loss: 0.000000\n",
            "epoch 65, step 4102, loss: 0.000000\n",
            "epoch 65, step 4103, loss: 0.000000\n",
            "epoch 65, step 4104, loss: 0.000000\n",
            "epoch 65, step 4105, loss: 44.132736\n",
            "epoch 65, step 4106, loss: 38.579197\n",
            "epoch 65, step 4107, loss: 0.000000\n",
            "epoch 65, step 4108, loss: 0.000000\n",
            "epoch 65, step 4109, loss: 0.000000\n",
            "epoch 65, step 4110, loss: 0.000048\n",
            "epoch 65, step 4111, loss: 0.000000\n",
            "epoch 65, step 4112, loss: 0.000000\n",
            "epoch 65, step 4113, loss: 0.000000\n",
            "epoch 65, step 4114, loss: 59.185665\n",
            "epoch 65, step 4115, loss: 0.000000\n",
            "epoch 65, step 4116, loss: 0.000013\n",
            "epoch 65, step 4117, loss: 0.000000\n",
            "epoch 65, step 4118, loss: 0.000000\n",
            "epoch 65, step 4119, loss: 0.000000\n",
            "epoch 65, step 4120, loss: 106.903069\n",
            "epoch 65, step 4121, loss: 0.000000\n",
            "epoch 65, step 4122, loss: 0.009203\n",
            "epoch 65, step 4123, loss: 0.004887\n",
            "epoch 65, step 4124, loss: 0.000000\n",
            "epoch 65, step 4125, loss: 0.000000\n",
            "epoch 65, step 4126, loss: 0.000000\n",
            "epoch 65, step 4127, loss: 0.000000\n",
            "epoch 65, step 4128, loss: 0.000000\n",
            "epoch 65, step 4129, loss: 0.000000\n",
            "epoch 65, step 4130, loss: 0.000000\n",
            "epoch 65, step 4131, loss: 0.000000\n",
            "epoch 65, step 4132, loss: 0.000000\n",
            "epoch 65, step 4133, loss: 0.000000\n",
            "epoch 65, step 4134, loss: 0.000087\n",
            "epoch 65, step 4135, loss: 0.000000\n",
            "epoch 65, step 4136, loss: 9.704653\n",
            "epoch 65, step 4137, loss: 0.000000\n",
            "epoch 65, step 4138, loss: 0.000000\n",
            "epoch 65, step 4139, loss: 0.000000\n",
            "epoch 65, step 4140, loss: 0.000000\n",
            "epoch 65, step 4141, loss: 0.000000\n",
            "epoch 65, step 4142, loss: 0.000000\n",
            "epoch 65, step 4143, loss: 0.000000\n",
            "epoch 65, step 4144, loss: 0.000000\n",
            "epoch 65, step 4145, loss: 0.000000\n",
            "epoch 65, step 4146, loss: 9.135907\n",
            "epoch 65, step 4147, loss: 0.000000\n",
            "epoch 65, step 4148, loss: 0.000000\n",
            "epoch 65, step 4149, loss: 0.000000\n",
            "epoch 65, step 4150, loss: 0.000000\n",
            "epoch 65, step 4151, loss: 0.000000\n",
            "epoch 65, step 4152, loss: 36.282352\n",
            "epoch 65, step 4153, loss: 63.174004\n",
            "epoch 65, step 4154, loss: 0.000000\n",
            "epoch 65, step 4155, loss: 0.000000\n",
            "epoch 65, step 4156, loss: 0.000000\n",
            "epoch 65, step 4157, loss: 86.104263\n",
            "epoch 66, step 4158, loss: 0.000000\n",
            "epoch 66, step 4159, loss: 0.000000\n",
            "epoch 66, step 4160, loss: 0.000000\n",
            "epoch 66, step 4161, loss: 27.766010\n",
            "epoch 66, step 4162, loss: 0.000000\n",
            "epoch 66, step 4163, loss: 0.000000\n",
            "epoch 66, step 4164, loss: 0.000000\n",
            "epoch 66, step 4165, loss: 0.000000\n",
            "epoch 66, step 4166, loss: 0.000000\n",
            "epoch 66, step 4167, loss: 0.000000\n",
            "epoch 66, step 4168, loss: 57.800350\n",
            "epoch 66, step 4169, loss: 44.303802\n",
            "epoch 66, step 4170, loss: 0.000000\n",
            "epoch 66, step 4171, loss: 0.000000\n",
            "epoch 66, step 4172, loss: 0.000000\n",
            "epoch 66, step 4173, loss: 2.923340\n",
            "epoch 66, step 4174, loss: 8.276175\n",
            "epoch 66, step 4175, loss: 0.000000\n",
            "epoch 66, step 4176, loss: 0.014064\n",
            "epoch 66, step 4177, loss: 44.075851\n",
            "epoch 66, step 4178, loss: 0.000000\n",
            "epoch 66, step 4179, loss: 3.313309\n",
            "epoch 66, step 4180, loss: 0.000000\n",
            "epoch 66, step 4181, loss: 0.000000\n",
            "epoch 66, step 4182, loss: 0.000000\n",
            "epoch 66, step 4183, loss: 93.042038\n",
            "epoch 66, step 4184, loss: 0.000000\n",
            "epoch 66, step 4185, loss: 0.000000\n",
            "epoch 66, step 4186, loss: 0.000000\n",
            "epoch 66, step 4187, loss: 0.000000\n",
            "epoch 66, step 4188, loss: 0.000000\n",
            "epoch 66, step 4189, loss: 0.000000\n",
            "epoch 66, step 4190, loss: 0.000000\n",
            "epoch 66, step 4191, loss: 0.000000\n",
            "epoch 66, step 4192, loss: 0.000000\n",
            "epoch 66, step 4193, loss: 0.000000\n",
            "epoch 66, step 4194, loss: 0.000211\n",
            "epoch 66, step 4195, loss: 0.000000\n",
            "epoch 66, step 4196, loss: 0.000000\n",
            "epoch 66, step 4197, loss: 0.000000\n",
            "epoch 66, step 4198, loss: 0.000000\n",
            "epoch 66, step 4199, loss: 34.835770\n",
            "epoch 66, step 4200, loss: 0.000000\n",
            "epoch 66, step 4201, loss: 0.000000\n",
            "epoch 66, step 4202, loss: 0.000000\n",
            "epoch 66, step 4203, loss: 0.000000\n",
            "epoch 66, step 4204, loss: 0.001081\n",
            "epoch 66, step 4205, loss: 0.000000\n",
            "epoch 66, step 4206, loss: 0.000000\n",
            "epoch 66, step 4207, loss: 0.000000\n",
            "epoch 66, step 4208, loss: 0.000000\n",
            "epoch 66, step 4209, loss: 0.472347\n",
            "epoch 66, step 4210, loss: 0.001532\n",
            "epoch 66, step 4211, loss: 0.000000\n",
            "epoch 66, step 4212, loss: 0.000000\n",
            "epoch 66, step 4213, loss: 0.000000\n",
            "epoch 66, step 4214, loss: 0.000000\n",
            "epoch 66, step 4215, loss: 29.759781\n",
            "epoch 66, step 4216, loss: 98.424385\n",
            "epoch 66, step 4217, loss: 0.000000\n",
            "epoch 66, step 4218, loss: 0.000000\n",
            "epoch 66, step 4219, loss: 0.000000\n",
            "epoch 66, step 4220, loss: 100.078262\n",
            "epoch 67, step 4221, loss: 157.772003\n",
            "epoch 67, step 4222, loss: 0.000000\n",
            "epoch 67, step 4223, loss: 0.000000\n",
            "epoch 67, step 4224, loss: 105.981544\n",
            "epoch 67, step 4225, loss: 11.837988\n",
            "epoch 67, step 4226, loss: 0.000000\n",
            "epoch 67, step 4227, loss: 0.000000\n",
            "epoch 67, step 4228, loss: 0.000000\n",
            "epoch 67, step 4229, loss: 0.000000\n",
            "epoch 67, step 4230, loss: 0.000000\n",
            "epoch 67, step 4231, loss: 8.420623\n",
            "epoch 67, step 4232, loss: 25.288437\n",
            "epoch 67, step 4233, loss: 0.000000\n",
            "epoch 67, step 4234, loss: 0.000000\n",
            "epoch 67, step 4235, loss: 0.000000\n",
            "epoch 67, step 4236, loss: 0.000000\n",
            "epoch 67, step 4237, loss: 0.000308\n",
            "epoch 67, step 4238, loss: 0.000006\n",
            "epoch 67, step 4239, loss: 0.000000\n",
            "epoch 67, step 4240, loss: 101.331757\n",
            "epoch 67, step 4241, loss: 0.000000\n",
            "epoch 67, step 4242, loss: 0.000000\n",
            "epoch 67, step 4243, loss: 0.000000\n",
            "epoch 67, step 4244, loss: 0.000000\n",
            "epoch 67, step 4245, loss: 0.000000\n",
            "epoch 67, step 4246, loss: 38.493637\n",
            "epoch 67, step 4247, loss: 0.000000\n",
            "epoch 67, step 4248, loss: 0.001259\n",
            "epoch 67, step 4249, loss: 0.000000\n",
            "epoch 67, step 4250, loss: 0.000000\n",
            "epoch 67, step 4251, loss: 0.000000\n",
            "epoch 67, step 4252, loss: 0.000016\n",
            "epoch 67, step 4253, loss: 0.000000\n",
            "epoch 67, step 4254, loss: 0.000000\n",
            "epoch 67, step 4255, loss: 0.000000\n",
            "epoch 67, step 4256, loss: 4.118192\n",
            "epoch 67, step 4257, loss: 34.115284\n",
            "epoch 67, step 4258, loss: 0.000000\n",
            "epoch 67, step 4259, loss: 0.000000\n",
            "epoch 67, step 4260, loss: 0.000504\n",
            "epoch 67, step 4261, loss: 0.000000\n",
            "epoch 67, step 4262, loss: 17.076813\n",
            "epoch 67, step 4263, loss: 0.000000\n",
            "epoch 67, step 4264, loss: 0.000000\n",
            "epoch 67, step 4265, loss: 0.000000\n",
            "epoch 67, step 4266, loss: 0.000000\n",
            "epoch 67, step 4267, loss: 0.000000\n",
            "epoch 67, step 4268, loss: 0.000000\n",
            "epoch 67, step 4269, loss: 0.000000\n",
            "epoch 67, step 4270, loss: 0.000000\n",
            "epoch 67, step 4271, loss: 0.000616\n",
            "epoch 67, step 4272, loss: 14.924446\n",
            "epoch 67, step 4273, loss: 0.000000\n",
            "epoch 67, step 4274, loss: 0.000000\n",
            "epoch 67, step 4275, loss: 0.000000\n",
            "epoch 67, step 4276, loss: 0.000000\n",
            "epoch 67, step 4277, loss: 1.334564\n",
            "epoch 67, step 4278, loss: 23.309484\n",
            "epoch 67, step 4279, loss: 60.331367\n",
            "epoch 67, step 4280, loss: 0.000000\n",
            "epoch 67, step 4281, loss: 0.000000\n",
            "epoch 67, step 4282, loss: 8.220518\n",
            "epoch 67, step 4283, loss: 92.539391\n",
            "epoch 68, step 4284, loss: 0.000000\n",
            "epoch 68, step 4285, loss: 0.000000\n",
            "epoch 68, step 4286, loss: 0.000000\n",
            "epoch 68, step 4287, loss: 214.382736\n",
            "epoch 68, step 4288, loss: 0.000000\n",
            "epoch 68, step 4289, loss: 0.000000\n",
            "epoch 68, step 4290, loss: 0.000000\n",
            "epoch 68, step 4291, loss: 0.000000\n",
            "epoch 68, step 4292, loss: 0.000000\n",
            "epoch 68, step 4293, loss: 0.000000\n",
            "epoch 68, step 4294, loss: 36.109467\n",
            "epoch 68, step 4295, loss: 49.491470\n",
            "epoch 68, step 4296, loss: 0.000000\n",
            "epoch 68, step 4297, loss: 0.000000\n",
            "epoch 68, step 4298, loss: 0.000000\n",
            "epoch 68, step 4299, loss: 0.009382\n",
            "epoch 68, step 4300, loss: 0.000000\n",
            "epoch 68, step 4301, loss: 0.000000\n",
            "epoch 68, step 4302, loss: 0.000000\n",
            "epoch 68, step 4303, loss: 73.500549\n",
            "epoch 68, step 4304, loss: 0.000000\n",
            "epoch 68, step 4305, loss: 0.000000\n",
            "epoch 68, step 4306, loss: 0.000000\n",
            "epoch 68, step 4307, loss: 0.000000\n",
            "epoch 68, step 4308, loss: 152.203720\n",
            "epoch 68, step 4309, loss: 124.974854\n",
            "epoch 68, step 4310, loss: 0.000000\n",
            "epoch 68, step 4311, loss: 160.276047\n",
            "epoch 68, step 4312, loss: 0.000121\n",
            "epoch 68, step 4313, loss: 0.000000\n",
            "epoch 68, step 4314, loss: 0.000000\n",
            "epoch 68, step 4315, loss: 0.000000\n",
            "epoch 68, step 4316, loss: 0.000000\n",
            "epoch 68, step 4317, loss: 0.000000\n",
            "epoch 68, step 4318, loss: 0.000373\n",
            "epoch 68, step 4319, loss: 0.000000\n",
            "epoch 68, step 4320, loss: 0.000000\n",
            "epoch 68, step 4321, loss: 0.000000\n",
            "epoch 68, step 4322, loss: 0.000000\n",
            "epoch 68, step 4323, loss: 24.268520\n",
            "epoch 68, step 4324, loss: 0.026798\n",
            "epoch 68, step 4325, loss: 37.418430\n",
            "epoch 68, step 4326, loss: 41.512501\n",
            "epoch 68, step 4327, loss: 0.000000\n",
            "epoch 68, step 4328, loss: 0.000001\n",
            "epoch 68, step 4329, loss: 0.000000\n",
            "epoch 68, step 4330, loss: 0.000000\n",
            "epoch 68, step 4331, loss: 0.000000\n",
            "epoch 68, step 4332, loss: 0.000000\n",
            "epoch 68, step 4333, loss: 0.000000\n",
            "epoch 68, step 4334, loss: 0.000000\n",
            "epoch 68, step 4335, loss: 0.001947\n",
            "epoch 68, step 4336, loss: 0.000000\n",
            "epoch 68, step 4337, loss: 0.000000\n",
            "epoch 68, step 4338, loss: 0.000000\n",
            "epoch 68, step 4339, loss: 0.000000\n",
            "epoch 68, step 4340, loss: 0.000000\n",
            "epoch 68, step 4341, loss: 29.761545\n",
            "epoch 68, step 4342, loss: 87.341820\n",
            "epoch 68, step 4343, loss: 0.000000\n",
            "epoch 68, step 4344, loss: 111.470413\n",
            "epoch 68, step 4345, loss: 0.000000\n",
            "epoch 68, step 4346, loss: 102.016991\n",
            "epoch 69, step 4347, loss: 0.000000\n",
            "epoch 69, step 4348, loss: 0.000000\n",
            "epoch 69, step 4349, loss: 0.000000\n",
            "epoch 69, step 4350, loss: 25.465487\n",
            "epoch 69, step 4351, loss: 0.000000\n",
            "epoch 69, step 4352, loss: 4.048621\n",
            "epoch 69, step 4353, loss: 0.000000\n",
            "epoch 69, step 4354, loss: 0.000000\n",
            "epoch 69, step 4355, loss: 0.000000\n",
            "epoch 69, step 4356, loss: 0.000000\n",
            "epoch 69, step 4357, loss: 38.273846\n",
            "epoch 69, step 4358, loss: 50.808983\n",
            "epoch 69, step 4359, loss: 0.000000\n",
            "epoch 69, step 4360, loss: 0.000011\n",
            "epoch 69, step 4361, loss: 0.000000\n",
            "epoch 69, step 4362, loss: 9.990519\n",
            "epoch 69, step 4363, loss: 0.000000\n",
            "epoch 69, step 4364, loss: 0.000000\n",
            "epoch 69, step 4365, loss: 0.000000\n",
            "epoch 69, step 4366, loss: 54.227375\n",
            "epoch 69, step 4367, loss: 0.000000\n",
            "epoch 69, step 4368, loss: 0.000000\n",
            "epoch 69, step 4369, loss: 0.030319\n",
            "epoch 69, step 4370, loss: 5.444563\n",
            "epoch 69, step 4371, loss: 0.000000\n",
            "epoch 69, step 4372, loss: 88.272156\n",
            "epoch 69, step 4373, loss: 0.000000\n",
            "epoch 69, step 4374, loss: 0.000192\n",
            "epoch 69, step 4375, loss: 0.000019\n",
            "epoch 69, step 4376, loss: 0.000000\n",
            "epoch 69, step 4377, loss: 0.000000\n",
            "epoch 69, step 4378, loss: 0.000000\n",
            "epoch 69, step 4379, loss: 0.000000\n",
            "epoch 69, step 4380, loss: 0.000000\n",
            "epoch 69, step 4381, loss: 0.003841\n",
            "epoch 69, step 4382, loss: 117.321533\n",
            "epoch 69, step 4383, loss: 0.000000\n",
            "epoch 69, step 4384, loss: 0.000000\n",
            "epoch 69, step 4385, loss: 0.000000\n",
            "epoch 69, step 4386, loss: 23.525852\n",
            "epoch 69, step 4387, loss: 0.000000\n",
            "epoch 69, step 4388, loss: 58.947304\n",
            "epoch 69, step 4389, loss: 0.000201\n",
            "epoch 69, step 4390, loss: 0.000000\n",
            "epoch 69, step 4391, loss: 0.000000\n",
            "epoch 69, step 4392, loss: 0.000000\n",
            "epoch 69, step 4393, loss: 0.000000\n",
            "epoch 69, step 4394, loss: 0.010054\n",
            "epoch 69, step 4395, loss: 1.669939\n",
            "epoch 69, step 4396, loss: 0.000000\n",
            "epoch 69, step 4397, loss: 30.505175\n",
            "epoch 69, step 4398, loss: 20.111515\n",
            "epoch 69, step 4399, loss: 0.000000\n",
            "epoch 69, step 4400, loss: 9.417670\n",
            "epoch 69, step 4401, loss: 0.000000\n",
            "epoch 69, step 4402, loss: 0.000000\n",
            "epoch 69, step 4403, loss: 0.000000\n",
            "epoch 69, step 4404, loss: 29.496706\n",
            "epoch 69, step 4405, loss: 67.998512\n",
            "epoch 69, step 4406, loss: 0.000000\n",
            "epoch 69, step 4407, loss: 0.000000\n",
            "epoch 69, step 4408, loss: 0.000000\n",
            "epoch 69, step 4409, loss: 89.360809\n",
            "epoch 70, step 4410, loss: 145.507614\n",
            "epoch 70, step 4411, loss: 16.221430\n",
            "epoch 70, step 4412, loss: 0.000000\n",
            "epoch 70, step 4413, loss: 77.477966\n",
            "epoch 70, step 4414, loss: 0.000000\n",
            "epoch 70, step 4415, loss: 0.063449\n",
            "epoch 70, step 4416, loss: 0.000000\n",
            "epoch 70, step 4417, loss: 0.000000\n",
            "epoch 70, step 4418, loss: 5.893649\n",
            "epoch 70, step 4419, loss: 0.429436\n",
            "epoch 70, step 4420, loss: 197.801636\n",
            "epoch 70, step 4421, loss: 64.086769\n",
            "epoch 70, step 4422, loss: 0.000000\n",
            "epoch 70, step 4423, loss: 0.000000\n",
            "epoch 70, step 4424, loss: 4.004994\n",
            "epoch 70, step 4425, loss: 0.000005\n",
            "epoch 70, step 4426, loss: 12.414039\n",
            "epoch 70, step 4427, loss: 0.015951\n",
            "epoch 70, step 4428, loss: 0.490896\n",
            "epoch 70, step 4429, loss: 71.543694\n",
            "epoch 70, step 4430, loss: 14.213057\n",
            "epoch 70, step 4431, loss: 0.000000\n",
            "epoch 70, step 4432, loss: 0.000000\n",
            "epoch 70, step 4433, loss: 0.296919\n",
            "epoch 70, step 4434, loss: 0.004891\n",
            "epoch 70, step 4435, loss: 68.247108\n",
            "epoch 70, step 4436, loss: 0.000000\n",
            "epoch 70, step 4437, loss: 23.258924\n",
            "epoch 70, step 4438, loss: 0.000014\n",
            "epoch 70, step 4439, loss: 0.000000\n",
            "epoch 70, step 4440, loss: 2.527843\n",
            "epoch 70, step 4441, loss: 0.000000\n",
            "epoch 70, step 4442, loss: 0.000000\n",
            "epoch 70, step 4443, loss: 0.095343\n",
            "epoch 70, step 4444, loss: 0.000000\n",
            "epoch 70, step 4445, loss: 15.364169\n",
            "epoch 70, step 4446, loss: 0.000000\n",
            "epoch 70, step 4447, loss: 0.000000\n",
            "epoch 70, step 4448, loss: 0.000000\n",
            "epoch 70, step 4449, loss: 0.000000\n",
            "epoch 70, step 4450, loss: 0.000000\n",
            "epoch 70, step 4451, loss: 40.100655\n",
            "epoch 70, step 4452, loss: 0.000000\n",
            "epoch 70, step 4453, loss: 0.000001\n",
            "epoch 70, step 4454, loss: 0.000000\n",
            "epoch 70, step 4455, loss: 0.000000\n",
            "epoch 70, step 4456, loss: 85.780304\n",
            "epoch 70, step 4457, loss: 133.393356\n",
            "epoch 70, step 4458, loss: 0.000000\n",
            "epoch 70, step 4459, loss: 0.000000\n",
            "epoch 70, step 4460, loss: 28.574043\n",
            "epoch 70, step 4461, loss: 29.194145\n",
            "epoch 70, step 4462, loss: 0.000000\n",
            "epoch 70, step 4463, loss: 3.371873\n",
            "epoch 70, step 4464, loss: 43.386219\n",
            "epoch 70, step 4465, loss: 64.987282\n",
            "epoch 70, step 4466, loss: 0.000213\n",
            "epoch 70, step 4467, loss: 37.879852\n",
            "epoch 70, step 4468, loss: 85.335365\n",
            "epoch 70, step 4469, loss: 305.169373\n",
            "epoch 70, step 4470, loss: 0.000091\n",
            "epoch 70, step 4471, loss: 0.000000\n",
            "epoch 70, step 4472, loss: 0.081776\n",
            "epoch 71, step 4473, loss: 0.000000\n",
            "epoch 71, step 4474, loss: 0.001626\n",
            "epoch 71, step 4475, loss: 0.000000\n",
            "epoch 71, step 4476, loss: 262.851624\n",
            "epoch 71, step 4477, loss: 0.000000\n",
            "epoch 71, step 4478, loss: 0.000000\n",
            "epoch 71, step 4479, loss: 0.000000\n",
            "epoch 71, step 4480, loss: 0.000003\n",
            "epoch 71, step 4481, loss: 0.000000\n",
            "epoch 71, step 4482, loss: 0.000000\n",
            "epoch 71, step 4483, loss: 0.262947\n",
            "epoch 71, step 4484, loss: 37.649666\n",
            "epoch 71, step 4485, loss: 3.608545\n",
            "epoch 71, step 4486, loss: 0.000000\n",
            "epoch 71, step 4487, loss: 4.498679\n",
            "epoch 71, step 4488, loss: 0.000000\n",
            "epoch 71, step 4489, loss: 0.000000\n",
            "epoch 71, step 4490, loss: 18.206060\n",
            "epoch 71, step 4491, loss: 0.000000\n",
            "epoch 71, step 4492, loss: 122.589172\n",
            "epoch 71, step 4493, loss: 3.641593\n",
            "epoch 71, step 4494, loss: 0.000000\n",
            "epoch 71, step 4495, loss: 24.953968\n",
            "epoch 71, step 4496, loss: 0.000000\n",
            "epoch 71, step 4497, loss: 0.000000\n",
            "epoch 71, step 4498, loss: 84.193718\n",
            "epoch 71, step 4499, loss: 0.000000\n",
            "epoch 71, step 4500, loss: 12.439203\n",
            "epoch 71, step 4501, loss: 2.270376\n",
            "epoch 71, step 4502, loss: 92.107529\n",
            "epoch 71, step 4503, loss: 0.000000\n",
            "epoch 71, step 4504, loss: 0.000000\n",
            "epoch 71, step 4505, loss: 0.000000\n",
            "epoch 71, step 4506, loss: 38.821808\n",
            "epoch 71, step 4507, loss: 52.558819\n",
            "epoch 71, step 4508, loss: 0.000000\n",
            "epoch 71, step 4509, loss: 0.000000\n",
            "epoch 71, step 4510, loss: 0.000000\n",
            "epoch 71, step 4511, loss: 0.000000\n",
            "epoch 71, step 4512, loss: 0.000000\n",
            "epoch 71, step 4513, loss: 0.000000\n",
            "epoch 71, step 4514, loss: 185.928680\n",
            "epoch 71, step 4515, loss: 0.000000\n",
            "epoch 71, step 4516, loss: 0.000000\n",
            "epoch 71, step 4517, loss: 0.000000\n",
            "epoch 71, step 4518, loss: 0.000000\n",
            "epoch 71, step 4519, loss: 0.000000\n",
            "epoch 71, step 4520, loss: 0.000000\n",
            "epoch 71, step 4521, loss: 0.000000\n",
            "epoch 71, step 4522, loss: 2.905158\n",
            "epoch 71, step 4523, loss: 0.000000\n",
            "epoch 71, step 4524, loss: 0.002550\n",
            "epoch 71, step 4525, loss: 0.000000\n",
            "epoch 71, step 4526, loss: 0.000000\n",
            "epoch 71, step 4527, loss: 11.342438\n",
            "epoch 71, step 4528, loss: 0.000000\n",
            "epoch 71, step 4529, loss: 0.000000\n",
            "epoch 71, step 4530, loss: 343.833374\n",
            "epoch 71, step 4531, loss: 97.895401\n",
            "epoch 71, step 4532, loss: 21.289858\n",
            "epoch 71, step 4533, loss: 14.960328\n",
            "epoch 71, step 4534, loss: 0.000000\n",
            "epoch 71, step 4535, loss: 13.706983\n",
            "epoch 72, step 4536, loss: 0.001024\n",
            "epoch 72, step 4537, loss: 0.001714\n",
            "epoch 72, step 4538, loss: 0.000000\n",
            "epoch 72, step 4539, loss: 12.246696\n",
            "epoch 72, step 4540, loss: 0.000138\n",
            "epoch 72, step 4541, loss: 0.000000\n",
            "epoch 72, step 4542, loss: 0.000000\n",
            "epoch 72, step 4543, loss: 0.000000\n",
            "epoch 72, step 4544, loss: 0.000000\n",
            "epoch 72, step 4545, loss: 45.021988\n",
            "epoch 72, step 4546, loss: 108.285675\n",
            "epoch 72, step 4547, loss: 35.931774\n",
            "epoch 72, step 4548, loss: 0.000000\n",
            "epoch 72, step 4549, loss: 44.768593\n",
            "epoch 72, step 4550, loss: 70.214836\n",
            "epoch 72, step 4551, loss: 138.207275\n",
            "epoch 72, step 4552, loss: 0.000000\n",
            "epoch 72, step 4553, loss: 0.000580\n",
            "epoch 72, step 4554, loss: 5.061602\n",
            "epoch 72, step 4555, loss: 44.007771\n",
            "epoch 72, step 4556, loss: 0.000000\n",
            "epoch 72, step 4557, loss: 0.000000\n",
            "epoch 72, step 4558, loss: 0.000000\n",
            "epoch 72, step 4559, loss: 0.000000\n",
            "epoch 72, step 4560, loss: 0.000000\n",
            "epoch 72, step 4561, loss: 75.257553\n",
            "epoch 72, step 4562, loss: 0.000000\n",
            "epoch 72, step 4563, loss: 0.000000\n",
            "epoch 72, step 4564, loss: 0.000000\n",
            "epoch 72, step 4565, loss: 0.000000\n",
            "epoch 72, step 4566, loss: 0.000019\n",
            "epoch 72, step 4567, loss: 0.000006\n",
            "epoch 72, step 4568, loss: 0.000000\n",
            "epoch 72, step 4569, loss: 173.215836\n",
            "epoch 72, step 4570, loss: 0.001220\n",
            "epoch 72, step 4571, loss: 8.064528\n",
            "epoch 72, step 4572, loss: 0.000000\n",
            "epoch 72, step 4573, loss: 5.230179\n",
            "epoch 72, step 4574, loss: 0.000000\n",
            "epoch 72, step 4575, loss: 0.000000\n",
            "epoch 72, step 4576, loss: 35.276787\n",
            "epoch 72, step 4577, loss: 107.316566\n",
            "epoch 72, step 4578, loss: 0.000000\n",
            "epoch 72, step 4579, loss: 0.000000\n",
            "epoch 72, step 4580, loss: 0.000000\n",
            "epoch 72, step 4581, loss: 0.000000\n",
            "epoch 72, step 4582, loss: 0.000000\n",
            "epoch 72, step 4583, loss: 0.000000\n",
            "epoch 72, step 4584, loss: 0.000000\n",
            "epoch 72, step 4585, loss: 0.000000\n",
            "epoch 72, step 4586, loss: 0.136274\n",
            "epoch 72, step 4587, loss: 0.007438\n",
            "epoch 72, step 4588, loss: 0.000000\n",
            "epoch 72, step 4589, loss: 0.000000\n",
            "epoch 72, step 4590, loss: 10.945230\n",
            "epoch 72, step 4591, loss: 0.000000\n",
            "epoch 72, step 4592, loss: 0.000836\n",
            "epoch 72, step 4593, loss: 22.365875\n",
            "epoch 72, step 4594, loss: 115.190002\n",
            "epoch 72, step 4595, loss: 0.000000\n",
            "epoch 72, step 4596, loss: 8.339679\n",
            "epoch 72, step 4597, loss: 0.000557\n",
            "epoch 72, step 4598, loss: 0.023178\n",
            "epoch 73, step 4599, loss: 229.113266\n",
            "epoch 73, step 4600, loss: 0.000001\n",
            "epoch 73, step 4601, loss: 0.000000\n",
            "epoch 73, step 4602, loss: 160.830597\n",
            "epoch 73, step 4603, loss: 0.000000\n",
            "epoch 73, step 4604, loss: 0.000000\n",
            "epoch 73, step 4605, loss: 0.000000\n",
            "epoch 73, step 4606, loss: 0.000000\n",
            "epoch 73, step 4607, loss: 0.000000\n",
            "epoch 73, step 4608, loss: 0.000000\n",
            "epoch 73, step 4609, loss: 0.000159\n",
            "epoch 73, step 4610, loss: 13.412329\n",
            "epoch 73, step 4611, loss: 0.000000\n",
            "epoch 73, step 4612, loss: 0.000000\n",
            "epoch 73, step 4613, loss: 0.000000\n",
            "epoch 73, step 4614, loss: 0.000000\n",
            "epoch 73, step 4615, loss: 0.000000\n",
            "epoch 73, step 4616, loss: 0.000403\n",
            "epoch 73, step 4617, loss: 0.000017\n",
            "epoch 73, step 4618, loss: 132.523041\n",
            "epoch 73, step 4619, loss: 0.000000\n",
            "epoch 73, step 4620, loss: 0.000000\n",
            "epoch 73, step 4621, loss: 0.000000\n",
            "epoch 73, step 4622, loss: 10.272760\n",
            "epoch 73, step 4623, loss: 0.000000\n",
            "epoch 73, step 4624, loss: 52.588646\n",
            "epoch 73, step 4625, loss: 0.000000\n",
            "epoch 73, step 4626, loss: 0.000003\n",
            "epoch 73, step 4627, loss: 29.768757\n",
            "epoch 73, step 4628, loss: 0.000000\n",
            "epoch 73, step 4629, loss: 0.000003\n",
            "epoch 73, step 4630, loss: 2.836710\n",
            "epoch 73, step 4631, loss: 0.000005\n",
            "epoch 73, step 4632, loss: 0.000000\n",
            "epoch 73, step 4633, loss: 1.730028\n",
            "epoch 73, step 4634, loss: 4.244099\n",
            "epoch 73, step 4635, loss: 0.000000\n",
            "epoch 73, step 4636, loss: 42.965168\n",
            "epoch 73, step 4637, loss: 2.605979\n",
            "epoch 73, step 4638, loss: 0.000170\n",
            "epoch 73, step 4639, loss: 0.023247\n",
            "epoch 73, step 4640, loss: 8.063444\n",
            "epoch 73, step 4641, loss: 0.000000\n",
            "epoch 73, step 4642, loss: 0.000000\n",
            "epoch 73, step 4643, loss: 0.000000\n",
            "epoch 73, step 4644, loss: 0.000000\n",
            "epoch 73, step 4645, loss: 0.000000\n",
            "epoch 73, step 4646, loss: 0.000000\n",
            "epoch 73, step 4647, loss: 0.000000\n",
            "epoch 73, step 4648, loss: 0.000000\n",
            "epoch 73, step 4649, loss: 0.000000\n",
            "epoch 73, step 4650, loss: 15.939478\n",
            "epoch 73, step 4651, loss: 0.086883\n",
            "epoch 73, step 4652, loss: 0.000001\n",
            "epoch 73, step 4653, loss: 0.000000\n",
            "epoch 73, step 4654, loss: 0.000000\n",
            "epoch 73, step 4655, loss: 0.000000\n",
            "epoch 73, step 4656, loss: 0.000010\n",
            "epoch 73, step 4657, loss: 50.363419\n",
            "epoch 73, step 4658, loss: 0.000295\n",
            "epoch 73, step 4659, loss: 0.000000\n",
            "epoch 73, step 4660, loss: 0.000000\n",
            "epoch 73, step 4661, loss: 15.030062\n",
            "epoch 74, step 4662, loss: 0.000000\n",
            "epoch 74, step 4663, loss: 0.000001\n",
            "epoch 74, step 4664, loss: 0.000000\n",
            "epoch 74, step 4665, loss: 149.108383\n",
            "epoch 74, step 4666, loss: 0.210401\n",
            "epoch 74, step 4667, loss: 0.000000\n",
            "epoch 74, step 4668, loss: 77.397942\n",
            "epoch 74, step 4669, loss: 0.000000\n",
            "epoch 74, step 4670, loss: 15.601587\n",
            "epoch 74, step 4671, loss: 39.937416\n",
            "epoch 74, step 4672, loss: 114.309723\n",
            "epoch 74, step 4673, loss: 37.049961\n",
            "epoch 74, step 4674, loss: 0.000000\n",
            "epoch 74, step 4675, loss: 0.179434\n",
            "epoch 74, step 4676, loss: 0.000000\n",
            "epoch 74, step 4677, loss: 0.381750\n",
            "epoch 74, step 4678, loss: 69.469666\n",
            "epoch 74, step 4679, loss: 45.144421\n",
            "epoch 74, step 4680, loss: 0.000000\n",
            "epoch 74, step 4681, loss: 61.663567\n",
            "epoch 74, step 4682, loss: 0.019313\n",
            "epoch 74, step 4683, loss: 0.000000\n",
            "epoch 74, step 4684, loss: 0.000000\n",
            "epoch 74, step 4685, loss: 0.000000\n",
            "epoch 74, step 4686, loss: 0.000000\n",
            "epoch 74, step 4687, loss: 49.394112\n",
            "epoch 74, step 4688, loss: 0.000000\n",
            "epoch 74, step 4689, loss: 0.045454\n",
            "epoch 74, step 4690, loss: 0.000000\n",
            "epoch 74, step 4691, loss: 0.000000\n",
            "epoch 74, step 4692, loss: 63.639629\n",
            "epoch 74, step 4693, loss: 0.000000\n",
            "epoch 74, step 4694, loss: 0.000278\n",
            "epoch 74, step 4695, loss: 0.000000\n",
            "epoch 74, step 4696, loss: 74.646729\n",
            "epoch 74, step 4697, loss: 0.000000\n",
            "epoch 74, step 4698, loss: 4.024294\n",
            "epoch 74, step 4699, loss: 48.579613\n",
            "epoch 74, step 4700, loss: 0.000000\n",
            "epoch 74, step 4701, loss: 0.000000\n",
            "epoch 74, step 4702, loss: 0.000000\n",
            "epoch 74, step 4703, loss: 24.566664\n",
            "epoch 74, step 4704, loss: 0.000000\n",
            "epoch 74, step 4705, loss: 0.000000\n",
            "epoch 74, step 4706, loss: 0.000000\n",
            "epoch 74, step 4707, loss: 0.000000\n",
            "epoch 74, step 4708, loss: 0.000000\n",
            "epoch 74, step 4709, loss: 0.000000\n",
            "epoch 74, step 4710, loss: 0.000000\n",
            "epoch 74, step 4711, loss: 11.939327\n",
            "epoch 74, step 4712, loss: 0.000000\n",
            "epoch 74, step 4713, loss: 2.490020\n",
            "epoch 74, step 4714, loss: 0.000000\n",
            "epoch 74, step 4715, loss: 0.000000\n",
            "epoch 74, step 4716, loss: 0.000161\n",
            "epoch 74, step 4717, loss: 0.000000\n",
            "epoch 74, step 4718, loss: 5.688341\n",
            "epoch 74, step 4719, loss: 7.038707\n",
            "epoch 74, step 4720, loss: 69.253189\n",
            "epoch 74, step 4721, loss: 0.076421\n",
            "epoch 74, step 4722, loss: 0.000000\n",
            "epoch 74, step 4723, loss: 0.000000\n",
            "epoch 74, step 4724, loss: 23.930067\n",
            "epoch 75, step 4725, loss: 0.000001\n",
            "epoch 75, step 4726, loss: 0.843471\n",
            "epoch 75, step 4727, loss: 0.000000\n",
            "epoch 75, step 4728, loss: 10.273029\n",
            "epoch 75, step 4729, loss: 0.000000\n",
            "epoch 75, step 4730, loss: 0.320002\n",
            "epoch 75, step 4731, loss: 0.000000\n",
            "epoch 75, step 4732, loss: 0.000000\n",
            "epoch 75, step 4733, loss: 0.000000\n",
            "epoch 75, step 4734, loss: 0.000000\n",
            "epoch 75, step 4735, loss: 107.873650\n",
            "epoch 75, step 4736, loss: 4.906757\n",
            "epoch 75, step 4737, loss: 0.000000\n",
            "epoch 75, step 4738, loss: 44.318085\n",
            "epoch 75, step 4739, loss: 0.467353\n",
            "epoch 75, step 4740, loss: 0.027954\n",
            "epoch 75, step 4741, loss: 0.000000\n",
            "epoch 75, step 4742, loss: 10.603894\n",
            "epoch 75, step 4743, loss: 0.000000\n",
            "epoch 75, step 4744, loss: 52.432335\n",
            "epoch 75, step 4745, loss: 4.072805\n",
            "epoch 75, step 4746, loss: 0.000521\n",
            "epoch 75, step 4747, loss: 40.914597\n",
            "epoch 75, step 4748, loss: 0.000000\n",
            "epoch 75, step 4749, loss: 0.000000\n",
            "epoch 75, step 4750, loss: 109.909645\n",
            "epoch 75, step 4751, loss: 0.000000\n",
            "epoch 75, step 4752, loss: 23.399931\n",
            "epoch 75, step 4753, loss: 0.077837\n",
            "epoch 75, step 4754, loss: 0.000000\n",
            "epoch 75, step 4755, loss: 0.000000\n",
            "epoch 75, step 4756, loss: 0.000002\n",
            "epoch 75, step 4757, loss: 0.000000\n",
            "epoch 75, step 4758, loss: 0.000000\n",
            "epoch 75, step 4759, loss: 0.000000\n",
            "epoch 75, step 4760, loss: 23.134132\n",
            "epoch 75, step 4761, loss: 0.000000\n",
            "epoch 75, step 4762, loss: 0.000000\n",
            "epoch 75, step 4763, loss: 0.000000\n",
            "epoch 75, step 4764, loss: 0.000000\n",
            "epoch 75, step 4765, loss: 0.000000\n",
            "epoch 75, step 4766, loss: 28.929583\n",
            "epoch 75, step 4767, loss: 0.000000\n",
            "epoch 75, step 4768, loss: 0.000000\n",
            "epoch 75, step 4769, loss: 0.007103\n",
            "epoch 75, step 4770, loss: 1.265851\n",
            "epoch 75, step 4771, loss: 0.000000\n",
            "epoch 75, step 4772, loss: 0.000000\n",
            "epoch 75, step 4773, loss: 8.905017\n",
            "epoch 75, step 4774, loss: 0.000000\n",
            "epoch 75, step 4775, loss: 0.000000\n",
            "epoch 75, step 4776, loss: 0.000000\n",
            "epoch 75, step 4777, loss: 32.185223\n",
            "epoch 75, step 4778, loss: 0.000000\n",
            "epoch 75, step 4779, loss: 0.000000\n",
            "epoch 75, step 4780, loss: 0.000000\n",
            "epoch 75, step 4781, loss: 0.000000\n",
            "epoch 75, step 4782, loss: 23.411747\n",
            "epoch 75, step 4783, loss: 0.232143\n",
            "epoch 75, step 4784, loss: 0.000000\n",
            "epoch 75, step 4785, loss: 0.000000\n",
            "epoch 75, step 4786, loss: 4.021047\n",
            "epoch 75, step 4787, loss: 0.000000\n",
            "epoch 76, step 4788, loss: 137.628784\n",
            "epoch 76, step 4789, loss: 1.476377\n",
            "epoch 76, step 4790, loss: 16.661888\n",
            "epoch 76, step 4791, loss: 73.301636\n",
            "epoch 76, step 4792, loss: 0.000000\n",
            "epoch 76, step 4793, loss: 0.000000\n",
            "epoch 76, step 4794, loss: 0.000000\n",
            "epoch 76, step 4795, loss: 0.000000\n",
            "epoch 76, step 4796, loss: 0.000000\n",
            "epoch 76, step 4797, loss: 0.000000\n",
            "epoch 76, step 4798, loss: 0.000000\n",
            "epoch 76, step 4799, loss: 133.654846\n",
            "epoch 76, step 4800, loss: 0.000000\n",
            "epoch 76, step 4801, loss: 0.000001\n",
            "epoch 76, step 4802, loss: 0.000000\n",
            "epoch 76, step 4803, loss: 0.000000\n",
            "epoch 76, step 4804, loss: 0.000000\n",
            "epoch 76, step 4805, loss: 0.000000\n",
            "epoch 76, step 4806, loss: 0.000001\n",
            "epoch 76, step 4807, loss: 116.749146\n",
            "epoch 76, step 4808, loss: 0.000000\n",
            "epoch 76, step 4809, loss: 0.000000\n",
            "epoch 76, step 4810, loss: 0.000000\n",
            "epoch 76, step 4811, loss: 0.000000\n",
            "epoch 76, step 4812, loss: 0.000000\n",
            "epoch 76, step 4813, loss: 0.000000\n",
            "epoch 76, step 4814, loss: 0.000000\n",
            "epoch 76, step 4815, loss: 44.860519\n",
            "epoch 76, step 4816, loss: 0.000000\n",
            "epoch 76, step 4817, loss: 0.000000\n",
            "epoch 76, step 4818, loss: 0.000000\n",
            "epoch 76, step 4819, loss: 0.000000\n",
            "epoch 76, step 4820, loss: 0.000000\n",
            "epoch 76, step 4821, loss: 0.000001\n",
            "epoch 76, step 4822, loss: 0.000000\n",
            "epoch 76, step 4823, loss: 0.000000\n",
            "epoch 76, step 4824, loss: 0.024327\n",
            "epoch 76, step 4825, loss: 0.000000\n",
            "epoch 76, step 4826, loss: 0.000000\n",
            "epoch 76, step 4827, loss: 0.000001\n",
            "epoch 76, step 4828, loss: 0.000000\n",
            "epoch 76, step 4829, loss: 12.719521\n",
            "epoch 76, step 4830, loss: 0.381059\n",
            "epoch 76, step 4831, loss: 0.000000\n",
            "epoch 76, step 4832, loss: 5.994298\n",
            "epoch 76, step 4833, loss: 0.000027\n",
            "epoch 76, step 4834, loss: 0.045310\n",
            "epoch 76, step 4835, loss: 0.000000\n",
            "epoch 76, step 4836, loss: 0.000000\n",
            "epoch 76, step 4837, loss: 0.000000\n",
            "epoch 76, step 4838, loss: 0.000000\n",
            "epoch 76, step 4839, loss: 2.862374\n",
            "epoch 76, step 4840, loss: 0.006745\n",
            "epoch 76, step 4841, loss: 24.314268\n",
            "epoch 76, step 4842, loss: 0.000000\n",
            "epoch 76, step 4843, loss: 0.000000\n",
            "epoch 76, step 4844, loss: 16.382879\n",
            "epoch 76, step 4845, loss: 0.030902\n",
            "epoch 76, step 4846, loss: 29.886354\n",
            "epoch 76, step 4847, loss: 0.000002\n",
            "epoch 76, step 4848, loss: 0.000000\n",
            "epoch 76, step 4849, loss: 0.000000\n",
            "epoch 76, step 4850, loss: 10.215023\n",
            "epoch 77, step 4851, loss: 0.002940\n",
            "epoch 77, step 4852, loss: 0.000000\n",
            "epoch 77, step 4853, loss: 0.000000\n",
            "epoch 77, step 4854, loss: 237.623917\n",
            "epoch 77, step 4855, loss: 0.000000\n",
            "epoch 77, step 4856, loss: 0.000000\n",
            "epoch 77, step 4857, loss: 0.000000\n",
            "epoch 77, step 4858, loss: 0.000000\n",
            "epoch 77, step 4859, loss: 0.000000\n",
            "epoch 77, step 4860, loss: 27.472404\n",
            "epoch 77, step 4861, loss: 115.240257\n",
            "epoch 77, step 4862, loss: 16.458082\n",
            "epoch 77, step 4863, loss: 0.000000\n",
            "epoch 77, step 4864, loss: 0.000000\n",
            "epoch 77, step 4865, loss: 0.000000\n",
            "epoch 77, step 4866, loss: 40.422871\n",
            "epoch 77, step 4867, loss: 1.563345\n",
            "epoch 77, step 4868, loss: 0.000000\n",
            "epoch 77, step 4869, loss: 0.000000\n",
            "epoch 77, step 4870, loss: 54.985222\n",
            "epoch 77, step 4871, loss: 0.000000\n",
            "epoch 77, step 4872, loss: 0.000000\n",
            "epoch 77, step 4873, loss: 0.000000\n",
            "epoch 77, step 4874, loss: 0.000000\n",
            "epoch 77, step 4875, loss: 0.000000\n",
            "epoch 77, step 4876, loss: 84.642395\n",
            "epoch 77, step 4877, loss: 0.000000\n",
            "epoch 77, step 4878, loss: 0.000000\n",
            "epoch 77, step 4879, loss: 0.000000\n",
            "epoch 77, step 4880, loss: 0.000000\n",
            "epoch 77, step 4881, loss: 0.000000\n",
            "epoch 77, step 4882, loss: 0.000000\n",
            "epoch 77, step 4883, loss: 0.000000\n",
            "epoch 77, step 4884, loss: 0.000000\n",
            "epoch 77, step 4885, loss: 0.000000\n",
            "epoch 77, step 4886, loss: 0.000000\n",
            "epoch 77, step 4887, loss: 0.000000\n",
            "epoch 77, step 4888, loss: 0.000000\n",
            "epoch 77, step 4889, loss: 0.000000\n",
            "epoch 77, step 4890, loss: 0.000000\n",
            "epoch 77, step 4891, loss: 0.000000\n",
            "epoch 77, step 4892, loss: 30.325275\n",
            "epoch 77, step 4893, loss: 0.000000\n",
            "epoch 77, step 4894, loss: 0.000000\n",
            "epoch 77, step 4895, loss: 0.000001\n",
            "epoch 77, step 4896, loss: 0.000000\n",
            "epoch 77, step 4897, loss: 0.000000\n",
            "epoch 77, step 4898, loss: 0.000000\n",
            "epoch 77, step 4899, loss: 0.000000\n",
            "epoch 77, step 4900, loss: 0.000000\n",
            "epoch 77, step 4901, loss: 0.000000\n",
            "epoch 77, step 4902, loss: 0.000001\n",
            "epoch 77, step 4903, loss: 1.659184\n",
            "epoch 77, step 4904, loss: 0.000000\n",
            "epoch 77, step 4905, loss: 0.000000\n",
            "epoch 77, step 4906, loss: 0.000000\n",
            "epoch 77, step 4907, loss: 0.000000\n",
            "epoch 77, step 4908, loss: 0.005438\n",
            "epoch 77, step 4909, loss: 27.297544\n",
            "epoch 77, step 4910, loss: 0.000794\n",
            "epoch 77, step 4911, loss: 0.000000\n",
            "epoch 77, step 4912, loss: 0.000000\n",
            "epoch 77, step 4913, loss: 1.676161\n",
            "epoch 78, step 4914, loss: 0.000000\n",
            "epoch 78, step 4915, loss: 0.000000\n",
            "epoch 78, step 4916, loss: 0.000000\n",
            "epoch 78, step 4917, loss: 35.790154\n",
            "epoch 78, step 4918, loss: 0.000000\n",
            "epoch 78, step 4919, loss: 0.000000\n",
            "epoch 78, step 4920, loss: 0.000000\n",
            "epoch 78, step 4921, loss: 0.000000\n",
            "epoch 78, step 4922, loss: 0.014344\n",
            "epoch 78, step 4923, loss: 43.122215\n",
            "epoch 78, step 4924, loss: 103.011452\n",
            "epoch 78, step 4925, loss: 76.131592\n",
            "epoch 78, step 4926, loss: 0.000000\n",
            "epoch 78, step 4927, loss: 1.114423\n",
            "epoch 78, step 4928, loss: 0.000000\n",
            "epoch 78, step 4929, loss: 13.544877\n",
            "epoch 78, step 4930, loss: 0.000000\n",
            "epoch 78, step 4931, loss: 0.000000\n",
            "epoch 78, step 4932, loss: 0.000000\n",
            "epoch 78, step 4933, loss: 47.429073\n",
            "epoch 78, step 4934, loss: 0.000000\n",
            "epoch 78, step 4935, loss: 0.000000\n",
            "epoch 78, step 4936, loss: 0.000000\n",
            "epoch 78, step 4937, loss: 0.000000\n",
            "epoch 78, step 4938, loss: 0.000000\n",
            "epoch 78, step 4939, loss: 97.444290\n",
            "epoch 78, step 4940, loss: 0.000000\n",
            "epoch 78, step 4941, loss: 0.000003\n",
            "epoch 78, step 4942, loss: 0.000000\n",
            "epoch 78, step 4943, loss: 0.000000\n",
            "epoch 78, step 4944, loss: 0.000000\n",
            "epoch 78, step 4945, loss: 0.000000\n",
            "epoch 78, step 4946, loss: 0.000000\n",
            "epoch 78, step 4947, loss: 0.000000\n",
            "epoch 78, step 4948, loss: 0.000000\n",
            "epoch 78, step 4949, loss: 0.000000\n",
            "epoch 78, step 4950, loss: 0.000000\n",
            "epoch 78, step 4951, loss: 0.000000\n",
            "epoch 78, step 4952, loss: 0.000000\n",
            "epoch 78, step 4953, loss: 0.000000\n",
            "epoch 78, step 4954, loss: 0.000000\n",
            "epoch 78, step 4955, loss: 28.911970\n",
            "epoch 78, step 4956, loss: 0.000000\n",
            "epoch 78, step 4957, loss: 0.000000\n",
            "epoch 78, step 4958, loss: 0.000000\n",
            "epoch 78, step 4959, loss: 0.000000\n",
            "epoch 78, step 4960, loss: 0.000000\n",
            "epoch 78, step 4961, loss: 0.000000\n",
            "epoch 78, step 4962, loss: 0.000000\n",
            "epoch 78, step 4963, loss: 0.000000\n",
            "epoch 78, step 4964, loss: 0.000000\n",
            "epoch 78, step 4965, loss: 0.001012\n",
            "epoch 78, step 4966, loss: 0.000000\n",
            "epoch 78, step 4967, loss: 0.000000\n",
            "epoch 78, step 4968, loss: 0.000000\n",
            "epoch 78, step 4969, loss: 0.000000\n",
            "epoch 78, step 4970, loss: 0.000000\n",
            "epoch 78, step 4971, loss: 3.270205\n",
            "epoch 78, step 4972, loss: 20.940626\n",
            "epoch 78, step 4973, loss: 1.176920\n",
            "epoch 78, step 4974, loss: 0.000000\n",
            "epoch 78, step 4975, loss: 0.000000\n",
            "epoch 78, step 4976, loss: 20.966021\n",
            "epoch 79, step 4977, loss: 111.285805\n",
            "epoch 79, step 4978, loss: 0.000000\n",
            "epoch 79, step 4979, loss: 0.000000\n",
            "epoch 79, step 4980, loss: 66.621773\n",
            "epoch 79, step 4981, loss: 0.000008\n",
            "epoch 79, step 4982, loss: 0.000000\n",
            "epoch 79, step 4983, loss: 0.000000\n",
            "epoch 79, step 4984, loss: 0.000000\n",
            "epoch 79, step 4985, loss: 0.000000\n",
            "epoch 79, step 4986, loss: 0.000000\n",
            "epoch 79, step 4987, loss: 38.418625\n",
            "epoch 79, step 4988, loss: 39.187206\n",
            "epoch 79, step 4989, loss: 0.000000\n",
            "epoch 79, step 4990, loss: 0.000799\n",
            "epoch 79, step 4991, loss: 0.000000\n",
            "epoch 79, step 4992, loss: 0.000025\n",
            "epoch 79, step 4993, loss: 0.000000\n",
            "epoch 79, step 4994, loss: 0.000000\n",
            "epoch 79, step 4995, loss: 0.000224\n",
            "epoch 79, step 4996, loss: 104.065430\n",
            "epoch 79, step 4997, loss: 0.065840\n",
            "epoch 79, step 4998, loss: 0.000000\n",
            "epoch 79, step 4999, loss: 0.000000\n",
            "epoch 79, step 5000, loss: 0.000000\n",
            "epoch 79, step 5001, loss: 0.000000\n",
            "epoch 79, step 5002, loss: 40.660416\n",
            "epoch 79, step 5003, loss: 0.000000\n",
            "epoch 79, step 5004, loss: 0.000357\n",
            "epoch 79, step 5005, loss: 0.000144\n",
            "epoch 79, step 5006, loss: 0.000000\n",
            "epoch 79, step 5007, loss: 0.000000\n",
            "epoch 79, step 5008, loss: 0.000000\n",
            "epoch 79, step 5009, loss: 0.000000\n",
            "epoch 79, step 5010, loss: 0.000000\n",
            "epoch 79, step 5011, loss: 0.000000\n",
            "epoch 79, step 5012, loss: 0.000000\n",
            "epoch 79, step 5013, loss: 0.000000\n",
            "epoch 79, step 5014, loss: 0.000007\n",
            "epoch 79, step 5015, loss: 0.000000\n",
            "epoch 79, step 5016, loss: 0.000001\n",
            "epoch 79, step 5017, loss: 0.000000\n",
            "epoch 79, step 5018, loss: 18.322548\n",
            "epoch 79, step 5019, loss: 2.427087\n",
            "epoch 79, step 5020, loss: 0.000000\n",
            "epoch 79, step 5021, loss: 0.000000\n",
            "epoch 79, step 5022, loss: 0.000000\n",
            "epoch 79, step 5023, loss: 0.000000\n",
            "epoch 79, step 5024, loss: 0.000000\n",
            "epoch 79, step 5025, loss: 0.000000\n",
            "epoch 79, step 5026, loss: 0.000000\n",
            "epoch 79, step 5027, loss: 0.000000\n",
            "epoch 79, step 5028, loss: 12.133263\n",
            "epoch 79, step 5029, loss: 0.000000\n",
            "epoch 79, step 5030, loss: 0.000000\n",
            "epoch 79, step 5031, loss: 0.000000\n",
            "epoch 79, step 5032, loss: 0.000000\n",
            "epoch 79, step 5033, loss: 0.000000\n",
            "epoch 79, step 5034, loss: 0.429790\n",
            "epoch 79, step 5035, loss: 34.230927\n",
            "epoch 79, step 5036, loss: 0.000000\n",
            "epoch 79, step 5037, loss: 0.000000\n",
            "epoch 79, step 5038, loss: 0.000000\n",
            "epoch 79, step 5039, loss: 21.201160\n",
            "epoch 80, step 5040, loss: 0.000437\n",
            "epoch 80, step 5041, loss: 19.584589\n",
            "epoch 80, step 5042, loss: 0.000000\n",
            "epoch 80, step 5043, loss: 194.680740\n",
            "epoch 80, step 5044, loss: 0.000000\n",
            "epoch 80, step 5045, loss: 0.001777\n",
            "epoch 80, step 5046, loss: 0.000000\n",
            "epoch 80, step 5047, loss: 0.000000\n",
            "epoch 80, step 5048, loss: 12.170048\n",
            "epoch 80, step 5049, loss: 7.120182\n",
            "epoch 80, step 5050, loss: 80.032982\n",
            "epoch 80, step 5051, loss: 60.928234\n",
            "epoch 80, step 5052, loss: 0.000000\n",
            "epoch 80, step 5053, loss: 0.000000\n",
            "epoch 80, step 5054, loss: 0.000000\n",
            "epoch 80, step 5055, loss: 1.535612\n",
            "epoch 80, step 5056, loss: 0.000000\n",
            "epoch 80, step 5057, loss: 0.000000\n",
            "epoch 80, step 5058, loss: 0.000025\n",
            "epoch 80, step 5059, loss: 99.854416\n",
            "epoch 80, step 5060, loss: 0.000000\n",
            "epoch 80, step 5061, loss: 0.000000\n",
            "epoch 80, step 5062, loss: 0.000000\n",
            "epoch 80, step 5063, loss: 0.000000\n",
            "epoch 80, step 5064, loss: 0.000000\n",
            "epoch 80, step 5065, loss: 55.687729\n",
            "epoch 80, step 5066, loss: 0.000000\n",
            "epoch 80, step 5067, loss: 0.003348\n",
            "epoch 80, step 5068, loss: 0.000000\n",
            "epoch 80, step 5069, loss: 0.000000\n",
            "epoch 80, step 5070, loss: 140.779892\n",
            "epoch 80, step 5071, loss: 0.000000\n",
            "epoch 80, step 5072, loss: 0.000000\n",
            "epoch 80, step 5073, loss: 0.000000\n",
            "epoch 80, step 5074, loss: 0.000000\n",
            "epoch 80, step 5075, loss: 0.000000\n",
            "epoch 80, step 5076, loss: 0.000000\n",
            "epoch 80, step 5077, loss: 0.000000\n",
            "epoch 80, step 5078, loss: 0.000000\n",
            "epoch 80, step 5079, loss: 0.000000\n",
            "epoch 80, step 5080, loss: 4.790432\n",
            "epoch 80, step 5081, loss: 26.638882\n",
            "epoch 80, step 5082, loss: 0.000000\n",
            "epoch 80, step 5083, loss: 0.000000\n",
            "epoch 80, step 5084, loss: 0.000000\n",
            "epoch 80, step 5085, loss: 0.000000\n",
            "epoch 80, step 5086, loss: 0.000000\n",
            "epoch 80, step 5087, loss: 0.000000\n",
            "epoch 80, step 5088, loss: 0.000000\n",
            "epoch 80, step 5089, loss: 0.000000\n",
            "epoch 80, step 5090, loss: 0.000000\n",
            "epoch 80, step 5091, loss: 32.571697\n",
            "epoch 80, step 5092, loss: 0.000000\n",
            "epoch 80, step 5093, loss: 0.000000\n",
            "epoch 80, step 5094, loss: 0.000000\n",
            "epoch 80, step 5095, loss: 0.000000\n",
            "epoch 80, step 5096, loss: 0.000000\n",
            "epoch 80, step 5097, loss: 0.000000\n",
            "epoch 80, step 5098, loss: 46.180828\n",
            "epoch 80, step 5099, loss: 0.000000\n",
            "epoch 80, step 5100, loss: 0.000000\n",
            "epoch 80, step 5101, loss: 0.000000\n",
            "epoch 80, step 5102, loss: 37.845310\n",
            "epoch 81, step 5103, loss: 7.059584\n",
            "epoch 81, step 5104, loss: 0.000000\n",
            "epoch 81, step 5105, loss: 0.000000\n",
            "epoch 81, step 5106, loss: 49.536022\n",
            "epoch 81, step 5107, loss: 0.000000\n",
            "epoch 81, step 5108, loss: 0.000000\n",
            "epoch 81, step 5109, loss: 0.000000\n",
            "epoch 81, step 5110, loss: 0.000000\n",
            "epoch 81, step 5111, loss: 8.547251\n",
            "epoch 81, step 5112, loss: 1.555221\n",
            "epoch 81, step 5113, loss: 79.744652\n",
            "epoch 81, step 5114, loss: 11.632892\n",
            "epoch 81, step 5115, loss: 0.000000\n",
            "epoch 81, step 5116, loss: 0.000000\n",
            "epoch 81, step 5117, loss: 0.000000\n",
            "epoch 81, step 5118, loss: 7.498584\n",
            "epoch 81, step 5119, loss: 0.000000\n",
            "epoch 81, step 5120, loss: 0.000000\n",
            "epoch 81, step 5121, loss: 0.000001\n",
            "epoch 81, step 5122, loss: 71.738953\n",
            "epoch 81, step 5123, loss: 0.000000\n",
            "epoch 81, step 5124, loss: 0.050980\n",
            "epoch 81, step 5125, loss: 0.000000\n",
            "epoch 81, step 5126, loss: 0.000000\n",
            "epoch 81, step 5127, loss: 0.000000\n",
            "epoch 81, step 5128, loss: 91.682243\n",
            "epoch 81, step 5129, loss: 0.000000\n",
            "epoch 81, step 5130, loss: 0.000001\n",
            "epoch 81, step 5131, loss: 0.000001\n",
            "epoch 81, step 5132, loss: 0.000000\n",
            "epoch 81, step 5133, loss: 12.636610\n",
            "epoch 81, step 5134, loss: 1.031957\n",
            "epoch 81, step 5135, loss: 0.000000\n",
            "epoch 81, step 5136, loss: 0.000019\n",
            "epoch 81, step 5137, loss: 0.000000\n",
            "epoch 81, step 5138, loss: 3.485576\n",
            "epoch 81, step 5139, loss: 0.000000\n",
            "epoch 81, step 5140, loss: 0.000000\n",
            "epoch 81, step 5141, loss: 9.181084\n",
            "epoch 81, step 5142, loss: 0.000000\n",
            "epoch 81, step 5143, loss: 0.000000\n",
            "epoch 81, step 5144, loss: 21.036829\n",
            "epoch 81, step 5145, loss: 0.000078\n",
            "epoch 81, step 5146, loss: 0.000000\n",
            "epoch 81, step 5147, loss: 0.000000\n",
            "epoch 81, step 5148, loss: 0.000000\n",
            "epoch 81, step 5149, loss: 0.000000\n",
            "epoch 81, step 5150, loss: 0.000000\n",
            "epoch 81, step 5151, loss: 0.000000\n",
            "epoch 81, step 5152, loss: 0.000000\n",
            "epoch 81, step 5153, loss: 0.000000\n",
            "epoch 81, step 5154, loss: 0.007230\n",
            "epoch 81, step 5155, loss: 0.000000\n",
            "epoch 81, step 5156, loss: 0.000000\n",
            "epoch 81, step 5157, loss: 0.000000\n",
            "epoch 81, step 5158, loss: 0.000000\n",
            "epoch 81, step 5159, loss: 0.000000\n",
            "epoch 81, step 5160, loss: 0.038297\n",
            "epoch 81, step 5161, loss: 19.433945\n",
            "epoch 81, step 5162, loss: 0.000000\n",
            "epoch 81, step 5163, loss: 0.000000\n",
            "epoch 81, step 5164, loss: 0.000000\n",
            "epoch 81, step 5165, loss: 11.956727\n",
            "epoch 82, step 5166, loss: 139.327469\n",
            "epoch 82, step 5167, loss: 0.000000\n",
            "epoch 82, step 5168, loss: 0.000000\n",
            "epoch 82, step 5169, loss: 95.536270\n",
            "epoch 82, step 5170, loss: 0.000000\n",
            "epoch 82, step 5171, loss: 0.000000\n",
            "epoch 82, step 5172, loss: 0.000000\n",
            "epoch 82, step 5173, loss: 0.000000\n",
            "epoch 82, step 5174, loss: 0.000000\n",
            "epoch 82, step 5175, loss: 0.011569\n",
            "epoch 82, step 5176, loss: 53.004910\n",
            "epoch 82, step 5177, loss: 32.126362\n",
            "epoch 82, step 5178, loss: 0.000000\n",
            "epoch 82, step 5179, loss: 58.892548\n",
            "epoch 82, step 5180, loss: 0.000000\n",
            "epoch 82, step 5181, loss: 2.011713\n",
            "epoch 82, step 5182, loss: 0.000000\n",
            "epoch 82, step 5183, loss: 0.000000\n",
            "epoch 82, step 5184, loss: 0.070820\n",
            "epoch 82, step 5185, loss: 31.906698\n",
            "epoch 82, step 5186, loss: 0.013507\n",
            "epoch 82, step 5187, loss: 0.000000\n",
            "epoch 82, step 5188, loss: 0.000000\n",
            "epoch 82, step 5189, loss: 0.000000\n",
            "epoch 82, step 5190, loss: 0.000000\n",
            "epoch 82, step 5191, loss: 41.550262\n",
            "epoch 82, step 5192, loss: 0.000000\n",
            "epoch 82, step 5193, loss: 0.027853\n",
            "epoch 82, step 5194, loss: 0.000001\n",
            "epoch 82, step 5195, loss: 0.000000\n",
            "epoch 82, step 5196, loss: 0.000000\n",
            "epoch 82, step 5197, loss: 0.000107\n",
            "epoch 82, step 5198, loss: 0.000000\n",
            "epoch 82, step 5199, loss: 0.000000\n",
            "epoch 82, step 5200, loss: 0.000181\n",
            "epoch 82, step 5201, loss: 0.000000\n",
            "epoch 82, step 5202, loss: 0.000000\n",
            "epoch 82, step 5203, loss: 0.000001\n",
            "epoch 82, step 5204, loss: 0.000000\n",
            "epoch 82, step 5205, loss: 0.000000\n",
            "epoch 82, step 5206, loss: 0.000000\n",
            "epoch 82, step 5207, loss: 23.535267\n",
            "epoch 82, step 5208, loss: 0.000000\n",
            "epoch 82, step 5209, loss: 0.000000\n",
            "epoch 82, step 5210, loss: 0.000000\n",
            "epoch 82, step 5211, loss: 0.000000\n",
            "epoch 82, step 5212, loss: 0.000000\n",
            "epoch 82, step 5213, loss: 0.000000\n",
            "epoch 82, step 5214, loss: 2.747223\n",
            "epoch 82, step 5215, loss: 0.000000\n",
            "epoch 82, step 5216, loss: 0.000000\n",
            "epoch 82, step 5217, loss: 0.004164\n",
            "epoch 82, step 5218, loss: 0.000000\n",
            "epoch 82, step 5219, loss: 0.000000\n",
            "epoch 82, step 5220, loss: 0.000000\n",
            "epoch 82, step 5221, loss: 0.000000\n",
            "epoch 82, step 5222, loss: 0.000000\n",
            "epoch 82, step 5223, loss: 61.797447\n",
            "epoch 82, step 5224, loss: 59.338493\n",
            "epoch 82, step 5225, loss: 0.000000\n",
            "epoch 82, step 5226, loss: 0.000000\n",
            "epoch 82, step 5227, loss: 0.000000\n",
            "epoch 82, step 5228, loss: 37.483562\n",
            "epoch 83, step 5229, loss: 0.000000\n",
            "epoch 83, step 5230, loss: 0.000000\n",
            "epoch 83, step 5231, loss: 0.000000\n",
            "epoch 83, step 5232, loss: 238.918335\n",
            "epoch 83, step 5233, loss: 0.000000\n",
            "epoch 83, step 5234, loss: 0.000003\n",
            "epoch 83, step 5235, loss: 0.000000\n",
            "epoch 83, step 5236, loss: 0.000000\n",
            "epoch 83, step 5237, loss: 0.000000\n",
            "epoch 83, step 5238, loss: 21.832626\n",
            "epoch 83, step 5239, loss: 69.135124\n",
            "epoch 83, step 5240, loss: 55.229263\n",
            "epoch 83, step 5241, loss: 0.000000\n",
            "epoch 83, step 5242, loss: 0.000000\n",
            "epoch 83, step 5243, loss: 0.000000\n",
            "epoch 83, step 5244, loss: 6.354666\n",
            "epoch 83, step 5245, loss: 0.000000\n",
            "epoch 83, step 5246, loss: 0.000000\n",
            "epoch 83, step 5247, loss: 0.000000\n",
            "epoch 83, step 5248, loss: 74.698380\n",
            "epoch 83, step 5249, loss: 0.000000\n",
            "epoch 83, step 5250, loss: 0.000014\n",
            "epoch 83, step 5251, loss: 0.000000\n",
            "epoch 83, step 5252, loss: 0.000000\n",
            "epoch 83, step 5253, loss: 0.000000\n",
            "epoch 83, step 5254, loss: 72.170204\n",
            "epoch 83, step 5255, loss: 0.000000\n",
            "epoch 83, step 5256, loss: 0.000001\n",
            "epoch 83, step 5257, loss: 0.006266\n",
            "epoch 83, step 5258, loss: 0.000000\n",
            "epoch 83, step 5259, loss: 0.000000\n",
            "epoch 83, step 5260, loss: 0.000512\n",
            "epoch 83, step 5261, loss: 0.000000\n",
            "epoch 83, step 5262, loss: 0.042894\n",
            "epoch 83, step 5263, loss: 23.676327\n",
            "epoch 83, step 5264, loss: 0.004884\n",
            "epoch 83, step 5265, loss: 0.000023\n",
            "epoch 83, step 5266, loss: 0.000000\n",
            "epoch 83, step 5267, loss: 0.000000\n",
            "epoch 83, step 5268, loss: 0.000000\n",
            "epoch 83, step 5269, loss: 0.000000\n",
            "epoch 83, step 5270, loss: 30.140541\n",
            "epoch 83, step 5271, loss: 8.810737\n",
            "epoch 83, step 5272, loss: 0.000000\n",
            "epoch 83, step 5273, loss: 0.000000\n",
            "epoch 83, step 5274, loss: 0.000000\n",
            "epoch 83, step 5275, loss: 0.000000\n",
            "epoch 83, step 5276, loss: 0.000000\n",
            "epoch 83, step 5277, loss: 0.000000\n",
            "epoch 83, step 5278, loss: 0.000000\n",
            "epoch 83, step 5279, loss: 0.000000\n",
            "epoch 83, step 5280, loss: 7.867095\n",
            "epoch 83, step 5281, loss: 0.000000\n",
            "epoch 83, step 5282, loss: 0.000000\n",
            "epoch 83, step 5283, loss: 0.000000\n",
            "epoch 83, step 5284, loss: 0.000000\n",
            "epoch 83, step 5285, loss: 0.000000\n",
            "epoch 83, step 5286, loss: 0.000000\n",
            "epoch 83, step 5287, loss: 55.998123\n",
            "epoch 83, step 5288, loss: 0.000000\n",
            "epoch 83, step 5289, loss: 0.000000\n",
            "epoch 83, step 5290, loss: 0.000000\n",
            "epoch 83, step 5291, loss: 37.075024\n",
            "epoch 84, step 5292, loss: 0.000000\n",
            "epoch 84, step 5293, loss: 0.000000\n",
            "epoch 84, step 5294, loss: 0.000000\n",
            "epoch 84, step 5295, loss: 29.645180\n",
            "epoch 84, step 5296, loss: 0.000000\n",
            "epoch 84, step 5297, loss: 0.000000\n",
            "epoch 84, step 5298, loss: 0.000000\n",
            "epoch 84, step 5299, loss: 0.000000\n",
            "epoch 84, step 5300, loss: 0.000000\n",
            "epoch 84, step 5301, loss: 18.164421\n",
            "epoch 84, step 5302, loss: 84.219681\n",
            "epoch 84, step 5303, loss: 95.718140\n",
            "epoch 84, step 5304, loss: 0.000000\n",
            "epoch 84, step 5305, loss: 0.000000\n",
            "epoch 84, step 5306, loss: 0.000000\n",
            "epoch 84, step 5307, loss: 13.895591\n",
            "epoch 84, step 5308, loss: 0.000000\n",
            "epoch 84, step 5309, loss: 0.000000\n",
            "epoch 84, step 5310, loss: 0.000000\n",
            "epoch 84, step 5311, loss: 66.957680\n",
            "epoch 84, step 5312, loss: 0.000000\n",
            "epoch 84, step 5313, loss: 0.000000\n",
            "epoch 84, step 5314, loss: 0.000000\n",
            "epoch 84, step 5315, loss: 0.000000\n",
            "epoch 84, step 5316, loss: 0.000000\n",
            "epoch 84, step 5317, loss: 102.694237\n",
            "epoch 84, step 5318, loss: 0.000000\n",
            "epoch 84, step 5319, loss: 0.000666\n",
            "epoch 84, step 5320, loss: 0.000101\n",
            "epoch 84, step 5321, loss: 0.000000\n",
            "epoch 84, step 5322, loss: 0.000000\n",
            "epoch 84, step 5323, loss: 0.000000\n",
            "epoch 84, step 5324, loss: 0.000000\n",
            "epoch 84, step 5325, loss: 0.000278\n",
            "epoch 84, step 5326, loss: 0.000866\n",
            "epoch 84, step 5327, loss: 0.000000\n",
            "epoch 84, step 5328, loss: 0.000000\n",
            "epoch 84, step 5329, loss: 0.000000\n",
            "epoch 84, step 5330, loss: 0.000000\n",
            "epoch 84, step 5331, loss: 0.000000\n",
            "epoch 84, step 5332, loss: 0.000000\n",
            "epoch 84, step 5333, loss: 21.889879\n",
            "epoch 84, step 5334, loss: 0.000000\n",
            "epoch 84, step 5335, loss: 0.000000\n",
            "epoch 84, step 5336, loss: 0.000000\n",
            "epoch 84, step 5337, loss: 0.000000\n",
            "epoch 84, step 5338, loss: 0.000000\n",
            "epoch 84, step 5339, loss: 0.000000\n",
            "epoch 84, step 5340, loss: 0.000000\n",
            "epoch 84, step 5341, loss: 0.000000\n",
            "epoch 84, step 5342, loss: 0.000000\n",
            "epoch 84, step 5343, loss: 0.005817\n",
            "epoch 84, step 5344, loss: 0.000000\n",
            "epoch 84, step 5345, loss: 0.000000\n",
            "epoch 84, step 5346, loss: 0.000000\n",
            "epoch 84, step 5347, loss: 0.000000\n",
            "epoch 84, step 5348, loss: 0.000000\n",
            "epoch 84, step 5349, loss: 0.000001\n",
            "epoch 84, step 5350, loss: 6.736595\n",
            "epoch 84, step 5351, loss: 0.000000\n",
            "epoch 84, step 5352, loss: 0.000000\n",
            "epoch 84, step 5353, loss: 0.000000\n",
            "epoch 84, step 5354, loss: 8.763319\n",
            "epoch 85, step 5355, loss: 157.873657\n",
            "epoch 85, step 5356, loss: 0.000000\n",
            "epoch 85, step 5357, loss: 0.000000\n",
            "epoch 85, step 5358, loss: 114.051666\n",
            "epoch 85, step 5359, loss: 0.000000\n",
            "epoch 85, step 5360, loss: 0.000000\n",
            "epoch 85, step 5361, loss: 0.000000\n",
            "epoch 85, step 5362, loss: 0.000000\n",
            "epoch 85, step 5363, loss: 0.000000\n",
            "epoch 85, step 5364, loss: 0.148651\n",
            "epoch 85, step 5365, loss: 55.848721\n",
            "epoch 85, step 5366, loss: 10.260277\n",
            "epoch 85, step 5367, loss: 0.000000\n",
            "epoch 85, step 5368, loss: 0.000000\n",
            "epoch 85, step 5369, loss: 0.000000\n",
            "epoch 85, step 5370, loss: 2.727273\n",
            "epoch 85, step 5371, loss: 0.000000\n",
            "epoch 85, step 5372, loss: 0.000000\n",
            "epoch 85, step 5373, loss: 0.000000\n",
            "epoch 85, step 5374, loss: 38.693176\n",
            "epoch 85, step 5375, loss: 0.000000\n",
            "epoch 85, step 5376, loss: 0.000000\n",
            "epoch 85, step 5377, loss: 0.000000\n",
            "epoch 85, step 5378, loss: 0.000000\n",
            "epoch 85, step 5379, loss: 0.000000\n",
            "epoch 85, step 5380, loss: 49.218185\n",
            "epoch 85, step 5381, loss: 0.000000\n",
            "epoch 85, step 5382, loss: 0.000000\n",
            "epoch 85, step 5383, loss: 0.000000\n",
            "epoch 85, step 5384, loss: 0.000000\n",
            "epoch 85, step 5385, loss: 0.000000\n",
            "epoch 85, step 5386, loss: 0.000000\n",
            "epoch 85, step 5387, loss: 1.937052\n",
            "epoch 85, step 5388, loss: 0.000000\n",
            "epoch 85, step 5389, loss: 0.000000\n",
            "epoch 85, step 5390, loss: 0.000000\n",
            "epoch 85, step 5391, loss: 0.000000\n",
            "epoch 85, step 5392, loss: 0.000000\n",
            "epoch 85, step 5393, loss: 0.000000\n",
            "epoch 85, step 5394, loss: 0.000000\n",
            "epoch 85, step 5395, loss: 0.000000\n",
            "epoch 85, step 5396, loss: 16.720383\n",
            "epoch 85, step 5397, loss: 0.000000\n",
            "epoch 85, step 5398, loss: 0.000000\n",
            "epoch 85, step 5399, loss: 0.000000\n",
            "epoch 85, step 5400, loss: 0.000000\n",
            "epoch 85, step 5401, loss: 0.000000\n",
            "epoch 85, step 5402, loss: 0.000000\n",
            "epoch 85, step 5403, loss: 0.000000\n",
            "epoch 85, step 5404, loss: 0.000000\n",
            "epoch 85, step 5405, loss: 0.000000\n",
            "epoch 85, step 5406, loss: 4.549910\n",
            "epoch 85, step 5407, loss: 0.000000\n",
            "epoch 85, step 5408, loss: 0.000000\n",
            "epoch 85, step 5409, loss: 0.000000\n",
            "epoch 85, step 5410, loss: 0.000000\n",
            "epoch 85, step 5411, loss: 0.000000\n",
            "epoch 85, step 5412, loss: 75.012207\n",
            "epoch 85, step 5413, loss: 24.668140\n",
            "epoch 85, step 5414, loss: 0.000000\n",
            "epoch 85, step 5415, loss: 0.000000\n",
            "epoch 85, step 5416, loss: 0.000000\n",
            "epoch 85, step 5417, loss: 21.535435\n",
            "epoch 86, step 5418, loss: 0.000000\n",
            "epoch 86, step 5419, loss: 0.000000\n",
            "epoch 86, step 5420, loss: 0.000000\n",
            "epoch 86, step 5421, loss: 191.030838\n",
            "epoch 86, step 5422, loss: 0.000000\n",
            "epoch 86, step 5423, loss: 0.000000\n",
            "epoch 86, step 5424, loss: 0.000000\n",
            "epoch 86, step 5425, loss: 0.000000\n",
            "epoch 86, step 5426, loss: 0.000000\n",
            "epoch 86, step 5427, loss: 33.818363\n",
            "epoch 86, step 5428, loss: 82.324211\n",
            "epoch 86, step 5429, loss: 77.777153\n",
            "epoch 86, step 5430, loss: 0.000000\n",
            "epoch 86, step 5431, loss: 0.000000\n",
            "epoch 86, step 5432, loss: 0.000000\n",
            "epoch 86, step 5433, loss: 0.000001\n",
            "epoch 86, step 5434, loss: 0.000000\n",
            "epoch 86, step 5435, loss: 0.000000\n",
            "epoch 86, step 5436, loss: 0.000000\n",
            "epoch 86, step 5437, loss: 85.164627\n",
            "epoch 86, step 5438, loss: 0.000000\n",
            "epoch 86, step 5439, loss: 0.000000\n",
            "epoch 86, step 5440, loss: 0.000000\n",
            "epoch 86, step 5441, loss: 0.000000\n",
            "epoch 86, step 5442, loss: 0.000000\n",
            "epoch 86, step 5443, loss: 83.789558\n",
            "epoch 86, step 5444, loss: 0.000000\n",
            "epoch 86, step 5445, loss: 0.000000\n",
            "epoch 86, step 5446, loss: 0.000000\n",
            "epoch 86, step 5447, loss: 0.000000\n",
            "epoch 86, step 5448, loss: 0.000000\n",
            "epoch 86, step 5449, loss: 0.000000\n",
            "epoch 86, step 5450, loss: 0.000000\n",
            "epoch 86, step 5451, loss: 0.000000\n",
            "epoch 86, step 5452, loss: 0.000000\n",
            "epoch 86, step 5453, loss: 0.000000\n",
            "epoch 86, step 5454, loss: 0.000000\n",
            "epoch 86, step 5455, loss: 0.000000\n",
            "epoch 86, step 5456, loss: 0.000000\n",
            "epoch 86, step 5457, loss: 0.000000\n",
            "epoch 86, step 5458, loss: 0.000000\n",
            "epoch 86, step 5459, loss: 32.271034\n",
            "epoch 86, step 5460, loss: 0.000000\n",
            "epoch 86, step 5461, loss: 0.000000\n",
            "epoch 86, step 5462, loss: 0.000000\n",
            "epoch 86, step 5463, loss: 0.000019\n",
            "epoch 86, step 5464, loss: 0.000000\n",
            "epoch 86, step 5465, loss: 0.000000\n",
            "epoch 86, step 5466, loss: 9.369609\n",
            "epoch 86, step 5467, loss: 0.000000\n",
            "epoch 86, step 5468, loss: 0.000000\n",
            "epoch 86, step 5469, loss: 0.000002\n",
            "epoch 86, step 5470, loss: 0.000000\n",
            "epoch 86, step 5471, loss: 0.000000\n",
            "epoch 86, step 5472, loss: 0.000000\n",
            "epoch 86, step 5473, loss: 0.000000\n",
            "epoch 86, step 5474, loss: 0.012427\n",
            "epoch 86, step 5475, loss: 0.000000\n",
            "epoch 86, step 5476, loss: 12.619793\n",
            "epoch 86, step 5477, loss: 0.000000\n",
            "epoch 86, step 5478, loss: 0.000000\n",
            "epoch 86, step 5479, loss: 0.000000\n",
            "epoch 86, step 5480, loss: 15.166307\n",
            "epoch 87, step 5481, loss: 0.728614\n",
            "epoch 87, step 5482, loss: 0.000000\n",
            "epoch 87, step 5483, loss: 0.000000\n",
            "epoch 87, step 5484, loss: 5.769669\n",
            "epoch 87, step 5485, loss: 0.000002\n",
            "epoch 87, step 5486, loss: 0.008715\n",
            "epoch 87, step 5487, loss: 0.000000\n",
            "epoch 87, step 5488, loss: 0.000000\n",
            "epoch 87, step 5489, loss: 0.000000\n",
            "epoch 87, step 5490, loss: 31.739225\n",
            "epoch 87, step 5491, loss: 101.394218\n",
            "epoch 87, step 5492, loss: 11.643070\n",
            "epoch 87, step 5493, loss: 0.000000\n",
            "epoch 87, step 5494, loss: 0.000000\n",
            "epoch 87, step 5495, loss: 0.000000\n",
            "epoch 87, step 5496, loss: 17.012968\n",
            "epoch 87, step 5497, loss: 0.000000\n",
            "epoch 87, step 5498, loss: 0.000000\n",
            "epoch 87, step 5499, loss: 0.011881\n",
            "epoch 87, step 5500, loss: 54.261253\n",
            "epoch 87, step 5501, loss: 0.000000\n",
            "epoch 87, step 5502, loss: 0.000000\n",
            "epoch 87, step 5503, loss: 0.000000\n",
            "epoch 87, step 5504, loss: 0.000000\n",
            "epoch 87, step 5505, loss: 0.000000\n",
            "epoch 87, step 5506, loss: 133.850845\n",
            "epoch 87, step 5507, loss: 0.000000\n",
            "epoch 87, step 5508, loss: 0.005978\n",
            "epoch 87, step 5509, loss: 0.001005\n",
            "epoch 87, step 5510, loss: 0.000000\n",
            "epoch 87, step 5511, loss: 0.000000\n",
            "epoch 87, step 5512, loss: 0.000000\n",
            "epoch 87, step 5513, loss: 0.000000\n",
            "epoch 87, step 5514, loss: 0.000000\n",
            "epoch 87, step 5515, loss: 0.192358\n",
            "epoch 87, step 5516, loss: 0.000000\n",
            "epoch 87, step 5517, loss: 0.000000\n",
            "epoch 87, step 5518, loss: 0.000000\n",
            "epoch 87, step 5519, loss: 0.000000\n",
            "epoch 87, step 5520, loss: 0.000000\n",
            "epoch 87, step 5521, loss: 0.000000\n",
            "epoch 87, step 5522, loss: 20.523678\n",
            "epoch 87, step 5523, loss: 0.254814\n",
            "epoch 87, step 5524, loss: 0.000000\n",
            "epoch 87, step 5525, loss: 0.000000\n",
            "epoch 87, step 5526, loss: 0.000000\n",
            "epoch 87, step 5527, loss: 0.000000\n",
            "epoch 87, step 5528, loss: 1.132107\n",
            "epoch 87, step 5529, loss: 0.000000\n",
            "epoch 87, step 5530, loss: 0.000000\n",
            "epoch 87, step 5531, loss: 0.000000\n",
            "epoch 87, step 5532, loss: 0.002099\n",
            "epoch 87, step 5533, loss: 0.000000\n",
            "epoch 87, step 5534, loss: 0.000000\n",
            "epoch 87, step 5535, loss: 0.000000\n",
            "epoch 87, step 5536, loss: 0.000000\n",
            "epoch 87, step 5537, loss: 0.000000\n",
            "epoch 87, step 5538, loss: 0.000000\n",
            "epoch 87, step 5539, loss: 0.643473\n",
            "epoch 87, step 5540, loss: 0.000000\n",
            "epoch 87, step 5541, loss: 0.000000\n",
            "epoch 87, step 5542, loss: 0.000000\n",
            "epoch 87, step 5543, loss: 21.097906\n",
            "epoch 88, step 5544, loss: 137.027817\n",
            "epoch 88, step 5545, loss: 0.000000\n",
            "epoch 88, step 5546, loss: 0.000013\n",
            "epoch 88, step 5547, loss: 86.594345\n",
            "epoch 88, step 5548, loss: 0.000000\n",
            "epoch 88, step 5549, loss: 0.000000\n",
            "epoch 88, step 5550, loss: 0.000000\n",
            "epoch 88, step 5551, loss: 0.000000\n",
            "epoch 88, step 5552, loss: 0.000000\n",
            "epoch 88, step 5553, loss: 0.000000\n",
            "epoch 88, step 5554, loss: 32.902153\n",
            "epoch 88, step 5555, loss: 118.858154\n",
            "epoch 88, step 5556, loss: 0.000000\n",
            "epoch 88, step 5557, loss: 0.000000\n",
            "epoch 88, step 5558, loss: 0.000000\n",
            "epoch 88, step 5559, loss: 0.000188\n",
            "epoch 88, step 5560, loss: 0.000000\n",
            "epoch 88, step 5561, loss: 0.000000\n",
            "epoch 88, step 5562, loss: 0.000156\n",
            "epoch 88, step 5563, loss: 38.788620\n",
            "epoch 88, step 5564, loss: 0.193175\n",
            "epoch 88, step 5565, loss: 0.000000\n",
            "epoch 88, step 5566, loss: 0.000000\n",
            "epoch 88, step 5567, loss: 0.000000\n",
            "epoch 88, step 5568, loss: 0.000000\n",
            "epoch 88, step 5569, loss: 0.515655\n",
            "epoch 88, step 5570, loss: 0.000000\n",
            "epoch 88, step 5571, loss: 0.054867\n",
            "epoch 88, step 5572, loss: 0.051145\n",
            "epoch 88, step 5573, loss: 0.000000\n",
            "epoch 88, step 5574, loss: 0.000000\n",
            "epoch 88, step 5575, loss: 0.000000\n",
            "epoch 88, step 5576, loss: 0.000000\n",
            "epoch 88, step 5577, loss: 0.000000\n",
            "epoch 88, step 5578, loss: 0.000000\n",
            "epoch 88, step 5579, loss: 0.000000\n",
            "epoch 88, step 5580, loss: 0.000000\n",
            "epoch 88, step 5581, loss: 0.000000\n",
            "epoch 88, step 5582, loss: 0.000000\n",
            "epoch 88, step 5583, loss: 0.018351\n",
            "epoch 88, step 5584, loss: 0.000000\n",
            "epoch 88, step 5585, loss: 4.200649\n",
            "epoch 88, step 5586, loss: 0.000000\n",
            "epoch 88, step 5587, loss: 0.000000\n",
            "epoch 88, step 5588, loss: 0.000312\n",
            "epoch 88, step 5589, loss: 0.000012\n",
            "epoch 88, step 5590, loss: 0.000000\n",
            "epoch 88, step 5591, loss: 0.000000\n",
            "epoch 88, step 5592, loss: 0.000000\n",
            "epoch 88, step 5593, loss: 0.000000\n",
            "epoch 88, step 5594, loss: 0.000000\n",
            "epoch 88, step 5595, loss: 13.398563\n",
            "epoch 88, step 5596, loss: 0.000000\n",
            "epoch 88, step 5597, loss: 0.000000\n",
            "epoch 88, step 5598, loss: 0.000000\n",
            "epoch 88, step 5599, loss: 0.000000\n",
            "epoch 88, step 5600, loss: 0.000000\n",
            "epoch 88, step 5601, loss: 67.523956\n",
            "epoch 88, step 5602, loss: 46.057480\n",
            "epoch 88, step 5603, loss: 0.000000\n",
            "epoch 88, step 5604, loss: 0.000000\n",
            "epoch 88, step 5605, loss: 0.000000\n",
            "epoch 88, step 5606, loss: 38.475765\n",
            "epoch 89, step 5607, loss: 0.000000\n",
            "epoch 89, step 5608, loss: 0.000000\n",
            "epoch 89, step 5609, loss: 0.000000\n",
            "epoch 89, step 5610, loss: 213.666458\n",
            "epoch 89, step 5611, loss: 0.000000\n",
            "epoch 89, step 5612, loss: 0.000366\n",
            "epoch 89, step 5613, loss: 0.000000\n",
            "epoch 89, step 5614, loss: 0.000000\n",
            "epoch 89, step 5615, loss: 0.000000\n",
            "epoch 89, step 5616, loss: 33.696583\n",
            "epoch 89, step 5617, loss: 80.495155\n",
            "epoch 89, step 5618, loss: 3.024896\n",
            "epoch 89, step 5619, loss: 0.000000\n",
            "epoch 89, step 5620, loss: 0.000000\n",
            "epoch 89, step 5621, loss: 0.000000\n",
            "epoch 89, step 5622, loss: 11.611097\n",
            "epoch 89, step 5623, loss: 0.000000\n",
            "epoch 89, step 5624, loss: 15.673882\n",
            "epoch 89, step 5625, loss: 0.002223\n",
            "epoch 89, step 5626, loss: 61.417278\n",
            "epoch 89, step 5627, loss: 0.000000\n",
            "epoch 89, step 5628, loss: 0.000000\n",
            "epoch 89, step 5629, loss: 0.000000\n",
            "epoch 89, step 5630, loss: 0.000000\n",
            "epoch 89, step 5631, loss: 0.000000\n",
            "epoch 89, step 5632, loss: 114.955742\n",
            "epoch 89, step 5633, loss: 0.000000\n",
            "epoch 89, step 5634, loss: 0.000040\n",
            "epoch 89, step 5635, loss: 0.000005\n",
            "epoch 89, step 5636, loss: 0.000000\n",
            "epoch 89, step 5637, loss: 0.000000\n",
            "epoch 89, step 5638, loss: 0.000000\n",
            "epoch 89, step 5639, loss: 0.000008\n",
            "epoch 89, step 5640, loss: 0.000000\n",
            "epoch 89, step 5641, loss: 0.000000\n",
            "epoch 89, step 5642, loss: 0.000000\n",
            "epoch 89, step 5643, loss: 0.000000\n",
            "epoch 89, step 5644, loss: 0.000000\n",
            "epoch 89, step 5645, loss: 0.000000\n",
            "epoch 89, step 5646, loss: 0.000000\n",
            "epoch 89, step 5647, loss: 0.000000\n",
            "epoch 89, step 5648, loss: 25.539719\n",
            "epoch 89, step 5649, loss: 0.000000\n",
            "epoch 89, step 5650, loss: 0.000000\n",
            "epoch 89, step 5651, loss: 0.000159\n",
            "epoch 89, step 5652, loss: 0.000000\n",
            "epoch 89, step 5653, loss: 0.000000\n",
            "epoch 89, step 5654, loss: 0.000000\n",
            "epoch 89, step 5655, loss: 0.000000\n",
            "epoch 89, step 5656, loss: 0.000000\n",
            "epoch 89, step 5657, loss: 0.000000\n",
            "epoch 89, step 5658, loss: 0.000169\n",
            "epoch 89, step 5659, loss: 0.000000\n",
            "epoch 89, step 5660, loss: 0.000000\n",
            "epoch 89, step 5661, loss: 0.000000\n",
            "epoch 89, step 5662, loss: 17.651424\n",
            "epoch 89, step 5663, loss: 0.000000\n",
            "epoch 89, step 5664, loss: 0.013005\n",
            "epoch 89, step 5665, loss: 0.238490\n",
            "epoch 89, step 5666, loss: 0.000000\n",
            "epoch 89, step 5667, loss: 0.000000\n",
            "epoch 89, step 5668, loss: 0.000000\n",
            "epoch 89, step 5669, loss: 35.699299\n",
            "epoch 90, step 5670, loss: 0.000000\n",
            "epoch 90, step 5671, loss: 0.000000\n",
            "epoch 90, step 5672, loss: 47.812878\n",
            "epoch 90, step 5673, loss: 20.247185\n",
            "epoch 90, step 5674, loss: 0.000000\n",
            "epoch 90, step 5675, loss: 0.000185\n",
            "epoch 90, step 5676, loss: 0.000000\n",
            "epoch 90, step 5677, loss: 0.000000\n",
            "epoch 90, step 5678, loss: 0.000000\n",
            "epoch 90, step 5679, loss: 20.173769\n",
            "epoch 90, step 5680, loss: 99.480316\n",
            "epoch 90, step 5681, loss: 82.441086\n",
            "epoch 90, step 5682, loss: 0.004541\n",
            "epoch 90, step 5683, loss: 0.000000\n",
            "epoch 90, step 5684, loss: 0.000000\n",
            "epoch 90, step 5685, loss: 3.756404\n",
            "epoch 90, step 5686, loss: 0.000000\n",
            "epoch 90, step 5687, loss: 0.000000\n",
            "epoch 90, step 5688, loss: 0.000002\n",
            "epoch 90, step 5689, loss: 83.555496\n",
            "epoch 90, step 5690, loss: 0.000000\n",
            "epoch 90, step 5691, loss: 0.000000\n",
            "epoch 90, step 5692, loss: 0.000000\n",
            "epoch 90, step 5693, loss: 0.000000\n",
            "epoch 90, step 5694, loss: 0.000000\n",
            "epoch 90, step 5695, loss: 35.453678\n",
            "epoch 90, step 5696, loss: 0.000000\n",
            "epoch 90, step 5697, loss: 0.000000\n",
            "epoch 90, step 5698, loss: 0.000000\n",
            "epoch 90, step 5699, loss: 0.000000\n",
            "epoch 90, step 5700, loss: 0.000000\n",
            "epoch 90, step 5701, loss: 0.000000\n",
            "epoch 90, step 5702, loss: 4.659121\n",
            "epoch 90, step 5703, loss: 0.000000\n",
            "epoch 90, step 5704, loss: 0.000000\n",
            "epoch 90, step 5705, loss: 0.000000\n",
            "epoch 90, step 5706, loss: 0.000000\n",
            "epoch 90, step 5707, loss: 0.000000\n",
            "epoch 90, step 5708, loss: 0.000000\n",
            "epoch 90, step 5709, loss: 0.000000\n",
            "epoch 90, step 5710, loss: 0.000000\n",
            "epoch 90, step 5711, loss: 44.790878\n",
            "epoch 90, step 5712, loss: 0.000000\n",
            "epoch 90, step 5713, loss: 0.000000\n",
            "epoch 90, step 5714, loss: 0.000007\n",
            "epoch 90, step 5715, loss: 0.000000\n",
            "epoch 90, step 5716, loss: 0.000000\n",
            "epoch 90, step 5717, loss: 0.000000\n",
            "epoch 90, step 5718, loss: 0.000000\n",
            "epoch 90, step 5719, loss: 0.000000\n",
            "epoch 90, step 5720, loss: 0.000000\n",
            "epoch 90, step 5721, loss: 0.000000\n",
            "epoch 90, step 5722, loss: 0.000000\n",
            "epoch 90, step 5723, loss: 0.000000\n",
            "epoch 90, step 5724, loss: 0.000000\n",
            "epoch 90, step 5725, loss: 0.000000\n",
            "epoch 90, step 5726, loss: 0.000011\n",
            "epoch 90, step 5727, loss: 0.002867\n",
            "epoch 90, step 5728, loss: 88.797935\n",
            "epoch 90, step 5729, loss: 0.000000\n",
            "epoch 90, step 5730, loss: 0.000000\n",
            "epoch 90, step 5731, loss: 0.000000\n",
            "epoch 90, step 5732, loss: 24.556894\n",
            "epoch 91, step 5733, loss: 140.571030\n",
            "epoch 91, step 5734, loss: 0.000000\n",
            "epoch 91, step 5735, loss: 0.000000\n",
            "epoch 91, step 5736, loss: 90.441063\n",
            "epoch 91, step 5737, loss: 0.000000\n",
            "epoch 91, step 5738, loss: 25.509926\n",
            "epoch 91, step 5739, loss: 0.014307\n",
            "epoch 91, step 5740, loss: 7.021305\n",
            "epoch 91, step 5741, loss: 0.000000\n",
            "epoch 91, step 5742, loss: 0.000000\n",
            "epoch 91, step 5743, loss: 54.965248\n",
            "epoch 91, step 5744, loss: 77.293610\n",
            "epoch 91, step 5745, loss: 0.000000\n",
            "epoch 91, step 5746, loss: 0.000000\n",
            "epoch 91, step 5747, loss: 0.000000\n",
            "epoch 91, step 5748, loss: 1.906152\n",
            "epoch 91, step 5749, loss: 0.000000\n",
            "epoch 91, step 5750, loss: 0.000000\n",
            "epoch 91, step 5751, loss: 0.000001\n",
            "epoch 91, step 5752, loss: 7.226274\n",
            "epoch 91, step 5753, loss: 0.000000\n",
            "epoch 91, step 5754, loss: 0.000000\n",
            "epoch 91, step 5755, loss: 0.000000\n",
            "epoch 91, step 5756, loss: 0.000000\n",
            "epoch 91, step 5757, loss: 0.000000\n",
            "epoch 91, step 5758, loss: 18.398195\n",
            "epoch 91, step 5759, loss: 0.000000\n",
            "epoch 91, step 5760, loss: 0.029672\n",
            "epoch 91, step 5761, loss: 0.007400\n",
            "epoch 91, step 5762, loss: 0.000000\n",
            "epoch 91, step 5763, loss: 0.000000\n",
            "epoch 91, step 5764, loss: 0.000000\n",
            "epoch 91, step 5765, loss: 0.000007\n",
            "epoch 91, step 5766, loss: 0.000000\n",
            "epoch 91, step 5767, loss: 0.000000\n",
            "epoch 91, step 5768, loss: 0.000000\n",
            "epoch 91, step 5769, loss: 0.000000\n",
            "epoch 91, step 5770, loss: 0.000000\n",
            "epoch 91, step 5771, loss: 0.000000\n",
            "epoch 91, step 5772, loss: 0.000001\n",
            "epoch 91, step 5773, loss: 0.000000\n",
            "epoch 91, step 5774, loss: 14.492360\n",
            "epoch 91, step 5775, loss: 0.000000\n",
            "epoch 91, step 5776, loss: 0.000000\n",
            "epoch 91, step 5777, loss: 0.000002\n",
            "epoch 91, step 5778, loss: 0.000000\n",
            "epoch 91, step 5779, loss: 0.000000\n",
            "epoch 91, step 5780, loss: 0.000253\n",
            "epoch 91, step 5781, loss: 0.000000\n",
            "epoch 91, step 5782, loss: 0.000000\n",
            "epoch 91, step 5783, loss: 0.000000\n",
            "epoch 91, step 5784, loss: 3.121552\n",
            "epoch 91, step 5785, loss: 0.000000\n",
            "epoch 91, step 5786, loss: 0.000000\n",
            "epoch 91, step 5787, loss: 0.000000\n",
            "epoch 91, step 5788, loss: 0.000000\n",
            "epoch 91, step 5789, loss: 0.000000\n",
            "epoch 91, step 5790, loss: 94.846123\n",
            "epoch 91, step 5791, loss: 95.877029\n",
            "epoch 91, step 5792, loss: 0.000000\n",
            "epoch 91, step 5793, loss: 0.000000\n",
            "epoch 91, step 5794, loss: 0.000000\n",
            "epoch 91, step 5795, loss: 19.345798\n",
            "epoch 92, step 5796, loss: 0.000000\n",
            "epoch 92, step 5797, loss: 0.000000\n",
            "epoch 92, step 5798, loss: 0.000000\n",
            "epoch 92, step 5799, loss: 215.396484\n",
            "epoch 92, step 5800, loss: 0.000000\n",
            "epoch 92, step 5801, loss: 0.073003\n",
            "epoch 92, step 5802, loss: 0.000000\n",
            "epoch 92, step 5803, loss: 0.000000\n",
            "epoch 92, step 5804, loss: 0.000000\n",
            "epoch 92, step 5805, loss: 41.812260\n",
            "epoch 92, step 5806, loss: 85.551964\n",
            "epoch 92, step 5807, loss: 77.251030\n",
            "epoch 92, step 5808, loss: 0.000000\n",
            "epoch 92, step 5809, loss: 0.000000\n",
            "epoch 92, step 5810, loss: 0.000000\n",
            "epoch 92, step 5811, loss: 0.000044\n",
            "epoch 92, step 5812, loss: 0.000000\n",
            "epoch 92, step 5813, loss: 0.000000\n",
            "epoch 92, step 5814, loss: 0.152216\n",
            "epoch 92, step 5815, loss: 57.580738\n",
            "epoch 92, step 5816, loss: 0.000000\n",
            "epoch 92, step 5817, loss: 0.000000\n",
            "epoch 92, step 5818, loss: 0.000000\n",
            "epoch 92, step 5819, loss: 0.000000\n",
            "epoch 92, step 5820, loss: 0.000000\n",
            "epoch 92, step 5821, loss: 30.703577\n",
            "epoch 92, step 5822, loss: 0.000000\n",
            "epoch 92, step 5823, loss: 0.000002\n",
            "epoch 92, step 5824, loss: 0.000001\n",
            "epoch 92, step 5825, loss: 0.000000\n",
            "epoch 92, step 5826, loss: 0.000000\n",
            "epoch 92, step 5827, loss: 0.000000\n",
            "epoch 92, step 5828, loss: 0.000002\n",
            "epoch 92, step 5829, loss: 0.000000\n",
            "epoch 92, step 5830, loss: 0.000000\n",
            "epoch 92, step 5831, loss: 0.000000\n",
            "epoch 92, step 5832, loss: 0.000000\n",
            "epoch 92, step 5833, loss: 0.000000\n",
            "epoch 92, step 5834, loss: 0.000000\n",
            "epoch 92, step 5835, loss: 0.000000\n",
            "epoch 92, step 5836, loss: 0.000000\n",
            "epoch 92, step 5837, loss: 22.849817\n",
            "epoch 92, step 5838, loss: 0.000000\n",
            "epoch 92, step 5839, loss: 0.000000\n",
            "epoch 92, step 5840, loss: 3.478846\n",
            "epoch 92, step 5841, loss: 0.000000\n",
            "epoch 92, step 5842, loss: 0.000000\n",
            "epoch 92, step 5843, loss: 0.000000\n",
            "epoch 92, step 5844, loss: 0.000000\n",
            "epoch 92, step 5845, loss: 0.000000\n",
            "epoch 92, step 5846, loss: 0.000000\n",
            "epoch 92, step 5847, loss: 0.009513\n",
            "epoch 92, step 5848, loss: 0.000000\n",
            "epoch 92, step 5849, loss: 0.000000\n",
            "epoch 92, step 5850, loss: 0.000000\n",
            "epoch 92, step 5851, loss: 0.000000\n",
            "epoch 92, step 5852, loss: 0.000000\n",
            "epoch 92, step 5853, loss: 0.388231\n",
            "epoch 92, step 5854, loss: 70.014931\n",
            "epoch 92, step 5855, loss: 0.000000\n",
            "epoch 92, step 5856, loss: 0.000000\n",
            "epoch 92, step 5857, loss: 0.000000\n",
            "epoch 92, step 5858, loss: 6.606641\n",
            "epoch 93, step 5859, loss: 0.000000\n",
            "epoch 93, step 5860, loss: 0.000143\n",
            "epoch 93, step 5861, loss: 0.000000\n",
            "epoch 93, step 5862, loss: 10.570179\n",
            "epoch 93, step 5863, loss: 0.003682\n",
            "epoch 93, step 5864, loss: 0.000043\n",
            "epoch 93, step 5865, loss: 0.000000\n",
            "epoch 93, step 5866, loss: 0.000000\n",
            "epoch 93, step 5867, loss: 0.000001\n",
            "epoch 93, step 5868, loss: 20.711657\n",
            "epoch 93, step 5869, loss: 110.446457\n",
            "epoch 93, step 5870, loss: 40.589626\n",
            "epoch 93, step 5871, loss: 0.000158\n",
            "epoch 93, step 5872, loss: 0.000000\n",
            "epoch 93, step 5873, loss: 0.000000\n",
            "epoch 93, step 5874, loss: 19.880344\n",
            "epoch 93, step 5875, loss: 0.000000\n",
            "epoch 93, step 5876, loss: 7.912294\n",
            "epoch 93, step 5877, loss: 0.003664\n",
            "epoch 93, step 5878, loss: 71.796562\n",
            "epoch 93, step 5879, loss: 0.000000\n",
            "epoch 93, step 5880, loss: 0.000084\n",
            "epoch 93, step 5881, loss: 0.000000\n",
            "epoch 93, step 5882, loss: 0.000000\n",
            "epoch 93, step 5883, loss: 0.108267\n",
            "epoch 93, step 5884, loss: 67.767036\n",
            "epoch 93, step 5885, loss: 0.000000\n",
            "epoch 93, step 5886, loss: 0.000066\n",
            "epoch 93, step 5887, loss: 0.000017\n",
            "epoch 93, step 5888, loss: 0.000000\n",
            "epoch 93, step 5889, loss: 0.000000\n",
            "epoch 93, step 5890, loss: 0.000000\n",
            "epoch 93, step 5891, loss: 15.102727\n",
            "epoch 93, step 5892, loss: 0.000001\n",
            "epoch 93, step 5893, loss: 0.000000\n",
            "epoch 93, step 5894, loss: 0.000000\n",
            "epoch 93, step 5895, loss: 0.000000\n",
            "epoch 93, step 5896, loss: 0.000000\n",
            "epoch 93, step 5897, loss: 0.000010\n",
            "epoch 93, step 5898, loss: 0.001403\n",
            "epoch 93, step 5899, loss: 0.000000\n",
            "epoch 93, step 5900, loss: 33.116280\n",
            "epoch 93, step 5901, loss: 0.000000\n",
            "epoch 93, step 5902, loss: 0.000000\n",
            "epoch 93, step 5903, loss: 0.000000\n",
            "epoch 93, step 5904, loss: 0.000000\n",
            "epoch 93, step 5905, loss: 0.000000\n",
            "epoch 93, step 5906, loss: 0.000000\n",
            "epoch 93, step 5907, loss: 0.000000\n",
            "epoch 93, step 5908, loss: 0.000000\n",
            "epoch 93, step 5909, loss: 0.000000\n",
            "epoch 93, step 5910, loss: 0.000000\n",
            "epoch 93, step 5911, loss: 0.000000\n",
            "epoch 93, step 5912, loss: 0.000000\n",
            "epoch 93, step 5913, loss: 0.000000\n",
            "epoch 93, step 5914, loss: 0.000000\n",
            "epoch 93, step 5915, loss: 0.000000\n",
            "epoch 93, step 5916, loss: 0.000000\n",
            "epoch 93, step 5917, loss: 79.616226\n",
            "epoch 93, step 5918, loss: 0.000000\n",
            "epoch 93, step 5919, loss: 0.000000\n",
            "epoch 93, step 5920, loss: 30.995089\n",
            "epoch 93, step 5921, loss: 23.209810\n",
            "epoch 94, step 5922, loss: 140.474121\n",
            "epoch 94, step 5923, loss: 0.000000\n",
            "epoch 94, step 5924, loss: 0.000000\n",
            "epoch 94, step 5925, loss: 102.656517\n",
            "epoch 94, step 5926, loss: 0.000000\n",
            "epoch 94, step 5927, loss: 0.000000\n",
            "epoch 94, step 5928, loss: 0.000000\n",
            "epoch 94, step 5929, loss: 0.000000\n",
            "epoch 94, step 5930, loss: 0.000000\n",
            "epoch 94, step 5931, loss: 0.000000\n",
            "epoch 94, step 5932, loss: 56.262615\n",
            "epoch 94, step 5933, loss: 34.733337\n",
            "epoch 94, step 5934, loss: 0.000000\n",
            "epoch 94, step 5935, loss: 0.000000\n",
            "epoch 94, step 5936, loss: 0.000001\n",
            "epoch 94, step 5937, loss: 0.025858\n",
            "epoch 94, step 5938, loss: 0.000000\n",
            "epoch 94, step 5939, loss: 0.754104\n",
            "epoch 94, step 5940, loss: 7.506647\n",
            "epoch 94, step 5941, loss: 50.768158\n",
            "epoch 94, step 5942, loss: 0.000000\n",
            "epoch 94, step 5943, loss: 0.000000\n",
            "epoch 94, step 5944, loss: 0.000000\n",
            "epoch 94, step 5945, loss: 0.000000\n",
            "epoch 94, step 5946, loss: 0.000000\n",
            "epoch 94, step 5947, loss: 16.933540\n",
            "epoch 94, step 5948, loss: 0.000000\n",
            "epoch 94, step 5949, loss: 5.274601\n",
            "epoch 94, step 5950, loss: 1.862028\n",
            "epoch 94, step 5951, loss: 0.000000\n",
            "epoch 94, step 5952, loss: 0.000000\n",
            "epoch 94, step 5953, loss: 0.000000\n",
            "epoch 94, step 5954, loss: 0.000000\n",
            "epoch 94, step 5955, loss: 0.000000\n",
            "epoch 94, step 5956, loss: 0.000000\n",
            "epoch 94, step 5957, loss: 0.000000\n",
            "epoch 94, step 5958, loss: 0.000000\n",
            "epoch 94, step 5959, loss: 35.229195\n",
            "epoch 94, step 5960, loss: 0.000000\n",
            "epoch 94, step 5961, loss: 0.000000\n",
            "epoch 94, step 5962, loss: 0.000000\n",
            "epoch 94, step 5963, loss: 39.318329\n",
            "epoch 94, step 5964, loss: 0.000000\n",
            "epoch 94, step 5965, loss: 0.000000\n",
            "epoch 94, step 5966, loss: 0.000000\n",
            "epoch 94, step 5967, loss: 0.000000\n",
            "epoch 94, step 5968, loss: 0.000000\n",
            "epoch 94, step 5969, loss: 0.000021\n",
            "epoch 94, step 5970, loss: 0.000000\n",
            "epoch 94, step 5971, loss: 0.000000\n",
            "epoch 94, step 5972, loss: 0.000000\n",
            "epoch 94, step 5973, loss: 0.000000\n",
            "epoch 94, step 5974, loss: 0.000000\n",
            "epoch 94, step 5975, loss: 0.000000\n",
            "epoch 94, step 5976, loss: 0.000000\n",
            "epoch 94, step 5977, loss: 0.000000\n",
            "epoch 94, step 5978, loss: 0.000000\n",
            "epoch 94, step 5979, loss: 51.800453\n",
            "epoch 94, step 5980, loss: 94.907974\n",
            "epoch 94, step 5981, loss: 0.000000\n",
            "epoch 94, step 5982, loss: 0.000000\n",
            "epoch 94, step 5983, loss: 0.000000\n",
            "epoch 94, step 5984, loss: 27.129229\n",
            "epoch 95, step 5985, loss: 0.000000\n",
            "epoch 95, step 5986, loss: 0.000000\n",
            "epoch 95, step 5987, loss: 0.000000\n",
            "epoch 95, step 5988, loss: 215.159958\n",
            "epoch 95, step 5989, loss: 0.000000\n",
            "epoch 95, step 5990, loss: 5.894616\n",
            "epoch 95, step 5991, loss: 0.000000\n",
            "epoch 95, step 5992, loss: 0.000000\n",
            "epoch 95, step 5993, loss: 0.003731\n",
            "epoch 95, step 5994, loss: 4.084583\n",
            "epoch 95, step 5995, loss: 78.166718\n",
            "epoch 95, step 5996, loss: 20.341129\n",
            "epoch 95, step 5997, loss: 0.000000\n",
            "epoch 95, step 5998, loss: 0.000000\n",
            "epoch 95, step 5999, loss: 0.000001\n",
            "epoch 95, step 6000, loss: 0.000001\n",
            "epoch 95, step 6001, loss: 0.000356\n",
            "epoch 95, step 6002, loss: 0.000000\n",
            "epoch 95, step 6003, loss: 0.000000\n",
            "epoch 95, step 6004, loss: 89.826019\n",
            "epoch 95, step 6005, loss: 0.000000\n",
            "epoch 95, step 6006, loss: 0.000001\n",
            "epoch 95, step 6007, loss: 0.000000\n",
            "epoch 95, step 6008, loss: 0.000002\n",
            "epoch 95, step 6009, loss: 0.000000\n",
            "epoch 95, step 6010, loss: 55.406124\n",
            "epoch 95, step 6011, loss: 0.000000\n",
            "epoch 95, step 6012, loss: 0.000004\n",
            "epoch 95, step 6013, loss: 0.507599\n",
            "epoch 95, step 6014, loss: 0.000000\n",
            "epoch 95, step 6015, loss: 0.000000\n",
            "epoch 95, step 6016, loss: 3.997107\n",
            "epoch 95, step 6017, loss: 0.000039\n",
            "epoch 95, step 6018, loss: 15.190027\n",
            "epoch 95, step 6019, loss: 0.001930\n",
            "epoch 95, step 6020, loss: 0.000000\n",
            "epoch 95, step 6021, loss: 0.000000\n",
            "epoch 95, step 6022, loss: 0.000000\n",
            "epoch 95, step 6023, loss: 0.000000\n",
            "epoch 95, step 6024, loss: 0.000000\n",
            "epoch 95, step 6025, loss: 0.000000\n",
            "epoch 95, step 6026, loss: 26.320734\n",
            "epoch 95, step 6027, loss: 0.000000\n",
            "epoch 95, step 6028, loss: 0.000000\n",
            "epoch 95, step 6029, loss: 0.000000\n",
            "epoch 95, step 6030, loss: 0.000000\n",
            "epoch 95, step 6031, loss: 0.000000\n",
            "epoch 95, step 6032, loss: 0.000000\n",
            "epoch 95, step 6033, loss: 0.000000\n",
            "epoch 95, step 6034, loss: 0.000000\n",
            "epoch 95, step 6035, loss: 0.000000\n",
            "epoch 95, step 6036, loss: 0.000105\n",
            "epoch 95, step 6037, loss: 0.000000\n",
            "epoch 95, step 6038, loss: 0.000000\n",
            "epoch 95, step 6039, loss: 0.000000\n",
            "epoch 95, step 6040, loss: 0.000000\n",
            "epoch 95, step 6041, loss: 0.000000\n",
            "epoch 95, step 6042, loss: 0.000000\n",
            "epoch 95, step 6043, loss: 68.500984\n",
            "epoch 95, step 6044, loss: 0.000000\n",
            "epoch 95, step 6045, loss: 0.000000\n",
            "epoch 95, step 6046, loss: 0.000000\n",
            "epoch 95, step 6047, loss: 19.728655\n",
            "epoch 96, step 6048, loss: 0.000000\n",
            "epoch 96, step 6049, loss: 0.000000\n",
            "epoch 96, step 6050, loss: 0.000000\n",
            "epoch 96, step 6051, loss: 13.537158\n",
            "epoch 96, step 6052, loss: 0.000000\n",
            "epoch 96, step 6053, loss: 0.000000\n",
            "epoch 96, step 6054, loss: 0.000000\n",
            "epoch 96, step 6055, loss: 0.000000\n",
            "epoch 96, step 6056, loss: 0.000000\n",
            "epoch 96, step 6057, loss: 14.847105\n",
            "epoch 96, step 6058, loss: 98.210411\n",
            "epoch 96, step 6059, loss: 21.704428\n",
            "epoch 96, step 6060, loss: 0.000000\n",
            "epoch 96, step 6061, loss: 0.000000\n",
            "epoch 96, step 6062, loss: 0.000000\n",
            "epoch 96, step 6063, loss: 20.533640\n",
            "epoch 96, step 6064, loss: 0.000000\n",
            "epoch 96, step 6065, loss: 0.000000\n",
            "epoch 96, step 6066, loss: 0.000000\n",
            "epoch 96, step 6067, loss: 73.880341\n",
            "epoch 96, step 6068, loss: 0.000000\n",
            "epoch 96, step 6069, loss: 0.000001\n",
            "epoch 96, step 6070, loss: 0.000000\n",
            "epoch 96, step 6071, loss: 0.000000\n",
            "epoch 96, step 6072, loss: 0.000000\n",
            "epoch 96, step 6073, loss: 57.104874\n",
            "epoch 96, step 6074, loss: 0.000000\n",
            "epoch 96, step 6075, loss: 0.046762\n",
            "epoch 96, step 6076, loss: 0.001781\n",
            "epoch 96, step 6077, loss: 0.000000\n",
            "epoch 96, step 6078, loss: 0.000000\n",
            "epoch 96, step 6079, loss: 0.000000\n",
            "epoch 96, step 6080, loss: 0.000000\n",
            "epoch 96, step 6081, loss: 0.000000\n",
            "epoch 96, step 6082, loss: 0.000000\n",
            "epoch 96, step 6083, loss: 0.000000\n",
            "epoch 96, step 6084, loss: 0.000000\n",
            "epoch 96, step 6085, loss: 0.000000\n",
            "epoch 96, step 6086, loss: 0.000000\n",
            "epoch 96, step 6087, loss: 0.000000\n",
            "epoch 96, step 6088, loss: 0.000000\n",
            "epoch 96, step 6089, loss: 28.594151\n",
            "epoch 96, step 6090, loss: 0.000000\n",
            "epoch 96, step 6091, loss: 38.377907\n",
            "epoch 96, step 6092, loss: 0.000000\n",
            "epoch 96, step 6093, loss: 2.967912\n",
            "epoch 96, step 6094, loss: 0.000000\n",
            "epoch 96, step 6095, loss: 0.000000\n",
            "epoch 96, step 6096, loss: 0.000000\n",
            "epoch 96, step 6097, loss: 0.000000\n",
            "epoch 96, step 6098, loss: 0.000000\n",
            "epoch 96, step 6099, loss: 0.651086\n",
            "epoch 96, step 6100, loss: 0.000000\n",
            "epoch 96, step 6101, loss: 0.000000\n",
            "epoch 96, step 6102, loss: 0.000000\n",
            "epoch 96, step 6103, loss: 40.004459\n",
            "epoch 96, step 6104, loss: 0.000000\n",
            "epoch 96, step 6105, loss: 0.000000\n",
            "epoch 96, step 6106, loss: 117.936302\n",
            "epoch 96, step 6107, loss: 0.000000\n",
            "epoch 96, step 6108, loss: 0.000004\n",
            "epoch 96, step 6109, loss: 0.000000\n",
            "epoch 96, step 6110, loss: 0.000001\n",
            "epoch 97, step 6111, loss: 189.474289\n",
            "epoch 97, step 6112, loss: 0.298927\n",
            "epoch 97, step 6113, loss: 0.000023\n",
            "epoch 97, step 6114, loss: 134.211319\n",
            "epoch 97, step 6115, loss: 0.000009\n",
            "epoch 97, step 6116, loss: 0.000000\n",
            "epoch 97, step 6117, loss: 0.000000\n",
            "epoch 97, step 6118, loss: 0.000000\n",
            "epoch 97, step 6119, loss: 0.000000\n",
            "epoch 97, step 6120, loss: 0.000000\n",
            "epoch 97, step 6121, loss: 0.000011\n",
            "epoch 97, step 6122, loss: 85.717041\n",
            "epoch 97, step 6123, loss: 0.000000\n",
            "epoch 97, step 6124, loss: 0.000000\n",
            "epoch 97, step 6125, loss: 0.000000\n",
            "epoch 97, step 6126, loss: 0.000000\n",
            "epoch 97, step 6127, loss: 0.000000\n",
            "epoch 97, step 6128, loss: 0.000000\n",
            "epoch 97, step 6129, loss: 0.000000\n",
            "epoch 97, step 6130, loss: 55.210125\n",
            "epoch 97, step 6131, loss: 0.000000\n",
            "epoch 97, step 6132, loss: 0.000000\n",
            "epoch 97, step 6133, loss: 0.000000\n",
            "epoch 97, step 6134, loss: 0.000000\n",
            "epoch 97, step 6135, loss: 0.000000\n",
            "epoch 97, step 6136, loss: 23.798107\n",
            "epoch 97, step 6137, loss: 0.000000\n",
            "epoch 97, step 6138, loss: 0.000041\n",
            "epoch 97, step 6139, loss: 0.000016\n",
            "epoch 97, step 6140, loss: 0.000000\n",
            "epoch 97, step 6141, loss: 0.000000\n",
            "epoch 97, step 6142, loss: 0.000000\n",
            "epoch 97, step 6143, loss: 0.000003\n",
            "epoch 97, step 6144, loss: 0.000000\n",
            "epoch 97, step 6145, loss: 28.930948\n",
            "epoch 97, step 6146, loss: 0.000000\n",
            "epoch 97, step 6147, loss: 0.000000\n",
            "epoch 97, step 6148, loss: 0.000000\n",
            "epoch 97, step 6149, loss: 0.000001\n",
            "epoch 97, step 6150, loss: 0.000000\n",
            "epoch 97, step 6151, loss: 0.000000\n",
            "epoch 97, step 6152, loss: 21.496674\n",
            "epoch 97, step 6153, loss: 0.000000\n",
            "epoch 97, step 6154, loss: 0.000000\n",
            "epoch 97, step 6155, loss: 0.000000\n",
            "epoch 97, step 6156, loss: 0.000000\n",
            "epoch 97, step 6157, loss: 0.000000\n",
            "epoch 97, step 6158, loss: 0.000000\n",
            "epoch 97, step 6159, loss: 0.000000\n",
            "epoch 97, step 6160, loss: 0.000000\n",
            "epoch 97, step 6161, loss: 0.000000\n",
            "epoch 97, step 6162, loss: 0.012308\n",
            "epoch 97, step 6163, loss: 0.000359\n",
            "epoch 97, step 6164, loss: 0.000000\n",
            "epoch 97, step 6165, loss: 0.000000\n",
            "epoch 97, step 6166, loss: 0.000000\n",
            "epoch 97, step 6167, loss: 0.000000\n",
            "epoch 97, step 6168, loss: 44.454815\n",
            "epoch 97, step 6169, loss: 103.834793\n",
            "epoch 97, step 6170, loss: 0.000533\n",
            "epoch 97, step 6171, loss: 0.000001\n",
            "epoch 97, step 6172, loss: 0.000000\n",
            "epoch 97, step 6173, loss: 21.627113\n",
            "epoch 98, step 6174, loss: 0.700191\n",
            "epoch 98, step 6175, loss: 0.000000\n",
            "epoch 98, step 6176, loss: 0.000000\n",
            "epoch 98, step 6177, loss: 190.661209\n",
            "epoch 98, step 6178, loss: 0.000000\n",
            "epoch 98, step 6179, loss: 0.000000\n",
            "epoch 98, step 6180, loss: 0.000000\n",
            "epoch 98, step 6181, loss: 0.000000\n",
            "epoch 98, step 6182, loss: 0.000007\n",
            "epoch 98, step 6183, loss: 15.808167\n",
            "epoch 98, step 6184, loss: 100.842934\n",
            "epoch 98, step 6185, loss: 40.272785\n",
            "epoch 98, step 6186, loss: 0.000000\n",
            "epoch 98, step 6187, loss: 0.000000\n",
            "epoch 98, step 6188, loss: 0.000000\n",
            "epoch 98, step 6189, loss: 19.414354\n",
            "epoch 98, step 6190, loss: 0.000000\n",
            "epoch 98, step 6191, loss: 0.000000\n",
            "epoch 98, step 6192, loss: 0.000000\n",
            "epoch 98, step 6193, loss: 89.488197\n",
            "epoch 98, step 6194, loss: 0.000000\n",
            "epoch 98, step 6195, loss: 0.000000\n",
            "epoch 98, step 6196, loss: 0.000000\n",
            "epoch 98, step 6197, loss: 0.000000\n",
            "epoch 98, step 6198, loss: 0.000000\n",
            "epoch 98, step 6199, loss: 49.243340\n",
            "epoch 98, step 6200, loss: 0.000000\n",
            "epoch 98, step 6201, loss: 0.009130\n",
            "epoch 98, step 6202, loss: 0.001103\n",
            "epoch 98, step 6203, loss: 0.000000\n",
            "epoch 98, step 6204, loss: 0.000000\n",
            "epoch 98, step 6205, loss: 0.000000\n",
            "epoch 98, step 6206, loss: 0.000000\n",
            "epoch 98, step 6207, loss: 0.000000\n",
            "epoch 98, step 6208, loss: 0.000000\n",
            "epoch 98, step 6209, loss: 0.000000\n",
            "epoch 98, step 6210, loss: 0.000000\n",
            "epoch 98, step 6211, loss: 0.000000\n",
            "epoch 98, step 6212, loss: 0.000000\n",
            "epoch 98, step 6213, loss: 0.000000\n",
            "epoch 98, step 6214, loss: 0.000000\n",
            "epoch 98, step 6215, loss: 21.077467\n",
            "epoch 98, step 6216, loss: 0.000000\n",
            "epoch 98, step 6217, loss: 0.000000\n",
            "epoch 98, step 6218, loss: 0.000000\n",
            "epoch 98, step 6219, loss: 0.000000\n",
            "epoch 98, step 6220, loss: 0.000002\n",
            "epoch 98, step 6221, loss: 71.650444\n",
            "epoch 98, step 6222, loss: 0.000000\n",
            "epoch 98, step 6223, loss: 0.000000\n",
            "epoch 98, step 6224, loss: 0.000000\n",
            "epoch 98, step 6225, loss: 0.000082\n",
            "epoch 98, step 6226, loss: 0.000000\n",
            "epoch 98, step 6227, loss: 0.000000\n",
            "epoch 98, step 6228, loss: 0.000000\n",
            "epoch 98, step 6229, loss: 0.000000\n",
            "epoch 98, step 6230, loss: 0.000000\n",
            "epoch 98, step 6231, loss: 0.000001\n",
            "epoch 98, step 6232, loss: 99.172592\n",
            "epoch 98, step 6233, loss: 22.225904\n",
            "epoch 98, step 6234, loss: 0.482247\n",
            "epoch 98, step 6235, loss: 0.000000\n",
            "epoch 98, step 6236, loss: 51.318268\n",
            "epoch 99, step 6237, loss: 0.000000\n",
            "epoch 99, step 6238, loss: 0.000000\n",
            "epoch 99, step 6239, loss: 0.000000\n",
            "epoch 99, step 6240, loss: 18.991388\n",
            "epoch 99, step 6241, loss: 0.000000\n",
            "epoch 99, step 6242, loss: 0.000000\n",
            "epoch 99, step 6243, loss: 0.000000\n",
            "epoch 99, step 6244, loss: 0.000000\n",
            "epoch 99, step 6245, loss: 0.000000\n",
            "epoch 99, step 6246, loss: 0.000008\n",
            "epoch 99, step 6247, loss: 72.511971\n",
            "epoch 99, step 6248, loss: 88.864685\n",
            "epoch 99, step 6249, loss: 0.000000\n",
            "epoch 99, step 6250, loss: 0.000003\n",
            "epoch 99, step 6251, loss: 0.000000\n",
            "epoch 99, step 6252, loss: 33.698433\n",
            "epoch 99, step 6253, loss: 0.000000\n",
            "epoch 99, step 6254, loss: 3.299442\n",
            "epoch 99, step 6255, loss: 3.650958\n",
            "epoch 99, step 6256, loss: 14.749823\n",
            "epoch 99, step 6257, loss: 0.000093\n",
            "epoch 99, step 6258, loss: 0.000000\n",
            "epoch 99, step 6259, loss: 0.000000\n",
            "epoch 99, step 6260, loss: 0.000000\n",
            "epoch 99, step 6261, loss: 2.069965\n",
            "epoch 99, step 6262, loss: 89.967163\n",
            "epoch 99, step 6263, loss: 0.000000\n",
            "epoch 99, step 6264, loss: 8.389412\n",
            "epoch 99, step 6265, loss: 2.250416\n",
            "epoch 99, step 6266, loss: 0.000000\n",
            "epoch 99, step 6267, loss: 0.000000\n",
            "epoch 99, step 6268, loss: 0.000007\n",
            "epoch 99, step 6269, loss: 0.000000\n",
            "epoch 99, step 6270, loss: 0.000000\n",
            "epoch 99, step 6271, loss: 0.000000\n",
            "epoch 99, step 6272, loss: 0.000000\n",
            "epoch 99, step 6273, loss: 0.000000\n",
            "epoch 99, step 6274, loss: 0.000000\n",
            "epoch 99, step 6275, loss: 0.000009\n",
            "epoch 99, step 6276, loss: 0.000000\n",
            "epoch 99, step 6277, loss: 0.000000\n",
            "epoch 99, step 6278, loss: 61.488770\n",
            "epoch 99, step 6279, loss: 0.000000\n",
            "epoch 99, step 6280, loss: 0.000000\n",
            "epoch 99, step 6281, loss: 0.000000\n",
            "epoch 99, step 6282, loss: 0.000000\n",
            "epoch 99, step 6283, loss: 0.000000\n",
            "epoch 99, step 6284, loss: 0.000000\n",
            "epoch 99, step 6285, loss: 86.841614\n",
            "epoch 99, step 6286, loss: 0.000000\n",
            "epoch 99, step 6287, loss: 0.000000\n",
            "epoch 99, step 6288, loss: 1.631148\n",
            "epoch 99, step 6289, loss: 0.000001\n",
            "epoch 99, step 6290, loss: 0.000000\n",
            "epoch 99, step 6291, loss: 0.000000\n",
            "epoch 99, step 6292, loss: 0.000000\n",
            "epoch 99, step 6293, loss: 0.000000\n",
            "epoch 99, step 6294, loss: 32.993065\n",
            "epoch 99, step 6295, loss: 76.028748\n",
            "epoch 99, step 6296, loss: 0.000000\n",
            "epoch 99, step 6297, loss: 0.000000\n",
            "epoch 99, step 6298, loss: 0.000000\n",
            "epoch 99, step 6299, loss: 5.711766\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "M1PTs6h1gdMk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "WLLt1E5PgdsU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c22ca4dd-fb8f-474c-e5dd-0a17989d5d0f"
      },
      "cell_type": "code",
      "source": [
        "score = accuracy_score(valid_y, valid_y_pred)\n",
        "print(\"Unweighted Classification Accuracy: %f\" % score)\n",
        "\n",
        "weighted_score = accuracy_score(valid_y, valid_y_pred, sample_weight=valid_w)\n",
        "print(\"Weighted Classification Accuracy: %f\" % weighted_score)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unweighted Classification Accuracy: 0.933589\n",
            "Weighted Classification Accuracy: 0.662390\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}