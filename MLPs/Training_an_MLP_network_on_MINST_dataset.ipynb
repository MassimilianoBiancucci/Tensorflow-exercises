{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training an MLP network on MINST dataset.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MassimilianoBiancucci/Tensorflow-exercises/blob/master/MLPs/Training_an_MLP_network_on_MINST_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "3Ta14g4qr10t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training an MLP network on MINST dataset"
      ]
    },
    {
      "metadata": {
        "id": "RYWYpNxaklsR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6pwrjRt2sCHd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Importing the **minst** dataset from tensorflow"
      ]
    },
    {
      "metadata": {
        "id": "J0kt9zAwjRLq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZRJFsQPHrDdS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Model declaration:\n",
        "In this section, we set all numeric variables needed for the training .\n"
      ]
    },
    {
      "metadata": {
        "id": "Upx77lZ-ke0s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_size = 784 #number of pixel per sprite in minst dataset (28x28)\n",
        "no_classes = 10 #number of labels in minst dataset (0 .. 9)\n",
        "batch_size = 100 #number of elements in each batch\n",
        "total_batches = 200 #number of batches (total elements / elements x batch)\n",
        "iterNum = 110 #number of iteration, each iteration is a complete training over all dataset \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pq0RuPkE9qSK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we set the tensorflow placeholder for input elements, and labels, we also declare a keep_prob variable for change the dropout probability during training or prediction. (remember tensorflow is  declarative framework!!)"
      ]
    },
    {
      "metadata": {
        "id": "F3jMq1cB9qxP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_input = tf.placeholder(tf.float32, shape=[None, input_size]) #input data placeholder \n",
        "y_input = tf.placeholder(tf.float32, shape=[None, no_classes]) #label data placeholder\n",
        "keep_prob = tf.placeholder(tf.float32) #dropuot keep_prob placeholder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "heNaZEsw-x57",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the block below we declare the first layer of our MLP network.\n",
        "The first step is d eclare the tf.variables, in short terms this are our tensors with which we go to work.\n",
        "\n",
        "**weights**:\n",
        "is a matrix (a 2D tensor), we can see this as a layer with a number of ingress given from first argument (input_size), the number of output is given from the second argument.\n",
        "note that this tesor are initialized with random numbers, given from normal distribution between 0 and 1 (Gaussian).\n",
        "\n",
        "**bais**:\n",
        "Bais is a vector (a 1D tensor), also in this case we use the random initialization. the bais element represent for each neuron the mean output. note that the bais vector need to have the same lenght of the output of its layer, in this case input_size value\n",
        "\n",
        " **matmul**:\n",
        "this command execute the simple matrix multiplication\n",
        "\n",
        "**relu**:\n",
        "(rectified linear unit) Is the non-linear function applied to the output of each neuron. \n",
        "\n",
        "**dropout**:\n",
        "with the **keep_prob** parameter it inserts a probability, with which each output of the layer is not forced to zero. it is possible to set alternatively the **rate** parameter (= 1 - **keep_prob**) which represents the probability that each neuron in the layer is forced to zero. This is a trick that increase the generalization of the network, and limit the overfitting problems, forcing the network to learn the desired function disabling for each time a different subset of its neurons.\n",
        "\n",
        "![alt text](https://i2.wp.com/laid.delanover.com/wp-content/uploads/2018/02/dropout.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "dOtWBtFCkpya",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "weights = tf.Variable(tf.random_normal([input_size, input_size]))\n",
        "bias = tf.Variable(tf.random_normal([input_size]))\n",
        "out1 = tf.matmul(x_input, weights) + bias\n",
        "out1 = tf.nn.relu(out1)\n",
        "out1 = tf.nn.dropout(out1, keep_prob = keep_prob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mrxOJcbnKaLq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "below we declare the output layer, note that in thos case we declare the layer whitout the relu function, because it is an output layer, and now we are using a one_hot encoding for output (this output encoding is used for classification problem which have exclusive class) and the right output function to apply is the softmax function, which tend to maximize the max output and lessen the other outputs."
      ]
    },
    {
      "metadata": {
        "id": "aIRlxO1_m1Hw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "weights2 = tf.Variable(tf.random_normal([input_size, no_classes]))\n",
        "bias2 = tf.Variable(tf.random_normal([no_classes]))\n",
        "logits = tf.matmul(out1, weights2) + bias2\n",
        "logits = tf.nn.dropout(logits, keep_prob = keep_prob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lOldLOHRM4BJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the upper block, we had talk about softmax function but there isn't, because is used in \"**tf.nn.softmax_cross_entropy_with_logits_v2**\"  for efficency reason, and we don't need to call it before.\n",
        "This function use internaly the softmax function and more over it compute the loss.\n",
        "\n",
        "After we call the **tf.reduce_mean** we calculate the mean value of a given tensor\n",
        "\n",
        "In the end we declare the optimizer, in this case** tf.train.AdagradOptimizer**, it is one of various method that calculate the error gradient and the wheights correction to apply. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "V7joKRErk8Xk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "softmax_cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_input, logits=logits)\n",
        "\n",
        "loss_operation = tf.reduce_mean(softmax_cross_entropy)\n",
        "\n",
        "optimiser = tf.train.AdagradOptimizer(learning_rate=0.02).minimize(loss_operation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WbUg388FO88L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Session declaration and tf.variables inizialization."
      ]
    },
    {
      "metadata": {
        "id": "hqdtm35LloXD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "session = tf.Session()\n",
        "session.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LXS6M_tgPFL1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this secction we can see the code necessary to train the network,\n",
        "for each batch iteration we get another batch from dataset, and charge it in the feed_dict dictionary, more specificaly in x_input and y_input which are respectivly the examples value and the labels for each example in the batch.\n",
        "moreover we pass at the feed_dict the keep_prob value, or the dropout probability that we have seen before, this is important because this probability must be 1 at prediction time, in other case we don't get the maximum accurancy from our model. note that the optimiser and loss operation upper declared are passed to session.run() in one vector and not in a feed_dict."
      ]
    },
    {
      "metadata": {
        "id": "bWRVwx0qlqcz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for iterN in range(iterNum):\n",
        "  for batch_no in range(total_batches):\n",
        "      mnist_batch = mnist_data.train.next_batch(batch_size)\n",
        "      _, loss_value = session.run([optimiser, loss_operation], feed_dict={\n",
        "          x_input: mnist_batch[0],\n",
        "          y_input: mnist_batch[1],\n",
        "          keep_prob: 0.6\n",
        "      })\n",
        "      \n",
        "      if(batch_no%20 == 0):\n",
        "        print(loss_value)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WhzONmFlR1gL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After training we want evaluete the generalization power of our model, computing its accurancy on the test dataset,\n",
        "note that it isn't the dataset that we have used for training it.\n",
        "\n",
        "step by step.. \n",
        "\n",
        "we create the variable **predictions**, which is the result of **tf.argmax**, this function return the index of max value contained in the tensor passed (logits that is the output vector of our model), at given axis (1 in this case).\n",
        "\n",
        "**correct predictions** is a simple test condition, that return true if the predicted output is the same of the label, else where false.\n",
        "\n",
        "**acurancy_operation**  the computed mean of prediction reult (1 correct or 0 wrong) in each example passed\n",
        "\n",
        "**note** that **acurancy_operation** at line 3 it isn't a value but a set of instruction to compute whene the session.run() is called, for this reason we pass it as an argument to session.run().\n",
        "\n",
        "as seen before we call session.run() and pass to it all necessary variables, that this time is the test dataset, and the keep_prob set to 1 for disable the dropout operation. (with 1.0 you tell to the framwork that the probability of disabling one neuron is 0)  \n"
      ]
    },
    {
      "metadata": {
        "id": "Q7OY72i_lwXc",
        "colab_type": "code",
        "outputId": "dfb0bd40-e068-4299-d219-3e7c42077eaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "predictions = tf.argmax(logits, 1) #return the index corresponding to max value in the given tensor\n",
        "correct_predictions = tf.equal(predictions, tf.argmax(y_input, 1)) #return a vector that contain for each example 1 if the result is the same in prediction and label or 0 else where.\n",
        "accuracy_operation = tf.reduce_mean(tf.cast(correct_predictions, tf.float32)) #compute the mean of the tensor containing the \n",
        "\n",
        "test_images, test_labels = mnist_data.test.images, mnist_data.test.labels\n",
        "\n",
        "accuracy_value = session.run(accuracy_operation, feed_dict={\n",
        "    x_input: test_images,\n",
        "    y_input: test_labels,\n",
        "    keep_prob: 1.0\n",
        "})\n",
        "print('Accuracy : ', accuracy_value)\n",
        "session.close()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy :  0.9504\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}